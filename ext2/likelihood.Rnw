\section{Model Formulation}

Let $Y_{ij}$ be the proportional score of the steak corresponding to time i and group j, i = 1,..,50 j = 1,2 \\
Let $X_{ij}$ be the storage time for steak i,j \\

We assume $Y_{ij} \sim \text{Beta}(\alpha_{ij}, \beta_{ij})$, and hence we can write the density as: \\

\begin{align*}
f(y_{ij} | \alpha_{ij},\beta_{ij}) = \frac{\Gamma(\alpha_{ij} + \beta_{ij})}{\Gamma(\alpha_{ij})\Gamma(\beta_{ij})}y_{ij}^{\alpha_{ij} - 1}(1 - y_{ij})^{\beta_{ij} - 1}, \quad 0 < y_{ij} < 1, \quad \alpha_{ij} > 0, \quad \beta_{ij} > 0
\end{align*}

We can reparameterize this by noting the expected value and the variance of a Beta random variable. Letting $\mu_{ij} = \frac{\alpha_{ij}}{\alpha_{ij} + \beta_{ij}}$ and $\phi_j = \alpha_{ij} + \beta_{ij}$. $\phi_j$ is constant within group j because we are holding $\alpha_{ij} + \beta_{ij}$ fixed - Note that this constraint was shown on a previous homework to not significantly impact the results of the analysis. Therefore, we have: \\

\begin{align*}
f(y_{ij} | \mu_{ij}, \phi_j) = \frac{\Gamma(\phi_j)}{\Gamma(\mu_{ij}\phi_{ij})\Gamma((1 - \mu_{ij})\phi_j)}y_{ij}^{\mu_{ij}\phi_j - 1}(1 - y_{ij})^{(1 - \mu_{ij})\phi_j - 1}, \quad 0 < y_{ij} < 1, \quad 0 < \mu_{ij} < 1, \quad \phi_j > 0
\end{align*}

We can calculate the expected value and the variance of $Y_{ij}$ given this new parameterization as:

\begin{align*}
E(Y_{ij})   &= \frac{\alpha_{ij}}{\alpha_{ij} + \beta_{ij}} \\
          &= \frac{\alpha_{ij}}{\phi_j} \\
          &= \mu_{ij} \\ \\
Var(Y_{ij}) &= \frac{\alpha_{ij}\beta_{ij}}{(\alpha_{ij} + \beta_{ij})^2(\alpha_{ij} + \beta_{ij} + 1)} \\
          &= \frac{\mu_{ij}(1 - \mu_{ij})}{\phi_j + 1}
\end{align*}

It may also be desired to write $\alpha_{ij}$ and $\beta_{ij}$ in terms of the parameters $\mu_{ij}$ and $\phi_j$ for future clarity:

\begin{align*}
\alpha_{ij} &= \mu_{ij}\phi_j \\
\beta_{ij}  &= (1 - \mu_{ij})\phi_j
\end{align*}

We are going to fit a regression to each group separately. The likelihood for group j is: \\

\begin{align*}
L_j(\mu_{ij}, \phi_j) &= \prod_{i=1}^n f(y_{ij} | \mu_{ij},\phi_j) \\
                  &= \prod_{i=1}^n \frac{\Gamma(\phi_j)}{\Gamma(\mu_{ij}\phi_j)\Gamma((1 - \mu_{ij})\phi_j)}y_{ij}^{\mu_{ij}\phi_j - 1}(1 - y_{ij})^{(1 - \mu_{ij})\phi_j - 1}
\end{align*}

The log-likelihood for group j is: \\

\begin{align*}
\ell_j(\mu_{ij}, \phi_j) &= \sum_{i=1}^n \log f(y_{ij} | \mu_{ij},\phi_j) \\
  &= \sum_{i=1}^n \log(\Gamma(\phi_j)) - \log(\Gamma(\mu_{ij}\phi_j)) - \log(\Gamma((1 - \mu_{ij})\phi_j)) + (\mu_{ij}\phi - 1)\log(y_{ij}) + ((1 - \mu_{ij})\phi_j - 1)\log(1 - y_{ij})
\end{align*}

Although the beta distribution cannot be written in terms of an exponential dispersion family, we will attempt to mimick GLMs in hopes of observing how a beta regression differs from a typical generalized linear model. To do so, we will use a link function which maps the mean $\mu_{ij}$ onto the real line, a logit link, i.e.,

\begin{align*}
\mu_{ij} &= h(x_{ij}, \beta_{kj}) \\
      &= \frac{e^{(\beta_{0j} + \beta_{1j}x_{ij})}}{e^{(\beta_{0j} + \beta_{1j}x_{ij})} + 1}
\end{align*}

\section{Likelihood Analysis}

We will use a newton-raphson algorithm in order to maximize the log-likelihood in $\beta$. We will profile out $\phi$ for each of the two groups to obtain maximum likelihood estimates for all of the regression parameters.

<<newtraph_beta>>=
y <- meat.data$score1
x <- meat.data$time
phi <- 22.4064558

beta.fn <- function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(gamma(phi) / (gamma(mu * phi) * gamma((1 - mu) * phi)) * y^(mu*phi - 1) * (1 - y)^((1 - mu) * phi - 1))
}

beta.loglik <- function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(sum(lgamma(phi) - lgamma(mu * phi) - lgamma(phi * (1 - mu)) + (mu * phi - 1) * log(y) + ((1 - mu) * phi - 1) * log(1 - y)))
}

beta.gradient <- function(par, y, x, phi) {
    T1 <- exp(par[1] + par[2] * x)
    mu <- 1 / (1 + exp(-(par[1] + par[2] * x)))
    
    dldmu <- -phi * digamma(mu * phi) + phi * digamma(phi - mu*phi) + phi * log(y) - phi * log(1 - y)
    dmudb0 <- T1 / (T1 + 1)^2
    dmudb1 <- x*T1 / (T1 + 1)^2
    
    dldb0 <- sum(dldmu * dmudb0)
    dldb1 <- sum(dldmu * dmudb1)
    
    return(c(dldb0, dldb1))
}

beta.hessian <- function(par, y, x, phi) {
    T1 <- exp(par[1] + par[2] * x)
    mu <- 1 / (1 + exp(-(par[1] + par[2] * x)))
    
    dldmu <- -phi * digamma(mu * phi) + phi * digamma(phi - mu*phi) + phi * log(y) - phi * log(1 - y)
    dmudb0 <- T1 / (T1 + 1)^2
    dmudb1 <- x*T1 / (T1 + 1)^2
    
    dldb0 <- sum(dldmu * dmudb0)
    dldb1 <- sum(dldmu * dmudb1)
    
    d2ldmu2 <- -phi^2 * trigamma(mu * phi) - phi^2 * trigamma(phi - mu * phi)
    d2mudb02 <- -T1 * (T1 - 1) / (T1 + 1)^3
    d2mudb12 <- -x^2 * T1 * (T1 - 1) / (T1 + 1)^3
    d2mudb0db1 <- -x * T1 * (T1 - 1) / (T1 + 1)^3
    
    d2ldb02 <- sum(d2ldmu2 * dmudb0 * dmudb0 + d2mudb02 * dldmu)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldmu2 * dmudb0 * dmudb1 + d2mudb0db1 * dldmu)
    d2ldb12 <- sum(d2ldmu2 * dmudb1 * dmudb1 + d2mudb12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb1db0, d2ldb12), nrow = 2, byrow=TRUE))
}
@

\subsection{Derivatives}

Note: For notational convenience, we will drop the j subscript indexing group and assume that these derivatives are written in terms of a group j.

  \subsubsection{First Derivatives}
  \begin{align*}
  \frac{\partial \ell_i}{\partial \beta_k} &=  \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \beta_k}   \quad \text{for $k=0,1$}
  \end{align*}
  \begin{align*}
   \frac{\partial \ell_i}{\partial \mu_i} &= -\phi\Gamma'(\mu_i\phi) + \phi\Gamma'(\phi - \mu_i\phi) + \phi\log{(y_i)} - \phi\log{(1 - y_i)} \\
   \frac{\partial \mu_i}{\partial \beta_0} &= \frac{e^{\beta_0 + \beta_1x_i}}{(e^{\beta_0 + \beta_1x_i} + 1)^2} \\
   \frac{\partial \mu_i}{\partial \beta_1} &= \frac{x_ie^{\beta_0 + \beta_1x_i}}{(e^{\beta_0 + \beta_1x_i} + 1)^2} \\
  \end{align*}
  
  \subsubsection{Second Derivatives}
    \begin{align*}
    \frac{\partial^2 \ell_i}{\partial \beta_k^2} &= \frac{\partial^2 \ell_i}{\partial \mu_i^2}\frac{\partial \mu_i}{\partial \beta_k} +\frac{\partial^2 \mu_i}{\partial \beta_k^2}\frac{\partial \ell_i}{\partial \mu_i}  \quad \text{for $k=0,1$} \\
    \frac{\partial^2 \ell_i}{\partial \beta_0\beta_1} &= \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial \mu_i}{\partial \beta_0} \frac{\partial \mu_i}{\partial \beta_1}+ \frac{\partial^2 \mu_i}{\partial \beta_0 \partial \beta_1}\frac{\partial \ell_i}{\partial \mu_i} =  \frac{\partial^2 \ell_i}{\partial \beta_1\beta_0} \\
    \frac{\partial^2 \ell_i}{\partial \mu_i^2} &= -\phi^2\Gamma''(\mu_i\phi) - \phi^2\Gamma''(\phi - \mu_i\phi) \\
    \frac{\partial^2 \mu_i}{\partial \beta_0^2} &= -\frac{e^{(\beta_0 + \beta_1x_i)}(e^{(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \frac{\partial^2 \mu_i}{\partial \beta_1^2} &= -x_i^2\frac{e^{(\beta_0 + \beta_1x_i)}(e^{(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \frac{\partial^2 \mu_i}{\partial \beta_0\beta_1^2} &= -x_i\frac{e^{(\beta_0 + \beta_1x_i)}(e^{-(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \end{align*}
    
\subsection{Newton-Raphson}
We wrote a newton-raphson function which follows the algorithm from STAT 520. The code is reproduced below:

<<newtraph, echo=TRUE>>=
newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-2, 3), max.iterations = 100, step.halving = TRUE, debug = FALSE, ...) {
    current <- start
    conditions <- TRUE
    
    iterations <- 0
    while (TRUE) {        
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        if(!(any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3])) break;
        
        if (debug) cat(paste("Current loglik is", loglik(current, ...), "\n"))
        if (debug) cat(paste("New is now", new, "\n"))
        if (debug) cat(paste("New loglik is", loglik(new, ...), "\n"))
        
        if (step.halving & (loglik(new, ...) < loglik(current, ...))) {
            if (debug) cat("Uh oh, time to fix this\n")
            m <- 1
            while (m < max.iterations & loglik(new, ...) < loglik(current, ...)) {
                new <- as.vector(current - (1 / (2 * m)) * solve(hessian(current, ...)) %*% gradient(current, ...))
                m <- 2*m;
            }
            if (debug) cat(paste("We have fixed it! its now", new, "\n"))
            if (debug) cat(paste("And the new loglik is finally", loglik(new, ...), "\n"))
        }
        
        iterations <- iterations + 1
        if (iterations > max.iterations) {
            if (debug) cat(paste("Didn't converge in", max.iterations, "iterations\n"))
            break;
        }
                
        if (debug) cat("\n")
        
        current <- new
    }
    
    return(list(loglik = loglik(new, ...), par = new))
}
@

<<profile_code>>=
y <- meat.data$score1
phis <- seq(21, 23, by = 0.01)
profile.beta1 <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.1 <- profile.beta1[which.max(profile.beta1$loglik), ]

y <- meat.data$score2
phis <- seq(6, 8, by = 0.01)
profile.beta2 <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.2 <- profile.beta2[which.max(profile.beta2$loglik), ]

y <- c(meat.data$score1, meat.data$score2)
x <- c(meat.data$time, meat.data$time)
phis <- seq(6, 8, by = 0.01)
profile.betacomb <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.comb <- profile.betacomb[which.max(profile.betacomb$loglik), ]
@

The MLEs obtained by this procedure are given in Table \ref{tbl:mles}. In obtaining the MLEs for $\phi$, we used a profile method. The profile plots of values of $\phi$ against the value of the log-likelihoods are given in Figure \ref{fig:profile_plots}.

<<mle_table, results='asis'>>=
my.df <- data.frame(Group = c("Group1", "Group2", "Combined"), beta0 = c(mle.1$par1, mle.2$par1, mle.comb$par1), beta1 = c(mle.1$par2, mle.2$par2, mle.comb$par2), phi = c(mle.1$phi, mle.2$phi, mle.comb$phi))

print(xtable(my.df, digits = 4, label = "tbl:mles", caption = "MLEs for the first group, second group, and the two groups combined."), include.rownames = FALSE)
@

<<profile_plots, fig.cap='Profile plots of the value of the log-likelihood at particular values of phi for each of the two groups'>>=
p1 <- qplot(profile.beta1$phi, profile.beta1$loglik, geom = "line", colour = I("red")) + theme(aspect.ratio = 1) + geom_vline(xintercept = mle.1$phi) + xlab("phi1") + ylab("Log-Likelihood")
p2 <- qplot(profile.beta2$phi, profile.beta2$loglik, geom = "line", colour = I("blue")) + theme(aspect.ratio = 1) + geom_vline(xintercept = mle.2$phi) + xlab("phi2") + ylab("Log-Likelihood")

grid.arrange(p1, p2, ncol = 2)
@

\subsection{Comparison to betareg Package}

We discovered a package that claims to perform beta regression on CRAN, called {\it betareg}. It uses a similar parameterization, but uses {\it optim} rather than a Newton-based method for optimization. We fit a beta regression using this package to each group, and to the combined data, and obtained the results given in Table \ref{tbl:mlesbreg}. The results are very similar.

<<betareg, echo=FALSE, results='asis'>>=
breg1 <- betareg(score1 ~ time, data = meat.data)
breg2 <- betareg(score2 ~ time, data = meat.data)

meat.data.comb <- data.frame(score = c(meat.data$score1, meat.data$score2), time = rep(meat.data$time, 2))
bregcomb <- betareg(score ~ time, data = meat.data.comb)

my.df <- data.frame(Group = c("Group1", "Group2", "Combined"), beta0 = c(coef(breg1)[1], coef(breg2)[1], coef(bregcomb)[1]), beta1 = c(coef(breg1)[2], coef(breg2)[2], coef(bregcomb)[2]), phi = c(coef(breg1)[3], coef(breg2)[3], coef(bregcomb)[3]))

print(xtable(my.df, digits = 4, label = "tbl:mlesbreg", caption = "MLEs for the first group, second group, and the two groups combined as given by the betareg package on CRAN."), include.rownames = FALSE)
@

\subsection{Regression Line Comparison}
To test whether regressions are the same for each group, we will use a likelihood ratio test, comparing a reduced model consisting of only a single regression line to describe the full set of data, to a full model with separate regression lines for the two scores. If we reject the null hypothesis, this suggests that we would prefer to use separate regression lines, and hence we cannot assume the regression lines are the same between the two groups. We compute $-2ln(\lambda)$ where $\lambda = \frac{\text{Likelihood maximized under reduced model}}{\text{Likelihood maximized under full model}}$. We obtain:

\begin{align*}
X = -2ln(\lambda) &= -2(\Sexpr{mle.comb$loglik} - (\Sexpr{mle.1$loglik} + \Sexpr{mle.2$loglik})) \\
                  &= \Sexpr{-2 * (mle.comb$loglik - (mle.1$loglik + mle.2$loglik))}
\end{align*}

Under the null, $X \sim \chi_3^2$, so at $\alpha = 0.05$ this yields a p-value of \Sexpr{1 - pchisq(-2 * (mle.comb$loglik - (mle.1$loglik + mle.2$loglik)), df = 3)}. Hence, clearly separate regression lines provide a better fit for the data. Let's assess in what way they do so. Figure \ref{fig:reg_plot} shows the three expectation functions for score1, score2, and the combined data in red, blue, and purple respectively. It also overlays the actual data points for score1 and score2. The separate model fit expectation functions certainly appear to do a good job of modeling the temporal trends in the data.

<<reg_plot, fig.cap='Plot of the data for score1 (red), score2 (blue), and the expectation functions for each regression line, and the combined regression line expectation (purple).'>>=
x <- seq(0.5, 30, by = 0.5)
y <- beta.fn(par = c(mle.1$par1, mle.1$par2), y=meat.data$score1, x = meat.data$time, phi=mle.1$phi)

mu.1 <- exp(mle.1$par1 + mle.1$par2 * meat.data$time) / (exp(mle.1$par1 + mle.1$par2 * meat.data$time) + 1)
mu.2 <- exp(mle.2$par1 + mle.2$par2 * meat.data$time) / (exp(mle.2$par1 + mle.2$par2 * meat.data$time) + 1)
mu.comb <- exp(mle.comb$par1 + mle.comb$par2 * meat.data$time) / (exp(mle.comb$par1 + mle.comb$par2 * meat.data$time) + 1)

qplot(meat.data$time, mu.1, geom = "line", colour = I("darkred"), size = I(2), alpha = I(0.6)) +
    geom_point(data = meat.data, aes(x = time, y = score1), colour = "red", size = 3) +
    geom_line(data = data.frame(x = meat.data$time, y = mu.2), inherit.aes = FALSE, aes(x = x, y = y), colour = "darkblue", size = 2, alpha = 0.6) + 
    geom_point(data = meat.data, aes(x = time, y = score2), colour = "blue", size = 3) +
    geom_line(data = data.frame(x = meat.data$time, y = mu.comb), inherit.aes = FALSE, aes(x = x, y = y), colour = "purple", size = 2, alpha = 0.6)
@

We can also consider the distribution at a fixed time values, including day 14, which is of particular interest. Figure \ref{fig:timeplots} illustrates the distributions according to the two models for each of days 1, 7, 14, and 21. Interesting trends can be observed. For instance, at day 1, the new method (group 2) has a peak that is closer to 1, a perfect score. By day 7, the two groups have about the same mean at about 0.75, but there is considerably more variability in the distribution of group 2. By day 14, group 1 has a larger mean score centered at about 0.3, while group 2 has a mean much closer to zero. Nonetheless, group 2's variability means the model places more density in group 2's distribution above about a score of 0.6. At day 21, group 1 is nearing zero while nearly all of the density for group 2 is at about zero.

<<timeplots, fig.cap='Distributions of proportional score according to the fitted model for group 1 (red) and group 2 (blue) at day 1, 7, 14, and 21.'>>=
y <- seq(0.01, 0.99, by = 0.01)

x <- 1
vals.11 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.21 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p1 <- qplot(y, vals.11, geom = "point", colour = I("red")) + geom_line(colour = I("red")) + ggtitle("Distribution at Day 1") +
    geom_point(data = data.frame(x = y, y = vals.21), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.21), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") + xlab("Proportional Score") + ylab("Density")

x <- 7
vals.12 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.22 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p2 <- qplot(y, vals.12, geom = "point", colour = I("red")) + geom_line(colour = I("red")) + ggtitle("Distribution at Day 7") +
    geom_point(data = data.frame(x = y, y = vals.22), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.22), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") + xlab("Proportional Score") + ylab("Density")

x <- 14
vals.13 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.23 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p3 <- qplot(y, vals.13, geom = "point", colour = I("red")) + geom_line(colour = I("red")) + ggtitle("Distribution at Day 14") +
    geom_point(data = data.frame(x = y, y = vals.23), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.23), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") + xlab("Proportional Score") + ylab("Density")

x <- 21
vals.14 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.24 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p4 <- qplot(y, vals.14, geom = "point", colour = I("red")) + geom_line(colour = I("red")) + ggtitle("Distribution at Day 21") +
    geom_point(data = data.frame(x = y, y = vals.24), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.24), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") + xlab("Proportional Score") + ylab("Density")

grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
@

\subsection{Model Assessment}

\subsubsection{Chi-Square}

To assess the model, we can use a Chi-Square distribution to compare the actual data to the postulated model. We compute:

\begin{align*}
d = \sum_{i=1}^n\frac{(y_i - \hat{\mu_i})^2}{\hat{Var(\mu_i)}}
\end{align*}

Noting that $\hat{Var(\mu_i)} = \hat{\mu_i}(1 - \hat{\mu_i})$, we obtain the results presented in Table \ref{tbl:chisq_table}.

<<chisq>>=
var.1 <- mu.1 * (1 - mu.1)
var.2 <- mu.2 * (1 - mu.2)
d.1 <- sum((meat.data$score1 - mu.1)^2 / var.1)
d.2 <- sum((meat.data$score2 - mu.2)^2 / var.2)
@

<<chisq_table, results='asis'>>=
pval.1 <- 1 - pchisq(d.1, df=(nrow(meat.data)-1))
pval.2 <- 1 - pchisq(d.2, df=(nrow(meat.data)-1))

my.df <- data.frame(TestStats = c(d.1, d.2), PValues = c(pval.1, pval.2))

print(xtable(my.df, caption = "Chi-Square test statistic and p-value for comparing the data to the postulated model.", label = "tbl:chisq_table"), include.rownames = FALSE)
@

\subsubsection{Kolmogorov Statistic}
We can simulate from our reference beta distributions by recalling the back-transform for $\alpha_i$ and $\beta_i$, which are: \\

$\alpha_i = \mu_i\phi$ \\
$\beta_i = (1 - \mu_i)\phi$ \\

Doing so, we obtain the results in Table \ref{tbl:sim_ref}. Note that both p-values are insignificant and hence we can conclude there is no strong evidence of a distributional difference between the simulated model data and the actual data.

<<sim_ref, results='asis'>>=
simvals.1 <- rbeta(nrow(meat.data), shape1=mu.1*mle.1$phi, shape2=(1 - mu.1)*mle.1$phi)
simvals.2 <- rbeta(nrow(meat.data), shape1=mu.2*mle.2$phi, shape2=(1 - mu.2)*mle.2$phi)

kstest1 <- ks.test(meat.data$score1, simvals.1)
kstest2 <- ks.test(meat.data$score2, simvals.2)

my.df <- data.frame(Group = c("Group1", "Group2"), "Statistic" = c(kstest1$statistic, kstest2$statistic), "P-Value" = c(kstest1$p.value, kstest2$p.value))

print(xtable(my.df, label = "tbl:sim_ref", caption = "Results of the Kolmogrov-Smirnoff Test for comparing the actual data to randomly generated values from the fitted model.", digits = 4), include.rownames = FALSE)
@

\subsubsection{Generalized Residuals}
By using the pbeta function in R, we can compute generalized residuals in order to assess the model. We did so, and then compared the distribution to a uniform(0, 1) using a Kolmogrov-Smirnoff test statistic simliar to above. Once again, as seen in Table \ref{tbl:gres}, the model seems to be a good fit to the actual data for both groups.

<<gres, results='asis'>>=
gres.1 <- pbeta(meat.data$score1, mu.1 * mle.1$phi, (1 - mu.1) * mle.1$phi)
random.unif.1 <- runif(length(meat.data$score1), 0, 1)
gres.2 <- pbeta(meat.data$score2, mu.2 * mle.2$phi, (1 - mu.2) * mle.2$phi)
random.unif.2 <- runif(length(meat.data$score2), 0, 1)

gres.test1 <- ks.test(gres.1, random.unif.1)
gres.test2 <- ks.test(gres.2, random.unif.2)

my.df <- data.frame(Group = c("Group1", "Group2"), "Statistic" = c(gres.test1$statistic, gres.test2$statistic), "P-Value" = c(gres.test1$p.value, gres.test2$p.value))

print(xtable(my.df, label = "tbl:gres", caption = "Results of the Kolmogrov-Smirnoff Test for comparing the Generalized Residuals of the fitted models to a sample from a uniform(0, 1) distribution", digits = 4), include.rownames = FALSE)
@

\subsubsection{At Day 14}
Of particular interest is day 14, since this is the typical shelf-life for steaks. Hence, it is interesting to see whether the new method, score2, may extend the shelf life beyond the traditional day 14. To assess whether our model describes what is seen in the data well, we will simulate data from the two fitted models and then compare it to the actual proportional score at day 14. Figure \ref{fig:day14} illustrates the results. Note first that the model produces less variability for the first group. The second group has a lower mean/peak, but has a larger upper tail - In other words, the model thinks that at day 14, the quality of the meat is likely to be lower for group 2, but there is significant variability such that there is a higher probability of scores in the upper range, around about 0.6.

<<day14, fig.cap='Simulated proportional scores at day 14 for the two groups. The red and blue lines indicates the true values.', message=FALSE>>=
mu14.1 <- exp(mle.1$par1 + mle.1$par2 * 14) / (exp(mle.1$par1 + mle.1$par2 * 14) + 1)
mu14.2 <- exp(mle.2$par1 + mle.2$par2 * 14) / (exp(mle.2$par1 + mle.2$par2 * 14) + 1)

sim14.1 <-  rbeta(100, shape1=mu14.1*mle.1$phi, shape2=(1 - mu14.1)*mle.1$phi)
sim14.2 <-  rbeta(100, shape1=mu14.2*mle.2$phi, shape2=(1 - mu14.2)*mle.2$phi)

p1 <- qplot(sim14.1, geom = "histogram") + geom_vline(xintercept = meat.data[28,2], colour = "red") + ggtitle("Simulated Proportional Scores at Day 14 for Groups 1 and 2") + xlim(c(0, 1)) + xlab("Proportional Score")
p2 <- qplot(sim14.2, geom = "histogram") + geom_vline(xintercept = meat.data[28,3], colour = "blue") + xlim(c(0, 1)) + xlab("Proportional Score")

grid.arrange(p1, p2, nrow = 2)
@
