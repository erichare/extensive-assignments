\section{Model Formulation}

Let $Y_{ij}$ be the proportional score of steak i in group j, i = 1,..,50 j = 1,2 \\
Let $X_{ij}$ be the storage time for steak i,j \\
Assume $Y_{ij}$ is independent over i and j. \\ \\

We assume $Y_{ij} \sim \text{Beta}(\alpha_{ij}, \beta_{ij})$, and hence we can write the density as: \\

\begin{align*}
f(y_{ij} | \alpha_{ij},\beta_{ij}) = \frac{\Gamma(\alpha_{ij} + \beta_{ij})}{\Gamma(\alpha_{ij})\Gamma(\beta_{ij})}y_{ij}^{\alpha_{ij} - 1}(1 - y_{ij})^{\beta_{ij} - 1}, \quad 0 < y_{ij} < 1, \quad \alpha_{ij} > 0, \quad \beta_{ij} > 0
\end{align*}

We can reparameterize this by noting the expected value and the variance of a Beta random variable. Letting $\mu_{ij} = \frac{\alpha_{ij}}{\alpha_{ij} + \beta_{ij}}$ and $\phi_j = \alpha_{ij} + \beta_{ij}$ (Note that $\phi_j$ is constant within group j because we are holding $\alpha_{ij} + \beta_{ij}$ fixed), we have: \\

\begin{align*}
f(y_{ij} | \mu_{ij}, \phi_j) = \frac{\Gamma(\phi_j)}{\Gamma(\mu_{ij}\phi_{ij})\Gamma((1 - \mu_{ij})\phi_j)}y_{ij}^{\mu_{ij}\phi_j - 1}(1 - y_{ij})^{(1 - \mu_{ij})\phi_j - 1}, \quad 0 < y_{ij} < 1, \quad 0 < \mu_{ij} < 1, \quad \phi_j > 0
\end{align*}

We can calculate the expected value and the variance of $Y_{ij}$ given this new parameterization as:

\begin{align*}
E(Y_{ij})   &= \frac{\alpha_{ij}}{\alpha_{ij} + \beta_{ij}} \\
          &= \frac{\alpha_{ij}}{\phi_j} \\
          &= \mu_{ij} \\ \\
Var(Y_{ij}) &= \frac{\alpha_{ij}\beta_{ij}}{(\alpha_{ij} + \beta_{ij})^2(\alpha_{ij} + \beta_{ij} + 1)} \\
          &= \frac{\mu_{ij}(1 - \mu_{ij})}{\phi_j + 1}
\end{align*}

It may also be desired to write $\alpha_{ij}$ and $\beta_{ij}$ in terms of the parameters $\mu_{ij}$ and $\phi_j$ for future clarity:


\begin{align*}
\alpha_{ij} &= \mu_{ij}\phi_j \\
\beta_{ij}  &= (1 - \mu_{ij})\phi_j
\end{align*}

We are going to fit a regression to each group separately. The likelihood for group j is: \\

\begin{align*}
L_j(\mu_{ij}, \phi_j) &= \prod_{i=1}^n f(y_{ij} | \mu_{ij},\phi_j) \\
                  &= \prod_{i=1}^n \frac{\Gamma(\phi_j)}{\Gamma(\mu_{ij}\phi_j)\Gamma((1 - \mu_{ij})\phi_j)}y_{ij}^{\mu_{ij}\phi_j - 1}(1 - y_{ij})^{(1 - \mu_{ij})\phi_j - 1}
\end{align*}

The log-likelihood for group j is: \\

\begin{align*}
\ell_j(\mu_{ij}, \phi_j) &= \sum_{i=1}^n \log f(y_{ij} | \mu_{ij},\phi_j) \\
  &= \sum_{i=1}^n \log(\Gamma(\phi_j)) - \log(\Gamma(\mu_{ij}\phi_j)) - \log(\Gamma((1 - \mu_{ij})\phi_j)) + (\mu_{ij}\phi - 1)\log(y_{ij}) + ((1 - \mu_{ij})\phi_j - 1)\log(1 - y_{ij})
\end{align*}

Although the beta distribution cannot be written in terms of an exponential dispersion family, we will attempt to mimick GLMs in hopes of observing how a beta regression differs from a typical generalized linear model. To do so, we will use a link function which maps the mean $\mu_{ij}$ onto the real line, a logit link, i.e.,

\begin{align*}
\mu_{ij} &= h(x_{ij}, \beta_{kj}) \\
      &= \frac{e^{(\beta_{0j} + \beta_{1j}x_{ij})}}{e^{(\beta_{0j} + \beta_{1j}x_{ij})} + 1}
\end{align*}

\section{Likelihood Analysis}

We will use a newton-raphson algorithm in order to maximize the log-likelihood in $\beta$. We will profile out $\phi$ for each of the two groups to obtain maximum likelihood estimates for all of the regression parameters.

<<newtraph_beta>>=
y <- meat.data$score1
x <- meat.data$time
phi <- 22.4064558

beta.fn <- function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(gamma(phi) / (gamma(mu * phi) * gamma((1 - mu) * phi)) * y^(mu*phi - 1) * (1 - y)^((1 - mu) * phi - 1))
}

beta.loglik <- function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(sum(lgamma(phi) - lgamma(mu * phi) - lgamma(phi * (1 - mu)) + (mu * phi - 1) * log(y) + ((1 - mu) * phi - 1) * log(1 - y)))
}

beta.gradient <- function(par, y, x, phi) {
    T1 <- exp(par[1] + par[2] * x)
    mu <- 1 / (1 + exp(-(par[1] + par[2] * x)))
    
    dldmu <- -phi * digamma(mu * phi) + phi * digamma(phi - mu*phi) + phi * log(y) - phi * log(1 - y)
    dmudb0 <- T1 / (T1 + 1)^2
    dmudb1 <- x*T1 / (T1 + 1)^2
    
    dldb0 <- sum(dldmu * dmudb0)
    dldb1 <- sum(dldmu * dmudb1)
    
    return(c(dldb0, dldb1))
}

beta.hessian <- function(par, y, x, phi) {
    T1 <- exp(par[1] + par[2] * x)
    mu <- 1 / (1 + exp(-(par[1] + par[2] * x)))
    
    dldmu <- -phi * digamma(mu * phi) + phi * digamma(phi - mu*phi) + phi * log(y) - phi * log(1 - y)
    dmudb0 <- T1 / (T1 + 1)^2
    dmudb1 <- x*T1 / (T1 + 1)^2
    
    dldb0 <- sum(dldmu * dmudb0)
    dldb1 <- sum(dldmu * dmudb1)
    
    d2ldmu2 <- -phi^2 * trigamma(mu * phi) - phi^2 * trigamma(phi - mu * phi)
    d2mudb02 <- -T1 * (T1 - 1) / (T1 + 1)^3
    d2mudb12 <- -x^2 * T1 * (T1 - 1) / (T1 + 1)^3
    d2mudb0db1 <- -x * T1 * (T1 - 1) / (T1 + 1)^3
    
    d2ldb02 <- sum(d2ldmu2 * dmudb0 * dmudb0 + d2mudb02 * dldmu)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldmu2 * dmudb0 * dmudb1 + d2mudb0db1 * dldmu)
    d2ldb12 <- sum(d2ldmu2 * dmudb1 * dmudb1 + d2mudb12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb1db0, d2ldb12), nrow = 2, byrow=TRUE))
}
@

\subsection{Derivatives}

Note: For notational convenience, we will drop the j subscript indexing group and assume that these derivatives are written in terms of a group j.

  \subsubsection{First Derivatives}
  \begin{align*}
  \frac{\partial \ell_i}{\partial \beta_k} &=  \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \beta_k}   \quad \text{for $k=0,1$}
  \end{align*}
  \begin{align*}
   \frac{\partial \ell_i}{\partial \mu_i} &= -\phi\Gamma'(\mu_i\phi) + \phi\Gamma'(\phi - \mu_i\phi) + \phi\log{(y_i)} - \phi\log{(1 - y_i)} \\
   \frac{\partial \mu_i}{\partial \beta_0} &= \frac{e^{\beta_0 + \beta_1x_i}}{(e^{\beta_0 + \beta_1x_i} + 1)^2} \\
   \frac{\partial \mu_i}{\partial \beta_1} &= \frac{x_ie^{\beta_0 + \beta_1x_i}}{(e^{\beta_0 + \beta_1x_i} + 1)^2} \\
  \end{align*}
  
  \subsubsection{Second Derivatives}
    \begin{align*}
    \frac{\partial^2 \ell_i}{\partial \beta_k^2} &= \frac{\partial^2 \ell_i}{\partial \mu_i^2}\frac{\partial \mu_i}{\partial \beta_k} +\frac{\partial^2 \mu_i}{\partial \beta_k^2}\frac{\partial \ell_i}{\partial \mu_i}  \quad \text{for $k=0,1$} \\
    \frac{\partial^2 \ell_i}{\partial \beta_0\beta_1} &= \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial \mu_i}{\partial \beta_0} \frac{\partial \mu_i}{\partial \beta_1}+ \frac{\partial^2 \mu_i}{\partial \beta_0 \partial \beta_1}\frac{\partial \ell_i}{\partial \mu_i} =  \frac{\partial^2 \ell_i}{\partial \beta_1\beta_0} \\
    \frac{\partial^2 \ell_i}{\partial \mu_i^2} &= -\phi^2\Gamma''(\mu_i\phi) - \phi^2\Gamma''(\phi - \mu_i\phi) \\
    \frac{\partial^2 \mu_i}{\partial \beta_0^2} &= -\frac{e^{(\beta_0 + \beta_1x_i)}(e^{(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \frac{\partial^2 \mu_i}{\partial \beta_1^2} &= -x_i^2\frac{e^{(\beta_0 + \beta_1x_i)}(e^{(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \frac{\partial^2 \mu_i}{\partial \beta_0\beta_1^2} &= -x_i\frac{e^{(\beta_0 + \beta_1x_i)}(e^{-(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \end{align*}
    
\subsection{Newton-Raphson}
We wrote a newton-raphson function which follows the algorithm from STAT 520. The code is reproduced below:

<<newtraph, echo=TRUE>>=
newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-2, 3), max.iterations = 100, step.halving = TRUE, debug = FALSE, ...) {
    current <- start
    conditions <- TRUE
    
    iterations <- 0
    while (TRUE) {        
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        if(!(any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3])) break;
        
        if (debug) cat(paste("Current loglik is", loglik(current, ...), "\n"))
        if (debug) cat(paste("New is now", new, "\n"))
        if (debug) cat(paste("New loglik is", loglik(new, ...), "\n"))
        
        if (step.halving & (loglik(new, ...) < loglik(current, ...))) {
            if (debug) cat("Uh oh, time to fix this\n")
            m <- 1
            while (m < max.iterations & loglik(new, ...) < loglik(current, ...)) {
                new <- as.vector(current - (1 / (2 * m)) * solve(hessian(current, ...)) %*% gradient(current, ...))
                m <- 2*m;
            }
            if (debug) cat(paste("We have fixed it! its now", new, "\n"))
            if (debug) cat(paste("And the new loglik is finally", loglik(new, ...), "\n"))
        }
        
        iterations <- iterations + 1
        if (iterations > max.iterations) {
            if (debug) cat(paste("Didn't converge in", max.iterations, "iterations\n"))
            break;
        }
                
        if (debug) cat("\n")
        
        current <- new
    }
    
    return(list(loglik = loglik(new, ...), par = new))
}
@

<<profile_code>>=
y <- meat.data$score1
phis <- seq(21, 23, by = 0.01)
profile.beta1 <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.1 <- profile.beta1[which.max(profile.beta1$loglik), ]

y <- meat.data$score2
phis <- seq(6, 8, by = 0.01)
profile.beta2 <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.2 <- profile.beta2[which.max(profile.beta2$loglik), ]

y <- c(meat.data$score1, meat.data$score2)
x <- c(meat.data$time, meat.data$time)
phis <- seq(6, 8, by = 0.01)
profile.betacomb <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.comb <- profile.betacomb[which.max(profile.betacomb$loglik), ]
@

<<profile_plots, fig.cap='Profile plots of the value of the log-likelihood at particular values of phi for each of the two groups'>>=
p1 <- qplot(profile.beta1$phi, profile.beta1$loglik, geom = "line", colour = I("red")) + theme(aspect.ratio = 1) + geom_vline(xintercept = mle.1$phi) + xlab("phi1") + ylab("Log-Likelihood")
p2 <- qplot(profile.beta2$phi, profile.beta2$loglik, geom = "line", colour = I("blue")) + theme(aspect.ratio = 1) + geom_vline(xintercept = mle.2$phi) + xlab("phi2") + ylab("Log-Likelihood")

grid.arrange(p1, p2, ncol = 2)
@

The MLEs obtained by this procedure are given in Table \ref{tbl:mles}.

<<mle_table, results='asis'>>=
my.df <- data.frame(Group = c("Group1", "Group2", "Combined"), beta0 = c(mle.1$par1, mle.2$par1, mle.comb$par1), beta1 = c(mle.1$par2, mle.2$par2, mle.comb$par2), phi = c(mle.1$phi, mle.2$phi, mle.comb$phi))

print(xtable(my.df, digits = 4, label = "tbl:mles", caption = "MLEs for the first group, second group, and the two groups combined."), include.rownames = FALSE)
@

\subsection{Comparison to betareg Package}

We discovered a package that claims to perform beta regression on CRAN, called {\it betareg}. It uses a similar parameterization, but uses {\it optim} rather than a Newton-based method for optimization. We fit a beta regression using this package to each group, and to the combined data, and obtained the results given in \ref{tbl:mlesbreg}. The results are very similar.

<<betareg, echo=FALSE, results='asis'>>=
breg1 <- betareg(score1 ~ time, data = meat.data)
breg2 <- betareg(score2 ~ time, data = meat.data)

meat.data.comb <- data.frame(score = c(meat.data$score1, meat.data$score2), time = rep(meat.data$time, 2))
bregcomb <- betareg(score ~ time, data = meat.data.comb)

my.df <- data.frame(Group = c("Group1", "Group2", "Combined"), beta0 = c(coef(breg1)[1], coef(breg2)[1], coef(bregcomb)[1]), beta1 = c(coef(breg1)[2], coef(breg2)[2], coef(bregcomb)[2]), phi = c(coef(breg1)[3], coef(breg2)[3], coef(bregcomb)[3]))

print(xtable(my.df, digits = 4, label = "tbl:mlesbreg", caption = "MLEs for the first group, second group, and the two groups combined as given by the betareg package on CRAN."), include.rownames = FALSE)
@

\subsection{Regression Line Comparison}
To test whether regressions are the same for each group, we will use a likelihood ratio test, comparing a reduced model consisting of only a single regression line to describe the full set of data, to a full model with separate regression lines for the two scores. We compute $-2ln(\lambda)$ where $\lambda = \frac{\text{Likelihood maximized under reduced model}}{\text{Likelihood maximized under full model}}$. We obtain:

\begin{align*}
X = -2ln(\lambda) &= -2(\Sexpr{mle.comb$loglik} - (\Sexpr{mle.1$loglik} + \Sexpr{mle.2$loglik})) \\
                  &= \Sexpr{-2 * (mle.comb$loglik - (mle.1$loglik + mle.2$loglik))}
\end{align*}

Under the null, $X \sim \chi_2^2$, so at $\alpha = 0.05$ this yields a p-value of \Sexpr{1 - pchisq(-2 * (mle.comb$loglik - (mle.1$loglik + mle.2$loglik)), df = 2)}. Hence, clearly separate regression lines provide a better fit for the data. Let's assess in what way they do so. Figure \ref{fig:reg_plot} shows the three expectation functions for score1, score2, and the combined data in red, blue, and purple respectively. It also overlays the actual data points for score1 and score2.

<<reg_plot, fig.cap='Plot of the data for score1 (red), score2 (blue), and the expectation functions for each regression line, and the combined regression line expectation (purple).'>>=
x <- seq(0.5, 30, by = 0.5)
y <- beta.fn(par = c(mle.1$par1, mle.1$par2), y=meat.data$score1, x = meat.data$time, phi=mle.1$phi)

mu.1 <- exp(mle.1$par1 + mle.1$par2 * meat.data$time) / (exp(mle.1$par1 + mle.1$par2 * meat.data$time) + 1)
mu.2 <- exp(mle.2$par1 + mle.2$par2 * meat.data$time) / (exp(mle.2$par1 + mle.2$par2 * meat.data$time) + 1)
mu.comb <- exp(mle.comb$par1 + mle.comb$par2 * meat.data$time) / (exp(mle.comb$par1 + mle.comb$par2 * meat.data$time) + 1)

qplot(meat.data$time, mu.1, geom = "line", colour = I("darkred"), size = I(2), alpha = I(0.6)) +
    geom_point(data = meat.data, aes(x = time, y = score1), colour = "red", size = 3) +
    geom_line(data = data.frame(x = meat.data$time, y = mu.2), inherit.aes = FALSE, aes(x = x, y = y), colour = "darkblue", size = 2, alpha = 0.6) + 
    geom_point(data = meat.data, aes(x = time, y = score2), colour = "blue", size = 3) +
    geom_line(data = data.frame(x = meat.data$time, y = mu.comb), inherit.aes = FALSE, aes(x = x, y = y), colour = "purple", size = 2, alpha = 0.6)
@

We can also consider the distribution at a fixed time value of 14, for day 14. 

<<time14>>=
y <- seq(0.01, 0.99, by = 0.01)

x <- 1
vals.11 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.21 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p1 <- qplot(y, vals.11, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.21), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.21), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

x <- 7
vals.12 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.22 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p2 <- qplot(y, vals.12, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.22), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.22), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

x <- 14
vals.13 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.23 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p3 <- qplot(y, vals.13, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.23), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.23), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

x <- 21
vals.14 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.24 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p4 <- qplot(y, vals.14, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.24), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.24), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
@

\subsection{Model Assessment}

\subsubsection{Chi-Square}

To assess the model, we can use a Chi-Square distribution to compare the actual data to the postulated model. We compute:

\begin{align*}
d = \sum_{i=1}^n\frac{(y_i - \hat{\mu_i})^2}{\hat{Var(\mu_i)}}
\end{align*}

Noting that $\hat{Var(\mu_i)} = \hat{\mu_i}(1 - \hat{\mu_i})$:

<<chisq>>=
var.1 <- mu.1 * (1 - mu.1)
var.2 <- mu.2 * (1 - mu.2)
d.1 <- sum((meat.data$score1 - mu.1)^2 / var.1)
d.2 <- sum((meat.data$score2 - mu.2)^2 / var.2)
@

<<chisq_table, results='asis'>>=
pval.1 <- 1 - pchisq(d.1, df=(nrow(meat.data)-1))
pval.2 <- 1 - pchisq(d.2, df=(nrow(meat.data)-1))

my.df <- data.frame(TestStats = c(d.1, d.2), PValues = c(pval.1, pval.2))

xtable(my.df)
@

\subsubsection{Kolmogorov Statistic}
We can simulate from our reference beta distributions by recalling the back-transform for $\alpha_i$ and $\beta_i$, which are: \\

$\alpha_i = \mu_i\phi$ \\
$\beta_i = (1 - \mu_i)\phi$ \\

<<sim_ref>>=
simvals.1 <- rbeta(nrow(meat.data), shape1=mu.1*mle.1$phi, shape2=(1 - mu.1)*mle.1$phi)
simvals.2 <- rbeta(nrow(meat.data), shape1=mu.2*mle.2$phi, shape2=(1 - mu.2)*mle.2$phi)

ks.test(meat.data$score1, simvals.1)
ks.test(meat.data$score2, simvals.2)
@

\subsubsection{Generalized Residuals}
<<gres>>=
gres.1 <- pbeta(meat.data$score1, mu.1 * mle.1$phi, (1 - mu.1) * mle.1$phi)
random.unif.1 <- runif(length(meat.data$score1), 0, 1)
gres.2 <- pbeta(meat.data$score2, mu.2 * mle.2$phi, (1 - mu.2) * mle.2$phi)
random.unif.2 <- runif(length(meat.data$score2), 0, 1)

ks.test(gres.1, random.unif.1)
ks.test(gres.2, random.unif.2)
@

