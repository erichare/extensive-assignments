\section{Likelihood Analysis}

\subsection{Model Formulation}

Let $Y_{ij}$ be the proportional score of steak i in group j, i = 1,..,50 j = 1,2 \\
Let $X_{ij}$ be the storage time for steak i,j \\
Assume $Y_{ij}$ is independent over i and j. \\ \\

We assume $Y_{ij} \sim \text{Beta}(\alpha, \beta)$, and hence we can write the density as: \\

\begin{align*}
f(y | \alpha,\beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}y^{\alpha - 1}(1 - y)^{\beta - 1}, \quad 0 < y < 1, \quad \alpha > 0, \quad \beta > 0
\end{align*}

We can reparameterize this by noting the expected value and the variance of a Beta random variable. Letting $\mu_i = \frac{\alpha}{\alpha + \beta}$ and $\phi = \alpha + \beta$, we have: \\

\begin{align*}
f(y_i | \mu_i,\phi) = \frac{\Gamma(\phi)}{\Gamma(\mu_i\phi)\Gamma((1 - \mu_i)\phi)}y_i^{\mu_i\phi - 1}(1 - y_i)^{(1 - \mu_i)\phi - 1}, \quad 0 < y_i < 1, \quad 0 < \mu_i < 1, \quad \phi > 0
\end{align*}

\begin{align*}
E(Y)   &= \frac{\alpha}{\alpha + \beta} \\
       &= \mu \\ \\
Var(Y) &= \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)} \\
       &= \frac{\mu_i(1 - \mu_i)}{\phi + 1}
\end{align*}

The likelihood is: \\

\begin{align*}
L &= \prod_{i=1}^n f(y_i | \mu_i,\phi) \\
  &= \prod_{i=1}^n \frac{\Gamma(\phi)}{\Gamma(\mu_i\phi)\Gamma((1 - \mu_i)\phi)}y_i^{\mu_i\phi - 1}(1 - y_i)^{(1 - \mu_i)\phi - 1}
\end{align*}

The log-likelihood is: \\

\begin{align*}
\ell(\mu_i, \phi) &= \sum_{i=1}^n \log f(y_i | \mu_i,\phi) \\
  &= \sum_{i=1}^n \log(\Gamma(\phi)) - \log(\Gamma(\mu_i\phi)) - \log(\Gamma((1 - \mu_i)\phi)) + (\mu_i\phi - 1)\log(y_i) + ((1 - \mu_i)\phi - 1)\log(1 - y_i)
\end{align*}

<<optim>>=
y <- meat.data$score1
x <- meat.data$time
phi <- 22.4064558

beta.fn <- function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(gamma(phi) / (gamma(mu * phi) * gamma((1 - mu) * phi)) * y^(mu*phi - 1) * (1 - y)^((1 - mu) * phi - 1))
}

beta.loglik <- function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(sum(lgamma(phi) - lgamma(mu * phi) - lgamma(phi * (1 - mu)) + (mu * phi - 1) * log(y) + ((1 - mu) * phi - 1) * log(1 - y)))
}

optim(c(5, -0.5), fn=beta.loglik, y = y, x = x, phi = phi, control=list(fnscale = -1), hessian=TRUE)
@

<<newtraph_beta>>=
beta.gradient <- function(par, y, x, phi) {
    T1 <- exp(par[1] + par[2] * x)
    mu <- 1 / (1 + exp(-(par[1] + par[2] * x)))
    
    dldmu <- -phi * digamma(mu * phi) + phi * digamma(phi - mu*phi) + phi * log(y) - phi * log(1 - y)
    dmudb0 <- T1 / (T1 + 1)^2
    dmudb1 <- x*T1 / (T1 + 1)^2
    
    dldb0 <- sum(dldmu * dmudb0)
    dldb1 <- sum(dldmu * dmudb1)
    
    return(c(dldb0, dldb1))
}

beta.hessian <- function(par, y, x, phi) {
    T1 <- exp(par[1] + par[2] * x)
    mu <- 1 / (1 + exp(-(par[1] + par[2] * x)))
    
    dldmu <- -phi * digamma(mu * phi) + phi * digamma(phi - mu*phi) + phi * log(y) - phi * log(1 - y)
    dmudb0 <- T1 / (T1 + 1)^2
    dmudb1 <- x*T1 / (T1 + 1)^2
    
    dldb0 <- sum(dldmu * dmudb0)
    dldb1 <- sum(dldmu * dmudb1)
    
    d2ldmu2 <- -phi^2 * trigamma(mu * phi) - phi^2 * trigamma(phi - mu * phi)
    d2mudb02 <- -T1 * (T1 - 1) / (T1 + 1)^3
    d2mudb12 <- -x^2 * T1 * (T1 - 1) / (T1 + 1)^3
    d2mudb0db1 <- -x * T1 * (T1 - 1) / (T1 + 1)^3
    
    d2ldb02 <- sum(d2ldmu2 * dmudb0 * dmudb0 + d2mudb02 * dldmu)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldmu2 * dmudb0 * dmudb1 + d2mudb0db1 * dldmu)
    d2ldb12 <- sum(d2ldmu2 * dmudb1 * dmudb1 + d2mudb12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb1db0, d2ldb12), nrow = 2, byrow=TRUE))
}
@


\subsection{Derivatives}
  \subsubsection{First Derivatives}
  \begin{align*}
  \frac{\partial \ell_i}{\partial \beta_j} &=  \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \beta_j}   \quad \text{for $j=0,1$}
  \end{align*}
  \begin{align*}
   \frac{\partial \ell_i}{\partial \mu_i} &= -\phi\Gamma'(\mu_i\phi) + \phi\Gamma'(\phi - \mu_i\phi) + \phi\log{(y_i)} - \phi\log{(1 - y_i)} \\
   \frac{\partial \mu_i}{\partial \beta_0} &= \frac{e^{\beta_0 + \beta_1x_i}}{(e^{\beta_0 + \beta_1x_i} + 1)^2} \\
   \frac{\partial \mu_i}{\partial \beta_1} &= \frac{x_ie^{\beta_0 + \beta_1x_i}}{(e^{\beta_0 + \beta_1x_i} + 1)^2} \\
  \end{align*}
  
  \subsubsection{Second Derivatives}
    \begin{align*}
    \frac{\partial^2 \ell_i}{\partial \beta_j^2} &= \frac{\partial^2 \ell_i}{\partial \mu_i^2}\frac{\partial \mu_i}{\partial \beta_j} +\frac{\partial^2 \mu_i}{\partial \beta_j^2}\frac{\partial \ell_i}{\partial \mu_i}  \quad \text{for $j=0,1$} \\
    \frac{\partial^2 \ell_i}{\partial \beta_0\beta_1} &= \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial \mu_i}{\partial \beta_0} \frac{\partial \mu_i}{\partial \beta_1}+ \frac{\partial^2 \mu_i}{\partial \beta_0 \partial \beta_1}\frac{\partial \ell_i}{\partial \mu_i} =  \frac{\partial^2 \ell_i}{\partial \beta_1\beta_0} \\
    \frac{\partial^2 \ell_i}{\partial \mu_i^2} &= -\phi^2\Gamma''(\mu_i\phi) - \phi^2\Gamma''(\phi - \mu_i\phi) \\
    \frac{\partial^2 \mu_i}{\partial \beta_0^2} &= -\frac{e^{(\beta_0 + \beta_1x_i)}(e^{(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \frac{\partial^2 \mu_i}{\partial \beta_1^2} &= -x_i^2\frac{e^{(\beta_0 + \beta_1x_i)}(e^{(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \frac{\partial^2 \mu_i}{\partial \beta_0\beta_1^2} &= -x_i\frac{e^{(\beta_0 + \beta_1x_i)}(e^{-(\beta_0 + \beta_1x_i)} - 1)}{(e^{(\beta_0 + \beta_1x_i)})^3} \\
    \end{align*}
    
    <<newtraph, echo=TRUE>>=
newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-2, 3), max.iterations = 100, step.halving = TRUE, debug = FALSE, ...) {
    current <- start
    conditions <- TRUE
    
    iterations <- 0
    while (TRUE) {        
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        if(!(any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3])) break;
        
        if (debug) cat(paste("Current loglik is", loglik(current, ...), "\n"))
        if (debug) cat(paste("New is now", new, "\n"))
        if (debug) cat(paste("New loglik is", loglik(new, ...), "\n"))
        
        if (step.halving & (loglik(new, ...) < loglik(current, ...))) {
            if (debug) cat("Uh oh, time to fix this\n")
            m <- 1
            while (m < max.iterations & loglik(new, ...) < loglik(current, ...)) {
                new <- as.vector(current - (1 / (2 * m)) * solve(hessian(current, ...)) %*% gradient(current, ...))
                m <- 2*m;
            }
            if (debug) cat(paste("We have fixed it! its now", new, "\n"))
            if (debug) cat(paste("And the new loglik is finally", loglik(new, ...), "\n"))
        }
        
        iterations <- iterations + 1
        if (iterations > max.iterations) {
            if (debug) cat(paste("Didn't converge in", max.iterations, "iterations\n"))
            break;
        }
                
        if (debug) cat("\n")
        
        current <- new
    }
    
    return(list(loglik = loglik(new, ...), par = new))
}
@

<<profile_code>>=

y <- meat.data$score1
phis <- seq(21, 23, by = 0.01)
profile.beta1 <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.1 <- profile.beta1[which.max(profile.beta1$loglik), ]

y <- meat.data$score2
phis <- seq(6, 8, by = 0.01)
profile.beta2 <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.2 <- profile.beta2[which.max(profile.beta2$loglik), ]

y <- c(meat.data$score1, meat.data$score2)
x <- c(meat.data$time, meat.data$time)
phis <- seq(6, 8, by = 0.01)
profile.betacomb <- ldply(phis, function(phi) {
    c(phi = phi, unlist(newton.raphson(beta.loglik, beta.gradient, beta.hessian, start = c(2, -0.2), y = y, x = x, phi = phi)))
})
mle.comb <- profile.betacomb[which.max(profile.betacomb$loglik), ]
@


\subsection{Comparison to betareg Package}

<<betareg, echo=FALSE>>=
betareg(score1 ~ time, data = meat.data)
betareg(score2 ~ time, data = meat.data)

meat.data.comb <- data.frame(score = c(meat.data$score1, meat.data$score2), time = rep(meat.data$time, 2))
betareg(score ~ time, data = meat.data.comb)
@

\subsection{Regression Line Comparison}
To test whether regressions are the same for each group, we will use a likelihood ratio test, comparing a reduced model consisting of only a single regression line to describe the full set of data, to a full model with separate regression lines for the two scores. We compute $-2ln(\lambda)$ where $\lambda = \frac{\text{Likelihood maximized under reduced model}}{\text{Likelihood maximized under full model}}$. We obtain:

\begin{align*}
X = -2ln(\lambda) &= -2(\Sexpr{mle.comb$loglik} - (\Sexpr{mle.1$loglik} + \Sexpr{mle.2$loglik})) \\
                  &= \Sexpr{-2 * (mle.comb$loglik - (mle.1$loglik + mle.2$loglik))}
\end{align*}

Under the null, $X \sim \chi_2^2$, so at $\alpha = 0.05$ this yields a p-value of \Sexpr{1 - pchisq(-2 * (mle.comb$loglik - (mle.1$loglik + mle.2$loglik)), df = 2)}. Hence, clearly separate regression lines provide a better fit for the data. Let's assess in what way they do so. Figure \ref{fig:reg_plot} shows the three expectation functions for score1, score2, and the combined data in red, blue, and purple respectively. It also overlays the actual data points for score1 and score2.

<<reg_plot, fig.cap='Plot of the data for score1 (red), score2 (blue), and the expectation functions for each regression line, and the combined regression line expectation (purple).'>>=
x <- seq(0.5, 30, by = 0.5)
y <- beta.fn(par = c(mle.1$par1, mle.1$par2), y=meat.data$score1, x = meat.data$time, phi=mle.1$phi)

mu.1 <- exp(mle.1$par1 + mle.1$par2 * meat.data$time) / (exp(mle.1$par1 + mle.1$par2 * meat.data$time) + 1)
mu.2 <- exp(mle.2$par1 + mle.2$par2 * meat.data$time) / (exp(mle.2$par1 + mle.2$par2 * meat.data$time) + 1)
mu.comb <- exp(mle.comb$par1 + mle.comb$par2 * meat.data$time) / (exp(mle.comb$par1 + mle.comb$par2 * meat.data$time) + 1)

qplot(meat.data$time, mu.1, geom = "line", colour = I("darkred"), size = I(2), alpha = I(0.6)) +
    geom_point(data = meat.data, aes(x = time, y = score1), colour = "red", size = 3) +
    geom_line(data = data.frame(x = meat.data$time, y = mu.2), inherit.aes = FALSE, aes(x = x, y = y), colour = "darkblue", size = 2, alpha = 0.6) + 
    geom_point(data = meat.data, aes(x = time, y = score2), colour = "blue", size = 3) +
    geom_line(data = data.frame(x = meat.data$time, y = mu.comb), inherit.aes = FALSE, aes(x = x, y = y), colour = "purple", size = 2, alpha = 0.6)
@

We can also consider the distribution at a fixed time value of 14, for day 14. 

<<time14>>=
y <- seq(0.01, 0.99, by = 0.01)

x <- 1
vals.11 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.21 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p1 <- qplot(y, vals.11, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.21), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.21), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

x <- 7
vals.12 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.22 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p2 <- qplot(y, vals.12, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.22), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.22), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

x <- 14
vals.13 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.23 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p3 <- qplot(y, vals.13, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.23), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.23), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

x <- 21
vals.14 <- beta.fn(c(mle.1$par1, mle.1$par2), y, x, mle.1$phi)
vals.24 <- beta.fn(c(mle.2$par1, mle.2$par2), y, x, mle.2$phi)

p4 <- qplot(y, vals.14, geom = "point", colour = I("red")) + geom_line(colour = I("red")) +
    geom_point(data = data.frame(x = y, y = vals.24), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue") +
    geom_line(data = data.frame(x = y, y = vals.24), inherit.aes = FALSE, aes(x = x, y = y), colour = "blue")

grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
@


\subsection{Model Assessment}

