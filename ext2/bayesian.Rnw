\section{Bayesian Approach}

\subsection{Model Formulation}

Again, let $Y_{ij}$ be the proportional score of steak $i$ in group $j$, $i = 1,..,50$, $j = 1,2$, and suppose that the $Y_i$s follow a Beta($\alpha,\beta$) distribution. Define $\mu_i$ and $\phi$ the same way as in section 1.1.  Then, the Bayesian model formulation of $Y_i$ for each value of $j$ is
\begin{align*}
y_i & \sim \text{Beta}(\mu_i\phi,(1-\mu_i)\phi) \\
\text{logit}(\mu_i) & = \beta_0 +\beta_1 x_i \\
\phi & \sim \text{Gamma}(0.01,0.01) \\
\beta_0,\beta_1 & \sim \text{Uniform}(-50,50)
\end{align*}
Note that the priors on $\phi,\beta_0,\beta_1$ are diffuse, and assume that $\phi$, $\beta_0$, and $\beta_1$ are independent. We chose the uniform priors on $\beta_1,\beta_2$ because there was no obvious conjugate choice for them. We chose the prior on $\phi$ to be Gamma because that is the same choice given by Branscum, Johnson, and Thurmond in their paper.\footnote{BRANSCUM, ADAM J., JOHNSON, WESLEY O., \& THURMOND, MARK C. (2007).  Bayesian Beta Regression: Applications to Household Expenditure Data and Genetic Distance between Foot-and-Mouth Disease Viruses. \textit{Australian \& New Zealand Journal of Statistics}. \textbf{49}(3), 287-301.} The joint posterior of interest here is
\begin{align*}
p(\phi,\beta_0,\beta_1 | \underaccent{\tilde}{y},\underaccent{\tilde}{x}) & \propto f(\underaccent{\tilde}{y}|\beta_0,\beta_1,\phi) \pi(\phi)\pi(\beta_0)\pi(\beta_1)
\end{align*}

We sample from this distribution by using Metropolis-within-Gibbs sampling, using a Metropolis step to sample from the full conditionals of $(\beta_0,\beta_1)$ and $\phi$, which are given below by 
\begin{align*}
p(\beta_0,\beta_1 |  \underaccent{\tilde}{y},\underaccent{\tilde}{x}, \phi)  & \propto \prod_{i=1}^n \frac{\Gamma(\phi)}{\Gamma(\mu_{i}\phi)\Gamma((1 - \mu_{i})\phi)}y_{i}^{\mu_{i}\phi - 1}(1 - y_{i})^{(1 - \mu_{i})\phi - 1} \\
p(\phi |  \underaccent{\tilde}{y},\underaccent{\tilde}{x}, \beta_0,\beta_1) & \propto \left[\prod_{i=1}^n \frac{\Gamma(\phi)}{\Gamma(\mu_{i}\phi)\Gamma((1 - \mu_{i})\phi)}y_{i}^{\mu_{i}\phi - 1}(1 - y_{i})^{(1 - \mu_{i})\phi - 1}\right] \times \phi^{0.01-1}e^{-0.01\phi}
\end{align*}
The code for our Metropolis-within-Gibbs sampler is given below. Note that we are using a normal proposal distribution for $\beta_0,\beta_1$ and a symmetric proposal distribution for $\phi$.  We chose a symmetric proposal distribution for $\phi$ because the Gamma distribution does not have a mean parameter like the normal distribution, so it was harder and more variable to incorporate the the previous sampling value into the proposal distribution.    

<<metro.in.gibbs,echo=TRUE>>=
#likelihood function
beta.loglik<-function(par, y, x, phi) {
    mu <- exp(par[1] + par[2] * x) / (exp(par[1] + par[2] * x) + 1)
    
    return(sum((mu * phi - 1) * log(y) + ((1 - mu) * phi - 1) * log(1 - y)-lbeta(mu*phi,(1-mu)*phi)))
}
#priors for phi (no prior for beta0,beta1 since they're uniform)
prior.phi<-function(phi,a=0.01,b=0.01){
  a*log(b)-lgamma(a) + (a-1)*log(phi)-b*phi
}
#Metropolis Hastings steps for beta, phi
sample_beta <- function(beta0, y, x, phi0,sigma.b){
  beta_star<-c(0,0)
  beta_star[1] <- rnorm(1,beta0[1],sigma.b) #proposal dists
  beta_star[2] <- rnorm(1,beta0[2],sigma.b)
  
  f_bstar<-beta.loglik(beta_star, y, x, phi0)
  f_b<-beta.loglik(beta0, y, x, phi0)
  
  varmat<-matrix(c(sigma.b,0,0,sigma.b),nrow=2)
  
  q_bstar<-log(det(varmat)^(-1/2)*exp((-1/2)*t(beta_star-beta0)%*%solve(varmat)%*%(beta_star-beta0)))
  q_b<-log(det(varmat)^(-1/2)*exp((-1/2)*t(beta0-beta_star)%*%solve(varmat)%*%(beta0-beta_star)))
  
  log_rho_b<-min((f_bstar - f_b) + (q_b - q_bstar) , 0)
  if (is.nan(log_rho_b)){
      log_rho_b <- 0
    }
  
  if (log(runif(1)) <= log_rho_b){
    acc.new = 1
  }
   else acc.new = 0
  
  if (acc.new == 1){
    return(beta_star)
  }
  else return(beta0)
}
sample_phi <- function(y,x,phi0,beta0){
  phi_star<-rgamma(1,shape=10,rate=.45) #symmetric proposal

  f_pstar<-beta.loglik(beta0, y, x, phi_star)+prior.phi(phi_star)
  f_p<-beta.loglik(beta0, y, x, phi0)+prior.phi(phi0)
  
  log_rho_p<-min((f_pstar - f_p),0) 
    if (is.nan(log_rho_p)){
      log_rho_p <- 0
    }
  p<-log(runif(1))
 if (p <= log_rho_p){
    acc.new.p = 1
  }
  else acc.new.p = 0
  
  if (acc.new.p == 1){
    return(phi_star)
  }
  else return(phi0)
}
#Final Metropolis-within-Gibbs MCMC sampler to draw samples from the joint posterior distribution
run_mcmc = function(y, x, beta0, phi0, n.reps=1e3, tune=TRUE, sigma.b=10) {
  beta_keep = matrix(0,ncol=2,nrow=n.reps); beta_keep[1,] = beta = beta0
  phi_keep =  matrix(0,nrow=n.reps); phi_keep[1,] = phi = phi0
  
  for (i in 1:n.reps) {
    # Automatically tune beta var
    beta_old <- beta
    beta <- sample_beta(beta_old,y,x,phi0,sigma.b) #Metropolis step
    if (tune==TRUE) {
       if (sum(beta==beta_old)==2) {  #if next draw is rejected then make var smaller  (accept prob too low)
         sigma.b = sigma.b/1.1 
       } else {
         sigma.b = sigma.b*1.1  #increase var to get more of the density sampled  (accept prob too high)
       }
     }
    
    phi_old <- phi
    phi <- sample_phi(y,x,phi_old,beta) #Metropolis step
    
    beta_keep[i,]  = beta
    phi_keep[i,]   = phi
    }
  return(list(beta=beta_keep,phi=phi_keep,sigma.b))  
}
@
We ran the sampler for 3 chains of 10,000 draws each for the score1, score2, and combined groups.  The posterior medians for the parameter values are given below: 
<<run_mcmc>>=
one_group<-data.frame(cbind(rep(meat.data$time,2),c(meat.data$score1,meat.data$score2)))
names(one_group)<-c('time','score')
y<-meat.data$score1
y2<-meat.data$score2
x<-meat.data$time
#score 1
run1.score1<-run_mcmc(y,x,c(1,-1),20,n.reps=1e4)
run2.score1<-run_mcmc(y,x,c(2,-.5),25,n.reps=1e4)
run3.score1<-run_mcmc(y,x,c(3,-3),15,n.reps=1e4)
betas.score1<-rbind(run1.score1[[1]],run2.score1[[1]],run3.score1[[1]])
phi.score1<-rbind(run1.score1[[2]],run2.score1[[2]],run3.score1[[2]])
#score 2
run1.score2<-run_mcmc(y2,x,c(1,-1),20,n.reps=1e4)
run2.score2<-run_mcmc(y2,x,c(2,-.5),25,n.reps=1e4)
run3.score2<-run_mcmc(y2,x,c(3,-3),15,n.reps=1e4)
betas.score2<-rbind(run1.score2[[1]],run2.score2[[1]],run3.score2[[1]])
phi.score2<-rbind(run1.score2[[2]],run2.score2[[2]],run3.score2[[2]])
#combined groups
one_group<-data.frame(cbind(rep(meat.data$time,2),c(meat.data$score1,meat.data$score2)))
names(one_group)<-c('time','score')
y3<-one_group$score
x3<-one_group$time
run1.both<-run_mcmc(y3,x3,c(1,-1),20,n.reps=1e4)
run2.both<-run_mcmc(y3,x3,c(2,-.5),25,n.reps=1e4)
run3.both<-run_mcmc(y3,x3,c(3,-3),15,n.reps=1e4)
betas.both<-rbind(run1.both[[1]],run2.both[[1]],run3.both[[1]])
phi.both<-rbind(run1.both[[2]],run2.both[[2]],run3.both[[2]])
@
<<table_medians,results='asis'>>=
#plots of draws
library(xtable)
meds.score1<-c(median(betas.score1[,1]),median(betas.score1[,2]),median(phi.score1))
meds.score2<-c(median(betas.score2[,1]),median(betas.score2[,2]),median(phi.score2))
meds.both<-c(median(betas.both[,1]),median(betas.both[,2]),median(phi.both))
medians<-data.frame(rbind(meds.score1,meds.score2,meds.both))
rownames(medians)<-c('Group1','Group2','Combined')
names(medians)<-c('beta0','beta1','phi')

medians<-medians
print(xtable(medians))
@
The posterior medians, which we are using as the parameter estimates throughout this section, are similar to the MLEs, but they do not match as closely as we would like. This is especially true for the estimates of $\phi$.
<<plots,warning=FALSE,message=FALSE,fig.cap='Posterior Distributions of the Parameter Values'>>=
library(ggplot2)
library(gridExtra)
p1 <- qplot(betas.score1[,1], geom = "histogram", binwidth = 0.05,
    main = "beta0 - score 1") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(betas.score1[,1]),color='red')
p2 <- qplot(betas.score1[,2], geom = "histogram", binwidth = 0.01,
    main = "beta1 - score 1") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(betas.score1[,2]),color='red')+xlim(-1,-.2)
p3 <- qplot(phi.score1, geom = "histogram", binwidth = .5,
    main = "phi - score 1") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(phi.score1),color='red')
p4 <- qplot(betas.score2[,1], geom = "histogram", binwidth = 0.05,
    main = "beta0 - score 2") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(betas.score2[,1]),color='red')
p5 <- qplot(betas.score2[,2], geom = "histogram", binwidth = 0.01,
    main = "beta1 - score 2") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(betas.score2[,2]),color='red')+xlim(-1,-.2)
p6 <- qplot(phi.score2, geom = "histogram", binwidth = .5,
    main = "phi - score 2") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(phi.score2),color='red')

p7 <- qplot(betas.both[,1], geom = "histogram", binwidth = 0.1,
    main = "beta0 - both scores") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(betas.both[,1]),color='red')
p8 <- qplot(betas.both[,2], geom = "histogram", binwidth = 0.01,
    main = "beta1 - both scores") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(betas.both[,2]),color='red')+xlim(-1,-.2)
p9 <- qplot(phi.both, geom = "histogram", binwidth = .25,
    main = "phi - both scores") + theme(text = element_text(size = 10)) + geom_vline(xintercept=median(phi.both),color='red')
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9)
@
\subsection{Model Checking}
<<post.simulation>>=
#quantities of interest: # less than .10, # greater than .90, value at time 14

new_scores<-function(x,par){
  mus<-exp(par[1]+par[2]*x) / (exp(par[1]+par[2]*x)+1)
  phi<-par[3]
  y.new<-rbeta(length(x),mus*phi,(1-mus)*phi)
  return(y.new)
}
post.sim.score1<-matrix(0,nrow=1e4,ncol=50)
for (i in 1:1e4){
  post.sim.score1[i,]<-new_scores(x,meds.score1)
}
post.sim.score2<-matrix(0,nrow=1e4,ncol=50)
for (i in 1:1e4){
  post.sim.score2[i,]<-new_scores(x,meds.score2)
}
post.sim.both<-matrix(0,nrow=1e4,ncol=100)
for (i in 1:1e4){
  post.sim.both[i,]<-new_scores(x3,meds.both)
}

lowvals.score1<-apply(post.sim.score1,1,function(x) length(which(x<=.1)))
hivals.score1<-apply(post.sim.score1,1,function(x) length(which(x>=.9)))
score1.14<-post.sim.score1[,28]
tstat.lvals.score1<-length(which(y<=.1))
tstat.hvals.score1<-length(which(y>=.9))
tstat.score1.14<-meat.data[28,2]
pval1.score1<-length(which(lowvals.score1>=tstat.lvals.score1))/length(lowvals.score1)
pval2.score1<-length(which(hivals.score1<=tstat.hvals.score1))/length(hivals.score1)
pval3.score1<-length(which(score1.14>=tstat.score1.14))/length(score1.14)

lowvals.score2<-apply(post.sim.score2,1,function(x) length(which(x<=.1)))
hivals.score2<-apply(post.sim.score2,1,function(x) length(which(x>=.9)))
score2.14<-post.sim.score2[,28]
tstat.lvals.score2<-length(which(y2<=.1))
tstat.hvals.score2<-length(which(y2>=.9))
tstat.score2.14<-meat.data[28,3]
pval1.score2<-length(which(lowvals.score2>=tstat.lvals.score2))/length(lowvals.score2)
pval2.score2<-length(which(hivals.score2<=tstat.hvals.score2))/length(hivals.score2)
pval3.score2<-length(which(score2.14>=tstat.score2.14))/length(score2.14)


lowvals.both<-apply(post.sim.both,1,function(x) length(which(x<=.1)))
hivals.both<-apply(post.sim.both,1,function(x) length(which(x>=.9)))
both.14<-c(post.sim.both[,28],post.sim.both[,78])
tstat.lvals.both<-length(which(y3<=.1))
tstat.hvals.both<-length(which(y3>=.9))
tstat.both.14<-mean(c(tstat.score1.14,tstat.score2.14))
pval1.both<-length(which(lowvals.both>=tstat.lvals.both))/length(lowvals.both)
pval2.both<-length(which(hivals.both<=tstat.hvals.both))/length(hivals.both)
pval3.both<-length(which(both.14>=tstat.both.14))/length(both.14)
@
<<post.plots,fig.cap='Values of the quantities of interest posterior predictive samples'>>=
h1<-qplot(lowvals.score1,geom='bar',binwidth=1,main='Number <= 0.1 in Predictive Samples')+geom_vline(xintercept=tstat.lvals.score1,color='red')+theme(text = element_text(size = 7))
h2<-qplot(hivals.score1,geom='bar',binwidth=1,main='Number >= 0.9 in Predictive Samples')+geom_vline(xintercept=tstat.hvals.score1,color='red')+theme(text = element_text(size = 7))
h3<-qplot(score1.14,geom='histogram',binwidth=.02,main='Score at time 14 in Predictive Samples')+geom_vline(xintercept=tstat.score1.14,color='red')+theme(text = element_text(size = 7))

h4<-qplot(lowvals.score2,geom='bar',binwidth=1,main='Number <= 0.1 in Predictive Samples')+geom_vline(xintercept=tstat.lvals.score2,color='red')+theme(text = element_text(size = 7))
h5<-qplot(hivals.score2,geom='bar',binwidth=1,main='Number >= 0.9 in Predictive Samples')+geom_vline(xintercept=tstat.hvals.score2,color='red')+theme(text = element_text(size = 7))
h6<-qplot(score2.14,geom='histogram',binwidth=.02,main='Score at time 14 in Predictive Samples')+geom_vline(xintercept=tstat.score2.14,color='red')+theme(text = element_text(size = 7))

h7<-qplot(lowvals.both,geom='bar',binwidth=1,main='Number <= 0.1 in Predictive Samples')+geom_vline(xintercept=tstat.lvals.both,color='red')+theme(text = element_text(size = 7))
h8<-qplot(hivals.both,geom='bar',binwidth=1,main='Number >= 0.9 in Predictive Samples')+geom_vline(xintercept=tstat.hvals.both,color='red')+theme(text = element_text(size = 7))
h9<-qplot(both.14,geom='histogram',binwidth=.02,main='Score at time 14 in Predictive Samples')+geom_vline(xintercept=tstat.both.14,color='red')+theme(text = element_text(size = 7))

grid.arrange(h1,h2,h3,h4,h5,h6,h7,h8,h9)
@

<<pvals.posts,results='asis'>>=
pvalues<-data.frame(rbind(c(pval1.score1,pval2.score1,pval3.score1), c(pval1.score2,pval2.score2,pval3.score2),c(pval1.both,pval2.both,pval3.both)))
names(pvalues)<-c('<=0.1','>=0.9','Score.14')
rownames(pvalues)<-c('Group1','Group2','Combined')
print(xtable(pvalues))
@


<<jags.check, cache=TRUE,warning=FALSE,message=FALSE,results='hide'>>=
model<-"model{
  for (i in 1:N){
    y[i]~dbeta(alpha[i],beta[i])
    alpha[i] <- mu[i]*phi
    beta[i] <- (1-mu[i])*phi
    logit(mu[i]) <- beta0+beta1*x[i]
    }
  beta0 ~ dunif(-50,50)
  beta1 ~ dunif(-50,50)
  phi ~ dgamma(0.01,0.01)
}"
dat1 <- list(N = nrow(meat.data), y = meat.data$score1, x = meat.data$time)
m1 <- jags.model(textConnection(model),dat1,n.chains=3, n.adapt=1000)
res1<- coda.samples(m1,c('beta0','beta1','phi'),n.iter=5000) 
diag1<-gelman.diag(res1)

betas.score1.jags<-rbind(res1[[1]][,1:2],res1[[2]][,1:2],res1[[3]][,1:2])
phi.score1.jags<-rbind(res1[[1]][,3],res1[[2]][,3],res1[[3]][,3])

dat2 <- list(N = nrow(meat.data), y = meat.data$score2, x = meat.data$time)
m2 <- jags.model(textConnection(model),dat2,n.chains=3, n.adapt=1000)
res2<- coda.samples(m2,c('beta0','beta1','phi'),n.iter=10000) 
diag2<-gelman.diag(res2)

betas.score2.jags<-rbind(res2[[1]][,1:2],res2[[2]][,1:2],res2[[3]][,1:2])
phi.score2.jags<-rbind(res2[[1]][,3],res2[[2]][,3],res2[[3]][,3])

dat3<- list(N = nrow(one_group), y = one_group$score, x = one_group$time)
m3 <- jags.model(textConnection(model),dat3,n.chains=3, n.adapt=1000)
res3<-coda.samples(m3,c('beta0','beta1','phi'),n.iter=10000) 
diag3<-gelman.diag(res3)

betas.both.jags<-rbind(res3[[1]][,1:2],res3[[2]][,1:2],res3[[3]][,1:2])
phi.both.jags<-rbind(res3[[1]][,3],res3[[2]][,3],res3[[3]][,3])

medians.jags<-data.frame(rbind(c(median(betas.score1.jags[,1]),median(betas.score1.jags[,2]),median(phi.score1.jags)),
c(median(betas.score2.jags[,1]),median(betas.score2.jags[,2]),median(phi.score2.jags)),
c(median(betas.both.jags[,1]),median(betas.both.jags[,2]),median(phi.both.jags))))
names(medians.jags)<-c('beta0','beta1','phi')
medians.jags$Group<-c('Score1','Score2','Combined')
medians.jags<-medians.jags[,c(4,1:3)]
@
We compared this to running the same model in the rjags package.  The JAGS posterior medians were much closer to the MLEs than our MCMC posterior medians.  
<<jags_medians,results='asis'>>=
print(xtable(medians.jags))
@
      


