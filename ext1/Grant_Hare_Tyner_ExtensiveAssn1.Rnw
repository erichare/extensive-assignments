\documentclass[11pt]{article}

\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{amsmath,amssymb,accents}

\begin{document}
<<concordance, echo=FALSE>>=
opts_chunk$set(concordance=TRUE, echo=FALSE, tidy=TRUE)
opts_knit$set(self.contained=FALSE)
@

<<libraries, cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE>>=
library(pscl)
library(plyr)
library(ggplot2)
library(reshape2)
library(xtable)
library(dplyr)
@

\setlength{\parskip}{3ex}
\setlength{\parindent}{0pt}

\title{Extensive Assignment 1}
\author{Millicent Grant, Eric Hare, Samantha Tyner}

\maketitle

\clearpage

<<readthedata, echo=FALSE>>=
bean.data <- read.table("greenbeandat.txt", header = TRUE)

## Remove unnecessary columns
## Exclude stores in 7000s based on MLEs being ridiculous
bean.data <- bean.data[,-c(2, 6, 7)]
bean.data <- subset(bean.data, store < 7000)

bean.indiv <- subset(bean.data, store == 1039)
@

\setcounter{page}{1}

\section{Exploratory Analysis}

<<price_plot, message=FALSE>>=
qplot(mvm, data = bean.indiv, geom = "histogram") + facet_wrap(~price)
@

\section{Newton-Raphson Algorithm}
In the spirit of conducting as much of this analysis ``by-hand" as possible, we elected to write our own Newton-Raphson function based on the STAT 520 course notes. This function accepts four required parameters, a function which returns the value of the log likelihood for given parameters which we are interested in maximizing, functions that return the values of the gradient and hessian respectly, and then starting values for the parameters that these functions accept.

Other parameters include:
\begin{itemize}
    \item lower - Lower bounds of the parameter space
    \item upper - Upper bounds of the parameter space
    \item tol - The three tolerance values for which we end the algorithm upon reaching
    \item max.iterations - The maximum number of iterations of the algorithm before terminating and returning
    \item step.halving - A boolean indicating whether to perform the step-halving procedure
    \item debug - A boolean indicating whether to print debug messages
\end{itemize}

This function will return the Maximum Likelihood Estimates of the parameters given in the loglik, gradient, and hessian functions. The full code is reproduced below.

<<newtraph, echo=TRUE>>=
newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-2, 3), max.iterations = 100, step.halving = TRUE, debug = FALSE, ...) {
    current <- start
    conditions <- TRUE
    
    iterations <- 0
    while (TRUE) {        
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        if(!(any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3])) break;
        
        if (debug) cat(paste("Current loglik is", loglik(current, ...), "\n"))
        if (debug) cat(paste("New is now", new, "\n"))
        if (debug) cat(paste("New loglik is", loglik(new, ...), "\n"))
        
        if (step.halving & (loglik(new, ...) < loglik(current, ...))) {
            if (debug) cat("Uh oh, time to fix this\n")
            m <- 1
            while (m < max.iterations & loglik(new, ...) < loglik(current, ...)) {
                new <- as.vector(current - (1 / (2 * m)) * solve(hessian(current, ...)) %*% gradient(current, ...))
                m <- 2*m;
            }
            if (debug) cat(paste("We have fixed it! its now", new, "\n"))
            if (debug) cat(paste("And the new loglik is finally", loglik(new, ...), "\n"))
        }
        
        iterations <- iterations + 1
        if (iterations > max.iterations) {
            if (debug) cat(paste("Didn't converge in", max.iterations, "iterations\n"))
            break;
        }
                
        if (debug) cat("\n")
        
        current <- new
    }
    
    return(list(loglik = loglik(new, ...), par = new))
}
@

We will use this function to determine the MLEs for the three models presented henceforth.

\section{Simple Poisson}

\subsection{Model Formulation}

As a baseline, consider a simple poisson regression model. Define random variables $\{Y_{i}: i = 1,..., n\}$ associated with the quantity of number of units of green beans sold on day $i$ in an individual store at a fixed price.  In this case, $i$ indexes days that the store was selling green bean units for a fixed price $x$.  Let the distribution of $Y_{i}$ be Poisson. Then,
\begin{align*}
f(y_{i}|\lambda_i) = \frac{e^{-\lambda_i}\lambda_i^{y_{i}}}{y_{i}!}, y_{i} > 0
\end{align*}
and
\begin{align*}
L(\lambda_i) &= \prod_{i = 1}^n f(y_{i}| \lambda_i) \\
&= \prod_{i = 1}^n \frac{e^{-\lambda_i}\lambda_i^{y_{i}}}{y_{i}!} \\
&= e^{-\lambda_i}\lambda_i^{\sum_{i = 1}^{n} y_{i}} \prod_{i = 1}^{n} \frac{1}{y_{i}!}
\end{align*}
Now, we will use a log-link to model the systematic component. In other words, $\lambda_i = e^{\delta_{0} + \delta_{1}x_{i}}$. Then,

\begin{align*}
\ell_i(\lambda_i) &= y_i\log{(\lambda_i)} - \lambda_i - \log{(y_i!)}
\end{align*}

\subsection{Maximum Likelihood Estimation}

\subsubsection{First Derivatives}

\begin{align*}
\frac{\partial \ell(\lambda_i)}{\partial \lambda_i} &= \frac{y_i}{\lambda_i} - 1 \\
\frac{\partial \lambda_i}{\partial \delta_{0}} &= e^{\delta_{0} + \delta_{1}x_{i}} \\
\frac{\partial \lambda_i}{\partial \delta_{1}} &= x_ie^{\delta_{0} + \delta_{1}x_{i}} \\ \\
\frac{\partial \ell(\lambda_i)}{\partial \delta_{0}} &= \frac{\partial \ell(\lambda_i)}{\partial \lambda_i}\frac{\partial \lambda_i}{\partial \delta_{0}} \\
\frac{\partial \ell(\lambda_i)}{\partial \delta_{1}} &= \frac{\partial \ell(\lambda_i)}{\partial \lambda_i}\frac{\partial \lambda_i}{\partial \delta_{1}}
\end{align*}

\subsubsection{Second Derivatives}

\begin{align*}
\frac{\partial^2 \ell(\lambda_i)}{\partial \lambda_i^{2}} &= -\frac{y_i}{\lambda_i^2} \\
\frac{\partial^2 \lambda_i}{\partial \delta_{0}^2} &= e^{\delta_{0} + \delta_{1}x_{i}} \\
\frac{\partial^2 \lambda_i}{\partial \delta_{1}^2} &= x_i^2e^{\delta_{0} + \delta_{1}x_{i}} \\
\frac{\partial^2 \lambda_i}{\partial \delta_{0}\delta_{1}} &= x_ie^{\delta_{0} + \delta_{1}x_{i}} \\ \\
\frac{\partial^2 \ell(\lambda_i)}{\partial \delta_{0}^{2}} &= \frac{\partial^2 \ell(\lambda_i)}{\partial \lambda_i^{2}}\frac{\partial \lambda_i}{\partial \delta_{0}}\frac{\partial \lambda_i}{\partial \delta_{0}} + \frac{\partial^2 \lambda_i}{\partial \delta_{0}^2}\frac{\partial \ell(\lambda_i)}{\partial \lambda_i} \\
\frac{\partial^2 \ell(\lambda_i)}{\partial \delta_{1}^{2}} &= \frac{\partial^2 \ell(\lambda_i)}{\partial \lambda_i^{2}}\frac{\partial \lambda_i}{\partial \delta_{1}}\frac{\partial \lambda_i}{\partial \delta_{1}} + \frac{\partial^2 \lambda_i}{\partial \delta_{1}^2}\frac{\partial \ell(\lambda_i)}{\partial \lambda_i} \\
\frac{\partial^2 \ell(\lambda_i)}{\partial \delta_{0}\delta_{1}} &= \frac{\partial^2 \ell(\lambda_i)}{\partial \lambda_i^{2}}\frac{\partial \lambda_i}{\partial \delta_{0}}\frac{\partial \lambda_i}{\partial \delta_{1}} + \frac{\partial^2 \lambda_i}{\partial \delta_{0}\delta_{1}}\frac{\partial \ell(\lambda_i)}{\partial \lambda_i}
\end{align*}

The MLEs for the regression parameters for the first nine stores in the data are given in Table \ref{tbl:sip.table}. As expected, in general the sales tends to decrease as the price increases, although the amount to which this is true varies across stores. In Figure \ref{fig:sip.plot}, the expectation function for the first nine stores, along with the actual frequencies of sales is displayed.

<<sip_newtraph>>=
sip.fn <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    
    val <- T1^y * exp(-T1) / factorial(y)
    val[is.nan(val)] <- 0
    
    return(val)
}

sip.loglik <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    lambda <- exp(d0 + d1*x)
    
    return(sum(y * log(lambda) - lambda - lfactorial(y)))
}

sip.gradient <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    lambda <- exp(d0 + d1*x)
    
    dldlambda <- y/lambda - 1
    dlambdadd0 <- lambda
    dlambdadd1 <- x*lambda
    
    dldd0 <- sum(dldlambda * dlambdadd0)
    dldd1 <- sum(dldlambda * dlambdadd1)
    
    return(c(dldd0, dldd1))
}

sip.hessian <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    lambda <- exp(d0 + d1*x)
    
    dldlambda <- y/lambda - 1
    dlambdadd0 <- lambda
    dlambdadd1 <- x*lambda
    
    dldd0 <- sum(dldlambda * dlambdadd0)
    dldd1 <- sum(dldlambda * dlambdadd1)
    
    d2ldlambda2 <- -y/lambda^2
    
    d2lambdadd02 <- lambda
    d2lambdadd0dd1 <- x*lambda
    d2lambdadd12 <- (x^2)*lambda
    
    d2ldd02 <- sum(d2ldlambda2 * dlambdadd0 * dlambdadd0 + d2lambdadd02 * dldlambda)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldlambda2 * dlambdadd0 * dlambdadd1 + dldlambda * d2lambdadd0dd1)
    d2ldd12 <- sum(d2ldlambda2 * dlambdadd1 * dlambdadd1 + d2lambdadd12 * dldlambda)
    
    return(matrix(c(d2ldd02, d2ldd0dd1, d2ldd0dd1, d2ldd12), nrow = 2))
}

stores <- unique(bean.data$store)[1:9]
sip.stores <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    sip.newtraph <- newton.raphson(sip.loglik, sip.gradient, sip.hessian, start = c(5, -5), y = y, x = x)
    par.sip.newtraph <- sip.newtraph$par
    
    d0 <- par.sip.newtraph[1]
    d1 <- par.sip.newtraph[2]

    prices <- seq(min(x) - .1, max(x) + .1, by = 0.01)
    
    data.frame(Store = sto, d0 = d0, d1 = d1, Price = prices, PredictedSales = exp(d0 + d1*prices))
})
@

<<sip.table, results='asis'>>=
sip.table <- ddply(sip.stores[,1:3], .(Store), unique)

print(xtable(sip.table, digits = c(0, 0, 6, 6), caption = "MLEs for the first nine stores", label = "tbl:sip.table"), table.position = 'H', include.rownames = FALSE)
@

<<sip.plot, warning=FALSE, fig.cap='Expectation function for the Simple Poisson model for the first 9 stores.'>>=
df1 <- subset(sip.stores, Store %in% stores[1:9])
df2 <- subset(bean.data, store %in% stores[1:9])
df3 <- ddply(df2, .(price, store), summarise, actual_mvm = mean(mvm))
sip.merge <- merge(df1, df3, by.x = c("Price", "Store"), by.y = c("price", "store"))

qplot(Price, PredictedSales, data = sip.merge, geom = "line", facets=~Store, colour = I("red"), size = I(2)) + geom_bar(aes(x = Price, y = actual_mvm), stat = "identity") + xlim(c(0.3, 0.75))
@

\subsection{Comparison to glm}
Because of the use of our own Newton-Raphson function, as well as calculating the hessian and the gradient by hand, we decided to do a quick comparison to the results obtained by using the glm function in base R. Table \ref{tbl:sip.compare} illustrates the MLEs for the first nine stores that we calculated, and the MLEs that GLM calculated using the poisson family argument. Our results match out to at least six digits.

<<sip.glm, results='asis'>>=
sip.glm.table <- ldply(stores, function(sto) {
    glm.model <- glm(mvm ~ price, data = subset(bean.data, store == sto), family = "poisson")
    glm.par <- glm.model$coefficients
    glm.par <- c(sto, as.numeric(glm.par))
    
    names(glm.par) <- c("Store", "d0_glm", "d1_glm")
    glm.par
})

my.df <- merge(sip.table, sip.glm.table, by = "Store")

print(xtable(my.df, digits = c(0, 0, rep(6, 4)), caption = "Comparison of MLEs to results obtained from glm package", label = "tbl:sip.compare"), table.position = 'H', include.rownames = FALSE)
@

\section{Zero-Inflated Poisson}

\subsection{Model Formulation}

Now, define another set of random variables $\{Z_{i}: i = 1,...,n\}$ associated with consumer interest in green beans. Take these random variables to be independent and identically distributed with a binary probability mass function having parameter $\pi$. So,
\[Z_{i} = \left\{
  \begin{array}{lr}
    1 & : \text{consumer interested}\\
    0 & : \text{consumer not interested}
  \end{array}
\right.
\]
Then, for $0 < p < 1$, 
\[g(z_{i}|\pi) = \left\{
  \begin{array}{lr}
    \pi^{z_{i}}(1 - \pi)^{1 - z_{i}} & : z_{i} = 0, 1 \\
    0 & : \text{otherwise}
  \end{array}
\right.
\]
Define random variables associated with units sold given $Z_{i} = 1$,
\begin{align*}
P(Y_{i} = 0) &= P(Y_{i} = 0 | Z_{i} = 0)P(Z_{i} = 0) + P(Y_{i} = 0 | Z_{i} = 1)P(Z_{i} = 1) \\
&= 1(1 - \pi) + \frac{1}{0!}\lambda^{0}e^{-\lambda}\pi \\
&= (1- \pi) + \pi exp(-\lambda) \hspace{0.2 cm} \text{and} \\
P(Y_{i} = y) &= P(Y_{i} = y | Z_{i} = 0)P(Z_{i} = 0) + P(Y_{i} = y | Z_{i} = 1)P(Z_{i} = 1) \\
&= 0 + \frac{1}{y_{i}!}\lambda^{y_{i}}e^{-\lambda}\pi \\
&= \frac{\pi \lambda^{y_{i}}e^{-\lambda}}{y_{i}!}
\end{align*}
The marginal distribution of $Y_{i}$ leads to the zero-inflated Poisson probability mass function. for $\lambda > 0$ and $0 < \pi < 1$,
\[h(y_{i}|\pi, \lambda) = \left\{
  \begin{array}{lr}
    (1 - \pi) + \pi exp(-\lambda)& : y_{i} = 0 \\
      \pi \lambda^{y_{i}}e^{-\lambda}\frac{1}{y_{i}!} & : y_{i} > 0 \\
      0 & : \hspace{0.1 cm} \text{otherwise}
  \end{array}
\right.
\]
The likelihood and log likelihood is as follows
\begin{align*}
L(\lambda, \pi) &= \prod_{i = 1}^n h(y_{i}| \lambda, \pi) \\
&= \prod_{i = 1}^n [(1 - \pi) + \pi e^{-\lambda}]I(y_{i} = 0) + [\frac{\pi e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}]I(y_{i} > 0) \\
&= [(1 - \pi) + \pi e^{-\lambda}]I(y_{i} = 0) + [\pi e^{-\lambda} \lambda^{\sum^{n}_{i = 1}y_{i}} \prod_{i = 1}^{n} \frac{1}{y_{i}!}]I(y_{i} > 0) \\
\ell_{1}(\lambda, \pi) &= log((1 - \pi) + \pi e^{-\lambda})I(y_{i} = 0) \\
&= log(1 + \pi(e^{-\lambda} - 1))I(y_{i} = 0) \\
\ell_{2}(\lambda, \pi) &= \bigg[log \pi - \lambda + \sum_{i = 1}^{n} y_{i} log \lambda + \sum_{i = 1}^{n} \frac{1}{y_{i}!}\bigg]I(y_{i} > 0) \\
\ell(\lambda, \pi) &= \ell_{1}(\lambda, \pi) + \ell_{2}(\lambda, \pi)
\end{align*}
Now, assume $\pi = \frac{e^{\beta_{0} + \beta_{1}x_{i}}}{1 + e^{\beta_{0} + \beta_{1}x_{i}}}$ and $\lambda = e^{\delta_{0} + \delta_{1}x_{i}}$.  Then, 
\begin{align*}
\ell_{1}(\lambda, \pi) &= log\bigg[1 - \frac{e^{\beta_{0} + \beta_{1}x_{i}}}{1 + e^{\beta_{0} + \beta_{1}x_{i}}}(exp\{-e^{\delta_{0} + \delta_{1}x_{i}}\} - 1)\bigg] \\
\ell_{2}(\lambda, \pi) &= log\bigg(\frac{e^{\beta_{0} + \beta_{1}x_{i}}}{1 + e^{\beta_{0} + \beta_{1}x_{i}}}\bigg) - e^{\delta_{0} + \delta_{1}x_{i}} + \sum_{i = 1}^{n} y_{i} log(e^{\delta_{0} + \delta_{1}x_{i}}) + \sum_{i = 1}^{n} \frac{1}{y_{i}!}\\
\ell(\lambda, \pi) &= \ell_{1}(\lambda, \pi) + \ell_{2}(\lambda, \pi)
\end{align*}
The first partial derivatives are given by
\begin{align*}
\frac{\partial \ell(\lambda, \pi)}{\partial \delta_{0}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \lambda}\frac{\partial \lambda}{\partial \delta_{0}} \\
\frac{\partial \ell(\lambda, \pi)}{\partial \delta_{1}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \lambda}\frac{\partial \lambda}{\partial \delta_{1}} \\
\frac{\partial}{\partial \lambda} \ell(\lambda, \pi) &= (\frac{y_{i}}{\lambda} - 1)I(y_{i} > 0) - \frac{\pi e^{-\lambda}}{(1 - \pi) + \pi e^{-\lambda}}I(y_{i} = 0) \\
\frac{\partial \lambda}{\partial \delta_{0}} &= exp\{\delta_{0} + \delta_{1}x_{i}\} \\
\frac{\partial \lambda}{\partial \delta_{1}} &= x_{i} exp\{\delta_{0} + \delta_{1}x_{i}\} 
\end{align*}

\begin{align*}
\frac{\partial \ell(\lambda, \pi)}{\partial \beta_{0}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \pi}\frac{\partial \pi}{\partial \beta_{0}} \\
\frac{\partial \ell(\lambda, \pi)}{\partial \beta_{1}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \pi}\frac{\partial \pi}{\partial \beta_{1}} \\
\frac{\partial \ell(\lambda, \pi)}{\partial \pi} &= \bigg[\frac{e^{\lambda} - 1}{e^{\lambda}(\pi - 1) - \pi}\bigg]I(y_{i} = 0) + \frac{1}{\pi}I(y_{i > 0})\\
\frac{\partial \pi}{\partial \beta_{0}} &= \frac{e^{\beta_{0} + \beta_{1}x_{i}}}{(e^{\beta_{0} + \beta_{1}x_{i}})^2} \\
\frac{\partial \pi}{\partial \beta_{1}} &= \frac{x_{i}e^{\beta_{0} + \beta_{1}x_{i}}}{(e^{\beta_{0} + \beta_{1}x_{i}})^2}
\end{align*}

The second partial derivatives are given by
\begin{align*}
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \delta_{0}^{2}} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \lambda^{2}}\frac{\partial^{2} \lambda}{\partial \delta_{0}^2} \\
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \delta_{1}^2} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \lambda^{2}}\frac{\partial^{2} \lambda}{\partial \delta_{1}^2} \\
\frac{\partial^{2}}{\partial \lambda^{2}} \ell(\lambda, \pi) &= (\frac{-1}{\lambda^{2}})I(y_{i} > 0) - \frac{1}{[(1 - \pi) + \pi e^{-\lambda}]^2}I(y_{i} > 0) \\
\frac{\partial^{2} \lambda}{\partial \delta_{0}^{2}} &= exp\{\delta_{0} + \delta_{1}x_{i}\} \\
\frac{\partial^2 \lambda}{\partial \delta_{1}^2} &= x_{i}^{2} exp\{\delta_{0} + \delta_{1}x_{i}\} 
\end{align*}

\begin{align*}
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \beta_{0}^{2}} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \pi^{2}}\frac{\partial^{2}\pi}{\partial \beta_{0}^{2}} \\
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \beta_{1}^{2}} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \pi^{2}}\frac{\partial^{2} \pi}{\partial \beta_{1}^{2}} \\
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \pi^{2}} &= \bigg[\frac{e^{\lambda} - 1}{e^{\lambda}(\pi - 1) - \pi}\bigg]^{2}I(y_{i} = 0) + \frac{1}{\pi^{2}}I(y_{i > 0})\\
\frac{\partial^{2} \pi}{\partial \beta_{0}^{2}} &= -e^{-\beta_{1}x_{i} - \beta_{0}} \\
\frac{\partial^{2} \pi}{\partial \beta_{1}^{2}} &= -x_{i}e^{-\beta_{0} - \beta_{1}x_{i}}
\end{align*}

\subsection{Maximum Likelihood Estimation}
<<zip.newtraph>>=
zip.fn <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    pi <- exp(b0 + b1*x) / (1 + exp(b0 + b1*x))
    lambda <- exp(d0 + d1*x)
    
    single <- ((y == 0) * ((1 - pi) + pi*exp(-lambda))) +
              ((y > 0) * (pi * lambda^y * exp(-lambda)) / factorial(y))
    
    single[is.nan(single)] <- 0
    return(single)
}

zip.loglik <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    pi <- exp(b0 + b1*x) / (1 + exp(b0 + b1*x))
    lambda <- exp(d0 + d1*x)
    
    single <- ((y == 0) * log((1 - pi) + pi*exp(-lambda))) +
              ((y > 0) * (log(pi) + y*log(lambda) - lambda))
    
    return(sum(single))
}
    
zip.gradient <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    pi <- T0 / (1 + T0)
    
    dldpi <- ((y == 0) * (exp(-T1) - 1) / (exp(-T1) * pi + 1 - pi)) +
             (y > 0) * (1 / pi)
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2
    
    dldmu <- ((y > 0) * (y/T1 - 1) - (y == 0)*(pi * exp(-T1))/((1 - pi) + pi*exp(-T1)))
    dmudd0 <- T1
    dmudd1 <- x*T1
  
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldb0, dldb1, dldd0, dldd1))
}

zip.hessian <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    pi <- T0 / (1 + T0)
    
    dldpi <- ((y == 0) * (exp(-T1) - 1) / (exp(-T1) * pi + 1 - pi)) +
             (y > 0) * (1 / pi)
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2
    
    dldmu <- ((y > 0) * (y/T1 - 1) - (y == 0)*(pi * exp(-T1))/((1 - pi) + pi*exp(-T1)))
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    d2ldmu2 <- (-y / T1^2)*(y > 0) + (y == 0)*((pi * (1 - pi) * exp(-T1)) / ((1 - pi) + pi*exp(-T1))^2)
    
    d2ldpi2 <- -((exp(T1) - 1) / (exp(T1) * (pi - 1) - pi))^2  * (y == 0) + (y > 0)*(-1 / pi^2)
    d2ldpidmu <- (y == 0) * -exp(-T1) / ((1 - pi) + pi * exp(-T1))^2
    
    d2pidb02 <- (T0 * (1 - T0)) / (1 + T0)^3
    d2pidb12 <- (x^2 * T0 * (1 - T0)) / (1 + T0)^3
    d2pidb0db1 <- (x * T0 * (1 - T0)) / (1 + T0)^3
    
    d2mudd02 <- T1
    d2mudd12 <- x^2 * T1
    d2mudd0dd1 <- x * T1
    
    d2ldb02 <- sum(d2ldpi2 * dpidb0 * dpidb0 + d2pidb02 * dldpi)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldpi2 * dpidb0 * dpidb1 + d2pidb0db1 * dldpi) 
    d2ldb0dd0 <- d2ldd0db0 <- sum(d2ldpidmu * dpidb0 * dmudd0)
    d2ldb0dd1 <- d2ldd1db0 <- sum(d2ldpidmu * dpidb0 * dmudd1)
    d2ldb12 <- sum(d2ldpi2 * dpidb1 * dpidb1 + d2pidb12 * dldpi)
    d2ldb1dd0 <- d2ldd0db1 <- sum(d2ldpidmu * dpidb1 * dmudd0)
    d2ldb1dd1 <- d2ldd1db1 <- sum(d2ldpidmu * dpidb1 * dmudd1)
    d2ldd02 <- sum(d2ldmu2 * dmudd0 * dmudd0 + d2mudd02 * dldmu)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldmu2 * dmudd0 * dmudd1 + d2mudd0dd1 * dldmu)
    d2ldd12 <- sum(d2ldmu2 * dmudd1 * dmudd1 + d2mudd12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb0dd0, d2ldb0dd1, d2ldb1db0, d2ldb12, d2ldb1dd0, d2ldb1dd1, d2ldd0db0, d2ldd0db1, d2ldd02, d2ldd0dd1, d2ldd1db0, d2ldd1db1, d2ldd1dd0, d2ldd12), nrow = 4, byrow = TRUE))
}

stores <- unique(bean.data$store)[1:9]
zip.stores <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    zip.newtraph <- newton.raphson(zip.loglik, zip.gradient, zip.hessian, start = c(5, -5, 5, -5), y = y, x = x)
    par.zip.newtraph <- zip.newtraph$par
    
    b0 <- par.zip.newtraph[1]
    b1 <- par.zip.newtraph[2]
    d0 <- par.zip.newtraph[3]
    d1 <- par.zip.newtraph[4]
    
    prices <- seq(min(x) - .1, max(x) + .1, by = 0.01)
    
    data.frame(Store = sto, b0 = b0, b1 = b1, d0 = d0, d1 = d1, Price = prices, PredictedSales = (exp(b0 + b1*prices) / (1 + exp(b0 + b1*prices))) * (exp(d0 + d1*prices)))
})
@

<<zip.table, results='asis'>>=
zip.table <- ddply(zip.stores[,1:5], .(Store), unique)

print(xtable(zip.table, digits = c(0, 0, 6, 6, 6, 6), caption = "MLEs for the first nine stores", label = "tbl:zip.table"), table.position = 'H')
@

<<zip.plot, warning=FALSE>>=
df1 <- subset(zip.stores, Store %in% stores[1:9])
df2 <- subset(bean.data, store %in% stores[1:9])
df3 <- ddply(df2, .(price, store), summarise, actual_mvm = mean(mvm))
zip.merge <- merge(df1, df3, by.x = c("Price", "Store"), by.y = c("price", "store"))
sip.merge$model <- "sip"
zip.merge$model <- "zip"

test <- cbind(sip.merge[,c("Price", "Store", "PredictedSales", "actual_mvm")], zip.merge$PredictedSales)
names(test)[3:5] <- c("SIP", "Actual", "ZIP")
predict.melt <- melt(test, id.vars = c("Price", "Store", "Actual"))

qplot(Price, value, data = predict.melt, geom = "line", facets=~Store, colour = variable, group = variable, alpha = I(0.4), size = I(3)) +
    geom_bar(aes(x = Price, y = Actual), stat = "identity", inherit.aes = FALSE) +
    theme(legend.position = "bottom")
@

\subsection{Comparison to pscl}

<<zip.pscl, results='asis'>>=
zip.pscl.table <- ldply(stores, function(sto) {
    pscl.zip <- zeroinfl(mvm ~ price, data = subset(bean.data, store == sto), dist = "poisson")

    pscl.par <- c(pscl.zip$coefficients$zero[1], pscl.zip$coefficients$zero[2], pscl.zip$coefficients$count[1], pscl.zip$coefficients$count[2])
    pscl.par <- c(sto, as.numeric(pscl.par))
    names(pscl.par) <- c("Store", "b0_pscl", "b1_pscl", "d0_pscl", "d1_pscl")
    
    pscl.par
})

my.df <- merge(zip.table, zip.pscl.table, by = "Store")

print(xtable(my.df, digits = c(0, 0, rep(6, 8)), caption = "Comparison of MLEs to results obtained from pscl package", label = "tbl:zip.compare"), table.position = 'H', include.rownames = FALSE)
@

\section{Zero-Inflated Gamma-Poisson}

\subsection{Model Formulation}
$Y_i|\lambda_i \sim \text{independent Poisson}(\lambda_i)$ \\
$\lambda_i|\alpha,b_i \sim \text{independent Gamma}(\alpha, b_i)$ \\

\begin{align*}
h(y_i | \alpha, b_i) &= \int f(y_i | \lambda_i, p)g(\lambda_i | \alpha,b_i)d\lambda_i \\
                       &= \frac{b_i^{\alpha}}{y_i!\Gamma{(\alpha)}}\int_0^{\infty} \lambda_i^{y_i}e^{-\lambda_i}\lambda_i^{\alpha - 1}e^{-b_i\lambda_i}d\lambda_i \\
                       &= \frac{b_i^{\alpha}}{y_i!\Gamma{(\alpha)}}\int_0^{\infty} \lambda_i^{y_i + \alpha - 1}e^{-\lambda_i(b_i + 1)}d\lambda_i \\
                       &= \frac{b_i^{\alpha}\Gamma{(\alpha + y_i)}}{y_i!\Gamma{(\alpha)}(b_i + 1)^{\alpha + y_i}}
\end{align*}

\begin{align*}
Pr(Y_i = 0 | \lambda_i, p) &= Pr(Y_i = 0 | Z_i = 0)Pr(Z_i = 0) + Pr(Y_i = 0 | Z_i = 1)Pr(Z_i = 1) \\
            &= Pr(Z_i = 0) + Pr(Y_i = 0 | Z_i = 1)Pr(Z_i = 1) \\
            &= (1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha \\
\end{align*}

\begin{align*}
Pr(Y_i = y | \lambda_i, p) &= Pr(Y_i = y | Z_i = 0)Pr(Z_i = 0) + Pr(Y_i = y | Z_i = 1)Pr(Z_i = 1) \\
            &= Pr(Y_i = y | Z_i = 1)Pr(Z_i = 1) \\
            &= \pi_i\frac{b_i^{\alpha}\Gamma{(\alpha + y_i)}}{y_i!\Gamma{(\alpha)}(b_i + 1)^{\alpha + y_i}} \\
            &= \pi_i{\alpha + y_i - 1 \choose y_i}\frac{b_i^{\alpha}}{(b_i + 1)^{\alpha + y_i}} \\
            &= \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}
\end{align*}

Hence, the full zero-inflated gamma-poisson distribution pmf is: \\

$(1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}I(y_i > 0)$ \\

Once again, we will use a log link for the mean, and a logit link for the consumer interest parameter $\pi_i$, which we derive as follows:

$log(E(Y|x)) = \delta_0 + \delta_1x_i$ \\
$\mu_i = E(Y|x) = e^{\delta_0 + \delta_1x_i} = \frac{\alpha}{b_i}$ \\
$\pi_i = \frac{e^{\beta_0 + \beta_1x_i}}{1 + e^{\beta_0 + \beta_1x_i}}$

Finally, in order to fit the model easily, we will reparameterize this model to include a mean parameter $\mu_i$:

\begin{align*}
L_i(\pi_i,\alpha, b_i) &= (1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}I(y_i > 0) \\
L_i(\pi_i,\mu_i,\alpha)  &= (1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}I(y_i > 0)\\
\ell_i(\pi_i,\mu_i,\alpha) & = (1-k_i)\log\left((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha\right)+k_i\log\left(\pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}\right)\\
\ell(\pi,\mu,\alpha) &= \sum_{i=1}^n (1-k_i)\log\left((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha\right)+k_i\log\left(\pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}\right) 
\end{align*}
where $k_i = 0$ if $y_i = 0$ and $k_i = 1$ if $y_i > 0$.  

<<zigp.newtraph, cache=TRUE>>=
zigp.fn <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha
    
    return((y == 0)*((1 - pi) + pi*T2) + (y > 0)*pi*choose(alpha + y - 1, y)*T2*(mu / (alpha + mu))^y)
}

zigp.loglik <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha
    
    return(sum((1 - (y > 0)) * log((1 - pi) + pi*T2) + ((y > 0) * log(pi * choose(alpha + y - 1, y) * T2 * (mu / (alpha + mu))^y))))
}

zigp.gradient <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha

    dldpi <- (1 - (y > 0)) * (T2 - 1)/((1 - pi) + pi*T2) + ((y > 0) * (1 / pi))
    
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2

    dldmu <- (1 - (y > 0)) * (-pi * (alpha / (alpha + mu))^(alpha + 1)) / ((1 - pi) + pi*T2) + ((y > 0) * (-alpha * (mu - y)) / (mu * (alpha + mu)))
    
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldb0, dldb1, dldd0, dldd1))
}

zigp.hessian <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    
    mu <- T1
    pi <- T0 / (1 + T0)
    
    T2 <- (alpha / (alpha + mu))^alpha
    
    dldpi <- (1 - (y > 0)) * (T2 - 1)/((1 - pi) + pi*T2) + ((y > 0) * (1 / pi))
    
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2

    dldmu <- (1 - (y > 0)) * (-pi * (alpha / (alpha + mu))^(alpha + 1)) / ((1 - pi) + pi*T2) + ((y > 0) * (-alpha * (mu - y)) / (mu * (alpha + mu)))
    
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    d2ldpi2 <- ((T2 - 1)^2 * ((y > 0) - 1) / (T2*pi - pi + 1)^2) + ((y > 0) * -1/pi^2)
    d2pidb02 <- (T0 * (1 - T0)) / (1 + T0)^3
    d2pidb12 <- (x^2 * T0 * (1 - T0)) / (1 + T0)^3
    d2pidb0db1 <- (x * T0 * (1 - T0)) / (1 + T0)^3
    
    piece1 <- (alpha * pi * T2 * ((alpha + 1) * (pi * (T2 - 1) + 1) - pi*(alpha + mu) * (alpha / (alpha + mu))^(alpha + 1))) / ((alpha + mu)^2 * (pi * (T2 - 1) + 1)^2)
    piece2 <- (alpha * (alpha*y - mu^2 + 2*mu*y))/(mu^2 * (alpha + mu)^2)
    d2ldmu2 <- (1 - (y > 0)) * piece1 - (y > 0)*piece2
    
    d2ldpidmu <- ((y > 0) - 1) * ((alpha / (alpha + mu))^(alpha + 1)) / (pi * (T2 - 1) + 1)^2
    
    d2mudd02 <- T1
    d2mudd12 <- x^2 * T1
    d2mudd0dd1 <- x * T1
    
    d2ldb02 <- sum(d2ldpi2 * dpidb0 * dpidb0 + d2pidb02 * dldpi)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldpi2 * dpidb0 * dpidb1 + d2pidb0db1 * dldpi) 
    d2ldb0dd0 <- d2ldd0db0 <- sum(d2ldpidmu * dpidb0 * dmudd0)
    d2ldb0dd1 <- d2ldd1db0 <- sum(d2ldpidmu * dpidb0 * dmudd1)
    d2ldb12 <- sum(d2ldpi2 * dpidb1 * dpidb1 + d2pidb12 * dldpi)
    d2ldb1dd0 <- d2ldd0db1 <- sum(d2ldpidmu * dpidb1 * dmudd0)
    d2ldb1dd1 <- d2ldd1db1 <- sum(d2ldpidmu * dpidb1 * dmudd1)
    d2ldd02 <- sum(d2ldmu2 * dmudd0 * dmudd0 + d2mudd02 * dldmu)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldmu2 * dmudd0 * dmudd1 + d2mudd0dd1 * dldmu)
    d2ldd12 <- sum(d2ldmu2 * dmudd1 * dmudd1 + d2mudd12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb0dd0, d2ldb0dd1, d2ldb1db0, d2ldb12, d2ldb1dd0, d2ldb1dd1, d2ldd0db0, d2ldd0db1, d2ldd02, d2ldd0dd1, d2ldd1db0, d2ldd1db1, d2ldd1dd0, d2ldd12), nrow = 4, byrow = TRUE))
}

alphas <- seq(1, 1.05, by = 0.001)
start <- c(5, -5, 5, -5)
profile.alpha.results <- ldply(alphas, function(alpha) {
    zigp.newtraph <- newton.raphson(zigp.loglik, zigp.gradient, zigp.hessian, start = start, y = bean.data$mvm, x = bean.data$price, alpha = alpha)
        c(alpha = alpha, loglik = zigp.newtraph$loglik, b0 = zigp.newtraph$par[1], b1 = zigp.newtraph$par[2], d0 = zigp.newtraph$par[3], d1 = zigp.newtraph$par[4])
})

zigp.newtraph <- profile.alpha.results[which.max(profile.alpha.results$loglik), ]
par.zigp.newtraph <- as.numeric(zigp.newtraph[1,3:6])
@

We can now proceed to find maximum likelihood estimates for the parameters.

\subsection{Maximum Likelihood}
To fit the model, we will use maximum likelihood estimation to once again estimate the regression parameters $\beta_0, \beta_1, \delta_0$, and $\delta_1$. Now, we also have an $\alpha$ parameter to account for. Our approach this will be to profile out $\alpha$. That is, maximimize $\ell(\beta_0, \beta_1, \delta_0, \delta_1)$ for a range of values of $\alpha$ and select the one that maximizes the log-likelihood. We will again use our Newton-Raphson algorithm to accomplish this.

  \subsubsection{First Derivatives}
  \begin{align*}
  \frac{\partial \ell_i}{\partial \beta_j} & =  \frac{\partial \ell_i}{\partial \pi_i} \frac{\partial \pi_i}{\partial \beta_j}   \quad \text{for $j=0,1$}\\
   \frac{\partial \ell_i}{\partial \delta_j} & =  \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \delta_j}  \quad \text{for $i = 0,1$}
  \end{align*}
  \begin{align*}
   \frac{\partial \ell_i}{\partial \pi_i}   & = (1-k_i) \cdot \frac{\left((\frac{\alpha}{\alpha+\mu_i})^\alpha - 1\right)}{(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha} + k_i \cdot \frac{1}{ \pi_i} \\
 \frac{\partial \pi_i}{\partial \beta_0} & = \frac{e^{\beta_0+\beta_1x_i}}{(1+e^{\beta_0+\beta_1x_i})^2} \\
  \frac{\partial \pi_i}{\partial \beta_1} & = \frac{x_ie^{\beta_0+\beta_1x_i}}{(1+e^{\beta_0+\beta_1x_i})^2} \\
  \frac{\partial \ell_i}{\partial \mu_i} & = (1-k_i)\cdot \frac{-\pi_i(\frac{\alpha}{\alpha+\mu_i})^{\alpha+1}}{(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha} + k_i \cdot -\frac{\alpha(\mu_i-y_i)}{\mu_i(\alpha+\mu_i)} \\
   \frac{\partial \mu_i}{\partial \delta_0} & = e^{\delta_0+\delta_1 x_i} \\
   \frac{\partial \mu_i}{\partial \delta_1} & = x_ie^{\delta_0+\delta_1 x_i}
  \end{align*}
  
  \subsubsection{Second Derivatives}
  \begin{align*}
    \frac{\partial^2 \ell_i}{\partial \beta_j^2} & = \frac{\partial^2 \ell_i}{\partial \pi_i^2}\frac{\partial \pi_i}{\partial \beta_j} +\frac{\partial^2 \pi_i}{\partial \beta_j^2}\frac{\partial \ell_i}{\partial \pi_i}  \quad \text{for $j=0,1$} \\
    \frac{\partial^2 \ell_i}{\partial \beta_0\beta_1} & = \frac{\partial^2 \ell_i}{\partial \pi_i^2} \frac{\partial \pi_i}{\partial \beta_0} \frac{\partial \pi_i}{\partial \beta_1}+ \frac{\partial^2 \pi_i}{\partial \beta_0 \partial \beta_1}\frac{\partial \ell_i}{\partial \pi_i} =  \frac{\partial^2 \ell_i}{\partial \beta_1\beta_0} \\
        \frac{\partial^2 \ell_i}{\partial \delta_j^2} & = \frac{\partial^2 \ell_i}{\partial \mu_i^2}\frac{\partial \mu_i}{\partial \delta_j} +\frac{\partial^2 \mu_i}{\partial \delta_j^2}\frac{\partial \ell_i}{\partial \mu_i}  \quad \text{for $j=0,1$} \\
	  \frac{\partial^2 \ell_i}{\partial \delta_0\delta_1} & = \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial \mu_i}{\partial \delta_0} \frac{\partial \mu_i}{\partial \delta_1}+ \frac{\partial^2 \mu_i}{\partial \delta_0 \partial \delta_1}\frac{\partial \ell_i}{\partial \mu_i} =  \frac{\partial^2 \ell_i}{\partial \delta_1\delta_0} \\
	  \frac{\partial^2 \ell_i}{\partial \beta_j\delta_k}& = \frac{\partial^2 \ell_i}{\partial \pi_i \partial \mu_i} \frac{\partial \pi_i}{\partial \beta_j}\frac{\partial \mu_i}{\partial \delta_k}  \quad \text{for $j,k =0,1$}
	    \end{align*}
  \begin{align*}
 \frac{\partial^2 \ell_i}{\partial \pi_i^2}  & = (1-k_i) \cdot \frac{-\left((\frac{\alpha}{\alpha+\mu_i})^\alpha - 1\right)^2}{[(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha]^2} - k_i \cdot \frac{1}{ \pi_i^2} \\
  \frac{\partial^2 \pi_i}{\partial \beta_0^2} & = \frac{(1+e^{\beta_0+\beta_1x_i})^2e^{\beta_0+\beta_1x_i} - 2(e^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
  	& = \frac{e^{\beta_0+\beta_1x_i} (1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
   \frac{\partial^2 \pi_i}{\partial \beta_1^2} & = \frac{(1+e^{\beta_0+\beta_1x_i})^2x_i^2e^{\beta_0+\beta_1x_i} - 2(x_ie^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
   		& = \frac{x_i^2e^{\beta_0+\beta_1x_i}(1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
  \frac{\partial^2 \pi_i}{\partial \beta_0 \partial \beta_1} & = \frac{(1+ e^{\beta_0+\beta_1x_i})^2x_i e^{\beta_0+\beta_1x_i} - 2x_i(e^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
  	& = \frac{x_ie^{\beta_0+\beta_1x_i}(1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
   \frac{\partial^2 \ell_i}{\partial \mu_i^2} & = (1-k_i) \left\{\alpha\mu_i (\frac{\alpha}{\alpha+\mu_i})^\alpha (\alpha+1) \{\pi_i [(\frac{\alpha}{\alpha+\mu_i})^\alpha-1]+1 \} - \frac{\pi_i (\alpha+\mu_i)(\frac{\alpha}{\alpha+\mu_i})^{\alpha+1}}{(\alpha+\mu_i)^2 \{\pi_i [(\frac{\alpha}{\alpha+\mu_i})^\alpha-1]+1\}^2  }	\right\}\\ 
   		& + k_i \left\{ \frac{\alpha(\alpha y_i - \mu_i^2 + 2 \mu_i y_i)}{(\alpha+\mu_i)^2 \{\pi_i [(\frac{\alpha}{\alpha+\mu_i})^\alpha-1]+1\}^2} \right\}\\
  \frac{\partial^2 \mu_i}{\partial \delta_0^2} & = e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \mu_i}{\partial \delta_1^2} & = x_i^2e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \mu_i}{\partial \delta_0 \partial \delta_1} & =x_i e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \ell_i}{\partial \pi_i \partial \mu_i} & = (1-k_i) \frac{-(\frac{\alpha}{\alpha+\mu_i})^{\alpha+1}}{ \{\pi_i [(\frac{\alpha}{\alpha+\mu_i})^\alpha-1]+1\}^2} 
   \end{align*}

Table \ref{tbl:zigp.table} gives the MLEs for the four regression parameters, and the $\alpha$ parameter. In Figure \ref{fig:zigp.plot}, a plot of the expectation function $E(Y)$ versus values of price is displayed. The fit appears to be pretty good in spite of the relative noise in the data.

<<zigp.table, results='asis'>>=
zigp.table <- data.frame(t(par.zigp.newtraph))
names(zigp.table) <- c("b0", "b1", "d0", "d1")

print(xtable(zigp.table, digits = c(0, 6, 6, 6, 6), caption = "MLEs for the model for all stores", label = "tbl:zigp.table"), table.position = 'H')
@

<<zigp.plot, warning=FALSE, fig.cap='Plot of the expectation function of the ZIGP model versus values of price, overlaying the actual data'>>=
bean.ddply <- ddply(bean.data, .(price), summarise, actual_mvm = mean(mvm))

prices <- seq(0, 1, by = 0.01)
b0 <- par.zigp.newtraph[1]; b1 <- par.zigp.newtraph[2]; d0 <- par.zigp.newtraph[3]; d1 <- par.zigp.newtraph[4]
zigp.predict <- data.frame(Price = prices, PredictedSales = (exp(b0 + b1*prices) / (1 + exp(b0 + b1*prices))) * exp(d0 + d1*prices))
zigp.merge <- merge(zigp.predict, bean.ddply, by.x = "Price", by.y = "price")
zigp.melt <- melt(zigp.merge, id.vars = c("Price"))

qplot(Price, PredictedSales, data = zigp.merge, geom = "line", size = I(3), colour = I("blue")) +
    geom_bar(aes(x = Price, y = actual_mvm), stat = "identity", inherit.aes = FALSE)
@

\subsection{Comparison with pscl}
The R package pscl includes a function to fit a zero-inflated negative-binomial model, statistically equivalent to a zero-inflated Gamma-Poisson mixture model. In this section, we will fit such a model and compare to the results derived by hand.

<<zigp.pscl, results='asis'>>=
pscl.zigp <- zeroinfl(mvm ~ price, data = bean.data, dist = "negbin")

pscl.par.zigp <- c(pscl.zigp$coefficients$zero[1], pscl.zigp$coefficients$zero[2], pscl.zigp$coefficients$count[1], pscl.zigp$coefficients$count[2])
pscl.par.zigp <- as.numeric(pscl.par.zigp)
names(pscl.par.zigp) <- c("b0_pscl", "b1_pscl", "d0_pscl", "d1_pscl")

names(par.zigp.newtraph) <- c("b0", "b1", "d0", "d1")
my.df <- data.frame(t(c(par.zigp.newtraph, pscl.par.zigp)))

print(xtable(my.df, digits = c(0, rep(6, 8)), caption = "Comparison of MLEs to results obtained from pscl package", label = "tbl:zip.compare"), table.position = 'H')
@

\section{Model Assessment}
<<compare, cache=TRUE>>=
zip.newtraph.full <- newton.raphson(zip.loglik, zip.gradient, zip.hessian, start = c(1, 1, 1, 1), y = bean.data$mvm, x = bean.data$price)
sip.newtraph.full <- newton.raphson(sip.loglik, sip.gradient, sip.hessian, start = c(1, 1), y = bean.data$mvm, x = bean.data$price)

comparison.results <- ldply(seq(0, 1, by = 0.01), function(pr) {
    sip <- exp(sip.newtraph.full$par[1] + sip.newtraph.full$par[2]*pr)
    zip <- exp(zip.newtraph.full$par[1] + zip.newtraph.full$par[2]*pr) / (1 + exp(zip.newtraph.full$par[1] + zip.newtraph.full$par[2]*pr)) * exp(zip.newtraph.full$par[3] + zip.newtraph.full$par[4]*pr)
    zigp <- as.numeric(exp(par.zigp.newtraph[1] + par.zigp.newtraph[2]*pr) / (1 + exp(par.zigp.newtraph[1] + par.zigp.newtraph[2]*pr)) * exp(par.zigp.newtraph[3] + par.zigp.newtraph[4]*pr))
    actual <- mean(subset(bean.data, price == pr)$mvm)
    if (is.nan(actual)) actual <- 0
    
    c(price = pr, sip = sip, zip = zip, zigp = zigp, actual = actual)
})
comparison.melt <- melt(comparison.results, id.vars = "price")

qplot(price, value, data = comparison.melt, group = variable, colour = variable, geom = "line", size = I(3))
qplot(price, value, data = subset(comparison.melt, price > 0.3 & price < 0.7), group = variable, colour = variable, geom = "line", size = I(3))
@

Let's compare the three models discussed in terms of expected daily sales at fixed price levels, compared to what was actually observed in the data.

<<compare2, results='asis'>>=
actual.1 <- table(factor(subset(bean.data, price == 0.66)$mvm, levels = 0:32))
expected.sip.1 <- sip.fn(sip.newtraph.full$par, 0:32, 0.66) * length(subset(bean.data, price == 0.66)$mvm)
expected.zip.1 <- zip.fn(zip.newtraph.full$par, 0:32, 0.66) * length(subset(bean.data, price == 0.66)$mvm)
expected.zigp.1 <- zigp.fn(c(zigp.newtraph$b0, zigp.newtraph$b1, zigp.newtraph$d0, zigp.newtraph$d1), 0:32, 0.66, zigp.newtraph$alpha) * length(subset(bean.data, price == 0.66)$mvm)

print(xtable(data.frame(y = 0:32, sip.expect = expected.sip.1, zip.expect = expected.zip.1, zigp.expect = expected.zigp.1, actual = as.numeric(actual.1)), digits = c(0, 0, 4, 4, 4, 0)), include.rownames = FALSE)
@

<<compare3, results='asis', warning=FALSE>>=
chi.table <- ldply(sort(unique(bean.data$price)), function(pr) {
    y <- subset(bean.data, price == pr)$mvm
    actual.1 <- table(factor(subset(bean.data, price == pr)$mvm, levels = min(y):min(max(y), 100)))
    expected.sip.1 <- sip.fn(sip.newtraph.full$par, min(max(y), 100), pr) * length(subset(bean.data, price == pr)$mvm)
    expected.zip.1 <- zip.fn(zip.newtraph.full$par, min(max(y), 100), pr) * length(subset(bean.data, price == pr)$mvm)
    expected.zigp.1 <- zigp.fn(c(zigp.newtraph$b0, zigp.newtraph$b1, zigp.newtraph$d0, zigp.newtraph$d1), min(max(y), 100), pr, zigp.newtraph$alpha) * length(subset(bean.data, price == pr)$mvm)
    
    ch1 <- sum((actual.1 - expected.sip.1)^2 / expected.sip.1)
    ch2 <- sum((actual.1 - expected.zip.1)^2 / expected.zip.1)
    ch3 <- sum((actual.1 - expected.zigp.1)^2 / expected.zigp.1)
    
    if (is.na(ch1)) ch1 <- Inf
    best <- "sip"
    if (ch3 < ch2 & ch3 < ch1) best <- "zigp"
    if (ch2 < ch3 & ch2 < ch1) best <- "zip"
    
    c(price = pr, sip.chi = ch1, zip.chi = ch2, zigp.chi = ch3, best = best)
})

xtable(chi.table, display = c("d", "f", "E", "E", "E", "s"), digits = c(0, 2, 4, 4, 4, 0))
@

\subsection{Comparison with pscl}
The R package pscl includes a function to fit a zero-inflated negative-binomial model, statistically equivalent to a zero-inflated Gamma-Poisson mixture model. In this section, we will fit such a model and compare to the results derived by hand.

<<zigp.pscl, results='asis'>>=
pscl.zigp <- zeroinfl(mvm ~ price, data = bean.data, dist = "negbin")

pscl.par.zigp <- c(pscl.zigp$coefficients$zero[1], pscl.zigp$coefficients$zero[2], pscl.zigp$coefficients$count[1], pscl.zigp$coefficients$count[2])
pscl.par.zigp <- as.numeric(pscl.par.zigp)
names(pscl.par.zigp) <- c("b0_pscl", "b1_pscl", "d0_pscl", "d1_pscl")

names(par.zigp.newtraph) <- c("b0", "b1", "d0", "d1")
my.df <- data.frame(t(c(par.zigp.newtraph, pscl.par.zigp)))

print(xtable(my.df, digits = c(0, rep(6, 8)), caption = "Comparison of MLEs to results obtained from pscl package", label = "tbl:zip.compare"), table.position = 'H')
@

\section{Bayesian Estimation of ZIP Model}
We now consider the ZIP model within a Bayesian framework.  The model we consider is a straightforward Bayesian model for one store with noninformative priors on the model parameters. 
  \begin{align*}
  Y_i & \sim \text{Poisson}(z_i\cdot \lambda_i ) \\
  Z_i & \sim \text{Bernoulli}(p_i) \\
  \log(\lambda_i) & = \beta_0 + \beta_1 x_i \\
  \log\left(\frac{p_i}{1-p_i}\right) & = \delta_0 + \delta_1 x_i \\
  (\beta_0, \beta_1)^T \equiv \underaccent{\tilde}{\beta} & \sim MVN\left(\binom{0}{0}, \Sigma_\beta \right) \\
  (\delta_0, \delta_1)^T \equiv \underaccent{\tilde}{\delta} & \sim MVN\left(\binom{0}{0}, \Sigma_\delta \right)
  \end{align*}
  where $\Sigma_\beta = \Sigma_\delta = \begin{bmatrix} 100 & 0 \\ 0 & 100 \end{bmatrix}$, so we assume that $\beta_0,\beta_1$ are independent and $\delta_0,\delta_1$ are independent.  We also assume that $\underaccent{\tilde}{\beta}$ and $\underaccent{\tilde}{\delta}$ are independent.  The priors of $ \underaccent{\tilde}{\beta},\underaccent{\tilde}{\delta}$ are given by
  \begin{align*}
  \pi( \underaccent{\tilde}{\beta}) & = \frac{1}{2\pi}|\Sigma_\beta|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2}  \underaccent{\tilde}{\beta}^T \Sigma_\beta^{-1} \underaccent{\tilde}{\beta}\right\} \\
   \pi( \underaccent{\tilde}{\delta}) & = \frac{1}{2\pi}|\Sigma_\delta|^{-\frac{1}{2}} \exp\left\{ -\frac{1}{2}  \underaccent{\tilde}{\delta}^T \Sigma_\delta^{-1} \underaccent{\tilde}{\delta}\right\}
  \end{align*}
   The joint distribution of the data is 
  $$f(\underaccent{\tilde}{y}| \underaccent{\tilde}{x}, \underaccent{\tilde}{\beta},\underaccent{\tilde}{\delta}) = \prod_{i=1}^n\left\{ \left[(1-p_i) + p_i e^{-\lambda_i}\right]\cdot \mathbb{I}(y_i = 0) + \left[\frac{p_i}{y_i!}e^{-\lambda_i}\lambda_i^{y_i}\right]\cdot \mathbb{I}(y_i > 0)\right\}.$$ 
  The joint posterior distribution is 
  \begin{align*}
   p(\underaccent{\tilde}{\beta},\underaccent{\tilde}{\delta}|\underaccent{\tilde}{y}, \underaccent{\tilde}{x}) & \propto f(\underaccent{\tilde}{y}| \underaccent{\tilde}{x}, \underaccent{\tilde}{\beta},\underaccent{\tilde}{\delta}) \pi(\underaccent{\tilde}{\beta}) \pi(\underaccent{\tilde}{\delta}) \\
     & \propto \prod_{i=1}^n\left\{ \left[(1-p_i) + p_i e^{-\lambda_i}\right]\cdot \mathbb{I}(y_i = 0) + \left[\frac{p_i}{y_i!}e^{-\lambda_i}\lambda_i^{y_i}\right]\cdot \mathbb{I}(y_i > 0)\right\} \\
	& \times \exp\left\{ -\frac{1}{2}  \underaccent{\tilde}{\beta}^T \Sigma_\beta^{-1} \underaccent{\tilde}{\beta}\right\} \times  \exp\left\{ -\frac{1}{2}  \underaccent{\tilde}{\delta}^T \Sigma_\delta^{-1} \underaccent{\tilde}{\delta}\right\}
  \end{align*}
  We cannot sample from this distribution directly, so we use Metropolis-Hastings within Gibbs to generate samples from the joint distribution by iteratively sampling from the full conditional distributions of $\underaccent{\tilde}{\beta}$ and $\underaccent{\tilde}{\delta}$. These full conditional distributions are given by
  \begin{align*}
  p(\underaccent{\tilde}{\beta}|\underaccent{\tilde}{y},\underaccent{\tilde}{x},\underaccent{\tilde}{\delta}) &\propto  \prod_{i=1}^n\left\{ \left[(1-p_i) + p_i e^{-\lambda_i}\right]\cdot \mathbb{I}(y_i = 0) + \left[\frac{p_i}{y_i!}e^{-\lambda_i}\lambda_i^{y_i}\right]\cdot \mathbb{I}(y_i > 0)\right\} \times \exp\left\{ -\frac{1}{2}  \underaccent{\tilde}{\beta}^T \Sigma_\beta^{-1} \underaccent{\tilde}{\beta}\right\} \\
   p(\underaccent{\tilde}{\delta}|\underaccent{\tilde}{y},\underaccent{\tilde}{x},\underaccent{\tilde}{\beta}) &\propto \prod_{i=1}^n\left\{ \left[(1-p_i) + p_i e^{-\lambda_i}\right]\cdot \mathbb{I}(y_i = 0) + \left[\frac{p_i}{y_i!}e^{-\lambda_i}\lambda_i^{y_i}\right]\cdot \mathbb{I}(y_i > 0)\right\} \times  \exp\left\{ -\frac{1}{2}  \underaccent{\tilde}{\delta}^T \Sigma_\delta^{-1} \underaccent{\tilde}{\delta}\right\}
  \end{align*}


\clearpage

\section{Code Appendix}
<<Rcode, eval=FALSE, ref.label=all_labels()[-1],echo=TRUE, cache=FALSE>>=
@

\end{document}
