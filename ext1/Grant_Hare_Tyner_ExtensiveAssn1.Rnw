\documentclass[11pt]{article}

\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{amsmath}

\begin{document}
<<concordance, echo=FALSE>>=
opts_chunk$set(concordance=TRUE, echo=FALSE, tidy=TRUE)
opts_knit$set(self.contained=FALSE)
@

<<libraries, cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE>>=
library(pscl)
library(plyr)
library(ggplot2)
library(reshape2)
library(xtable)
@

\setlength{\parskip}{3ex}
\setlength{\parindent}{0pt}

\title{Extensive Assignment 1}
\author{Millicent Grant, Eric Hare, Samantha Tyner}

\maketitle

\clearpage

<<readthedata, echo=FALSE>>=
bean.data <- read.table("greenbeandat.txt", header = TRUE)

## Remove unnecessary columns
## Exclude stores in 7000s based on MLEs being ridiculous
bean.data <- bean.data[,-c(2, 6, 7)]
bean.data <- subset(bean.data, store < 7000)

bean.indiv <- subset(bean.data, store == 1039)
@

\setcounter{page}{1}

\section{Exploratory Analysis}

<<price_plot, message=FALSE>>=
qplot(mvm, data = bean.indiv, geom = "histogram") + facet_wrap(~price)
@

\section{Newton-Raphson Algorithm}

<<newtraph, echo=TRUE>>=
newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-2, 3), max.iterations = 100, step.halving = TRUE, debug = FALSE, ...) {
    current <- start
    conditions <- TRUE
    
    iterations <- 0
    while (TRUE) {        
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        if(!(any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3])) break;
        
        if (debug) cat(paste("Current loglik is", loglik(current, ...), "\n"))
        if (debug) cat(paste("New is now", new, "\n"))
        if (debug) cat(paste("New loglik is", loglik(new, ...), "\n"))
        
        if (step.halving & (loglik(new, ...) < loglik(current, ...))) {
            if (debug) cat("Uh oh, time to fix this\n")
            m <- 1
            while (m < max.iterations & loglik(new, ...) < loglik(current, ...)) {
                new <- as.vector(current - (1 / (2 * m)) * solve(hessian(current, ...)) %*% gradient(current, ...))
                m <- 2*m;
            }
            if (debug) cat(paste("We have fixed it! its now", new, "\n"))
            if (debug) cat(paste("And the new loglik is finally", loglik(new, ...), "\n"))
        }
        
        iterations <- iterations + 1
        if (iterations > max.iterations) {
            if (debug) cat(paste("Didn't converge in", max.iterations, "iterations\n"))
            break;
        }
                
        if (debug) cat("\n")
        
        current <- new
    }
    
    return(new)
}
@


\section{Simple Poisson}

\subsection{Model Formulation}

Define random variables $\{Y_{i}: i = 1,..., n\}$ associated with the quantity of number of units of green beans sold on day $i$ in an individual store at a fixed price.  In this case, $i$ indexes days that the store was selling green bean units for a fixed price $x$.  Let the distribution of $Y_{i}$ be Poisson. Then,
\begin{align*}
f(y_{i}|\lambda) = \frac{e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}, y_{i} > 0
\end{align*}
and
\begin{align*}
L(\lambda) &= \prod_{i = 1}^n f(y_{i}| \lambda) \\
&= \prod_{i = 1}^n \frac{e^{-\lambda}\lambda^{y_{i}}}{y_{i}!} \\
&= e^{-\lambda}\lambda^{\sum_{i = 1}^{n} y_{i}} \prod_{i = 1}^{n} \frac{1}{y_{i}!}
\end{align*}
Now, assume $\lambda = exp\{\delta_{0} + \delta_{1}x_{i}\}$. Then,
\begin{align*}
L(\lambda) &= exp[-exp\{\delta_{0} + \delta_{1}x_{i}\}]exp\{\delta_{0} + \delta_{1}x_{i}\}^{\sum_{i = 1}^{n}y_{i}}\prod_{i = 1}^{n} \frac{1}{y_{i}!}\\
\ell(\lambda) &= -exp\{\delta_{0} + \delta_{1}x_{i}\} + \sum_{i = 1}^{n}y_{i}(\delta_{0} + \delta_{1}x_{i})
\end{align*}
The partial derivatives of $\delta_{0}$ and $\delta_{1}$ are
\begin{align*}
\frac{\partial \ell(\lambda)}{\partial \delta_{0}} &= -exp\{\delta_{0} + \delta_{1}x_{i}\} + \sum_{i = 1}^{n}y_{i} \\
\frac{\partial \ell(\lambda)}{\partial \delta_{1}} &= -x_{i}exp\{\delta_{0} + \delta_{1}x_{i}\} + x_{i}\sum_{i = 1}^{n}y_{i} \\
\frac{\partial^2 \ell(\lambda)}{\partial \delta_{0}^{2}} &= -exp\{\delta_{0} + \delta_{1}x_{i}\} \\
\frac{\partial^2 \ell(\lambda)}{\partial \delta_{1}^2} &= -x_{i}^{2}exp\{\delta_{0} + \delta_{1}x_{i}\}
\end{align*}

\subsection{Maximum Likelihood Estimation}
<<sip_newtraph>>=
sip.loglik <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    
    return(sum(y * log(T1) - T1 - lfactorial(y)))
}

sip.gradient <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    mu <- T1
    
    dldmu <- y/mu - 1
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldd0, dldd1))
}

sip.hessian <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    mu <- T1
    
    d2ldmu2 <- -y/mu^2
    
    d2mudd02 <- T1
    d2mudd0dd1 <- x*T1
    d2mudd12 <- (x^2)*T1
    
    d2ldd02 <- sum(d2ldmu2 * d2mudd02)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldmu2 * d2mudd0dd1)
    d2ldd12 <- sum(d2ldmu2 * d2mudd12)
    
    return(matrix(c(d2ldd02, d2ldd0dd1, d2ldd0dd1, d2ldd12), nrow = 2))
}

stores <- unique(bean.data$store)[1:9]
sip.stores <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    par.sip.newtraph <- newton.raphson(sip.loglik, sip.gradient, sip.hessian, start = c(5, -5), y = y, x = x)
    
    d0 <- par.sip.newtraph[1]
    d1 <- par.sip.newtraph[2]

    prices <- seq(min(x) - .1, max(x) + .1, by = 0.01)
    
    data.frame(Store = sto, d0 = d0, d1 = d1, Price = prices, PredictedSales = exp(d0 + d1*prices))
})
@

<<sip.table, results='asis'>>=
sip.table <- ddply(sip.stores[,1:3], .(Store), unique)

print(xtable(sip.table, digits = c(0, 0, 6, 6), caption = "MLEs for the first nine stores", label = "tbl:sip.table"), table.position = 'H')
@

<<sip.plot, warning=FALSE>>=
df1 <- subset(sip.stores, Store %in% stores[1:9])
df2 <- subset(bean.data, store %in% stores[1:9])
df3 <- ddply(df2, .(price, store), summarise, actual_mvm = mean(mvm))
sip.merge <- merge(df1, df3, by.x = c("Price", "Store"), by.y = c("price", "store"))

qplot(Price, PredictedSales, data = sip.merge, geom = "line", facets=~Store, colour = I("red"), size = I(2)) + geom_bar(aes(x = Price, y = actual_mvm), stat = "identity") + xlim(c(0.3, 0.75))
@

\subsection{Comparison to glm}
<<sip.glm, results='asis'>>=
sip.glm.table <- ldply(stores, function(sto) {
    glm.model <- glm(mvm ~ price, data = subset(bean.data, store == sto), family = "poisson")
    glm.par <- glm.model$coefficients
    glm.par <- c(sto, as.numeric(glm.par))
    
    names(glm.par) <- c("Store", "d0_glm", "d1_glm")
    glm.par
})

my.df <- merge(sip.table, sip.glm.table, by = "Store")

print(xtable(my.df, digits = c(0, 0, rep(6, 4)), caption = "Comparison of MLEs to results obtained from glm package", label = "tbl:sip.compare"), table.position = 'H')
@

\section{Zero-Inflated Poisson}

\subsection{Model Formulation}

Now, define another set of random variables $\{Z_{i}: i = 1,...,n\}$ associated with consumer interest in green beans. Take these random variables to be independent and identically distributed with a binary probability mass function having parameter $\pi$. So,
\[Z_{i} = \left\{
  \begin{array}{lr}
    1 & : \text{consumer interested}\\
    0 & : \text{consumer not interested}
  \end{array}
\right.
\]
Then, for $0 < p < 1$, 
\[g(z_{i}|\pi) = \left\{
  \begin{array}{lr}
    \pi^{z_{i}}(1 - \pi)^{1 - z_{i}} & : z_{i} = 0, 1 \\
    0 & : \text{otherwise}
  \end{array}
\right.
\]
Define random variables associated with units sold given $Z_{i} = 1$,
\begin{align*}
P(Y_{i} = 0) &= P(Y_{i} = 0 | Z_{i} = 0)P(Z_{i} = 0) + P(Y_{i} = 0 | Z_{i} = 1)P(Z_{i} = 1) \\
&= 1(1 - \pi) + \frac{1}{0!}\lambda^{0}e^{-\lambda}\pi \\
&= (1- \pi) + \pi exp(-\lambda) \hspace{0.2 cm} \text{and} \\
P(Y_{i} = y) &= P(Y_{i} = y | Z_{i} = 0)P(Z_{i} = 0) + P(Y_{i} = y | Z_{i} = 1)P(Z_{i} = 1) \\
&= 0 + \frac{1}{y_{i}!}\lambda^{y_{i}}e^{-\lambda}\pi \\
&= \frac{\pi \lambda^{y_{i}}e^{-\lambda}}{y_{i}!}
\end{align*}
The marginal distribution of $Y_{i}$ leads to the zero-inflated Poisson probability mass function. for $\lambda > 0$ and $0 < \pi < 1$,
\[h(y_{i}|\pi, \lambda) = \left\{
  \begin{array}{lr}
    (1 - \pi) + \pi exp(-\lambda)& : y_{i} = 0 \\
      \pi \lambda^{y_{i}}e^{-\lambda}\frac{1}{y_{i}!} & : y_{i} > 0 \\
      0 & : \hspace{0.1 cm} \text{otherwise}
  \end{array}
\right.
\]
The likelihood and log likelihood is as follows
\begin{align*}
L(\lambda, \pi) &= \prod_{i = 1}^n h(y_{i}| \lambda, \pi) \\
&= \prod_{i = 1}^n [(1 - \pi) + \pi e^{-\lambda}]I(y_{i} = 0) + [\frac{\pi e^{-\lambda}\lambda^{y_{i}}}{y_{i}!}]I(y_{i} > 0) \\
&= [(1 - \pi) + \pi e^{-\lambda}]I(y_{i} = 0) + [\pi e^{-\lambda} \lambda^{\sum^{n}_{i = 1}y_{i}} \prod_{i = 1}^{n} \frac{1}{y_{i}!}]I(y_{i} > 0) \\
\ell_{1}(\lambda, \pi) &= log((1 - \pi) + \pi e^{-\lambda})I(y_{i} = 0) \\
&= log(1 + \pi(e^{-\lambda} - 1))I(y_{i} = 0) \\
\ell_{2}(\lambda, \pi) &= \bigg[log \pi - \lambda + \sum_{i = 1}^{n} y_{i} log \lambda + \sum_{i = 1}^{n} \frac{1}{y_{i}!}\bigg]I(y_{i} > 0) \\
\ell(\lambda, \pi) &= \ell_{1}(\lambda, \pi) + \ell_{2}(\lambda, \pi)
\end{align*}
Now, assume $\pi = \frac{e^{\beta_{0} + \beta_{1}x_{i}}}{1 + e^{\beta_{0} + \beta_{1}x_{i}}}$ and $\lambda = e^{\delta_{0} + \delta_{1}x_{i}}$.  Then, 
\begin{align*}
\ell_{1}(\lambda, \pi) &= log\bigg[1 - \frac{e^{\beta_{0} + \beta_{1}x_{i}}}{1 + e^{\beta_{0} + \beta_{1}x_{i}}}(exp\{-e^{\delta_{0} + \delta_{1}x_{i}}\} - 1)\bigg] \\
\ell_{2}(\lambda, \pi) &= log\bigg(\frac{e^{\beta_{0} + \beta_{1}x_{i}}}{1 + e^{\beta_{0} + \beta_{1}x_{i}}}\bigg) - e^{\delta_{0} + \delta_{1}x_{i}} + \sum_{i = 1}^{n} y_{i} log(e^{\delta_{0} + \delta_{1}x_{i}}) + \sum_{i = 1}^{n} \frac{1}{y_{i}!}\\
\ell(\lambda, \pi) &= \ell_{1}(\lambda, \pi) + \ell_{2}(\lambda, \pi)
\end{align*}
The first partial derivatives are given by
\begin{align*}
\frac{\partial \ell(\lambda, \pi)}{\partial \delta_{0}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \lambda}\frac{\partial \lambda}{\partial \delta_{0}} \\
\frac{\partial \ell(\lambda, \pi)}{\partial \delta_{1}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \lambda}\frac{\partial \lambda}{\partial \delta_{1}} \\
\frac{\partial}{\partial \lambda} \ell(\lambda, \pi) &= (\frac{y_{i}}{\lambda} - 1)I(y_{i} > 0) - \frac{\pi e^{-\lambda}}{(1 - \pi) + \pi e^{-\lambda}}I(y_{i} = 0) \\
\frac{\partial \lambda}{\partial \delta_{0}} &= exp\{\delta_{0} + \delta_{1}x_{i}\} \\
\frac{\partial \lambda}{\partial \delta_{1}} &= x_{i} exp\{\delta_{0} + \delta_{1}x_{i}\} 
\end{align*}

\begin{align*}
\frac{\partial \ell(\lambda, \pi)}{\partial \beta_{0}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \pi}\frac{\partial \pi}{\partial \beta_{0}} \\
\frac{\partial \ell(\lambda, \pi)}{\partial \beta_{1}} &= \frac{\partial \ell(\lambda, \pi)}{\partial \pi}\frac{\partial \pi}{\partial \beta_{1}} \\
\frac{\partial \ell(\lambda, \pi)}{\partial \pi} &= \bigg[\frac{e^{\lambda} - 1}{e^{\lambda}(\pi - 1) - \pi}\bigg]I(y_{i} = 0) + \frac{1}{\pi}I(y_{i > 0})\\
\frac{\partial \pi}{\partial \beta_{0}} &= \frac{e^{\beta_{0} + \beta_{1}x_{i}}}{(e^{\beta_{0} + \beta_{1}x_{i}})^2} \\
\frac{\partial \pi}{\partial \beta_{1}} &= \frac{x_{i}e^{\beta_{0} + \beta_{1}x_{i}}}{(e^{\beta_{0} + \beta_{1}x_{i}})^2}
\end{align*}

The second partial derivatives are given by
\begin{align*}
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \delta_{0}^{2}} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \lambda^{2}}\frac{\partial^{2} \lambda}{\partial \delta_{0}^2} \\
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \delta_{1}^2} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \lambda^{2}}\frac{\partial^{2} \lambda}{\partial \delta_{1}^2} \\
\frac{\partial^{2}}{\partial \lambda^{2}} \ell(\lambda, \pi) &= (\frac{-1}{\lambda^{2}})I(y_{i} > 0) - \frac{1}{[(1 - \pi) + \pi e^{-\lambda}]^2}I(y_{i} > 0) \\
\frac{\partial^{2} \lambda}{\partial \delta_{0}^{2}} &= exp\{\delta_{0} + \delta_{1}x_{i}\} \\
\frac{\partial^2 \lambda}{\partial \delta_{1}^2} &= x_{i}^{2} exp\{\delta_{0} + \delta_{1}x_{i}\} 
\end{align*}

\begin{align*}
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \beta_{0}^{2}} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \pi^{2}}\frac{\partial^{2}\pi}{\partial \beta_{0}^{2}} \\
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \beta_{1}^{2}} &= \frac{\partial^{2} \ell(\lambda, \pi)}{\partial \pi^{2}}\frac{\partial^{2} \pi}{\partial \beta_{1}^{2}} \\
\frac{\partial^{2} \ell(\lambda, \pi)}{\partial \pi^{2}} &= \bigg[\frac{e^{\lambda} - 1}{e^{\lambda}(\pi - 1) - \pi}\bigg]^{2}I(y_{i} = 0) + \frac{1}{\pi^{2}}I(y_{i > 0})\\
\frac{\partial^{2} \pi}{\partial \beta_{0}^{2}} &= -e^{-\beta_{1}x_{i} - \beta_{0}} \\
\frac{\partial^{2} \pi}{\partial \beta_{1}^{2}} &= -x_{i}e^{-\beta_{0} - \beta_{1}x_{i}}
\end{align*}

\subsection{Maximum Likelihood Estimation}
<<zip.newtraph>>=
zip.loglik <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    pi <- exp(b0 + b1*x) / (1 + exp(b0 + b1*x))
    lambda <- exp(d0 + d1*x)
    
    single <- ((y == 0) * log((1 - pi) + pi*exp(-lambda))) +
              ((y > 0) * (log(pi) + y*log(lambda) - lambda))
    
    return(sum(single))
}
    
zip.gradient <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    pi <- T0 / (1 + T0)
    
    dldpi <- ((y == 0) * (exp(-T1) - 1) / (exp(-T1) * pi + 1 - pi)) +
             (y > 0) * (1 / pi)
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2
    
    dldmu <- ((y > 0) * (y/T1 - 1) - (y == 0)*(pi * exp(-T1))/((1 - pi) + pi*exp(-T1)))
    dmudd0 <- T1
    dmudd1 <- x*T1
  
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldb0, dldb1, dldd0, dldd1))
}

zip.hessian <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    pi <- T0 / (1 + T0)
    
    dldpi <- ((y == 0) * (exp(-T1) - 1) / (exp(-T1) * pi + 1 - pi)) +
             (y > 0) * (1 / pi)
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2
    
    dldmu <- ((y > 0) * (y/T1 - 1) - (y == 0)*(pi * exp(-T1))/((1 - pi) + pi*exp(-T1)))
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    d2ldmu2 <- (-y / T1^2)*(y > 0) + (y == 0)*((pi * (1 - pi) * exp(-T1)) / ((1 - pi) + pi*exp(-T1))^2)
    
    d2ldpi2 <- -((exp(T1) - 1) / (exp(T1) * (pi - 1) - pi))^2  * (y == 0) + (y > 0)*(-1 / pi^2)
    d2ldpidmu <- (y == 0) * -exp(-T1) / ((1 - pi) + pi * exp(-T1))^2
    
    d2pidb02 <- (T0 * (1 - T0)) / (1 + T0)^3
    d2pidb12 <- (x^2 * T0 * (1 - T0)) / (1 + T0)^3
    d2pidb0db1 <- (x * T0 * (1 - T0)) / (1 + T0)^3
    
    d2mudd02 <- T1
    d2mudd12 <- x^2 * T1
    d2mudd0dd1 <- x * T1
    
    d2ldb02 <- sum(d2ldpi2 * dpidb0 * dpidb0 + d2pidb02 * dldpi)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldpi2 * dpidb0 * dpidb1 + d2pidb0db1 * dldpi) 
    d2ldb0dd0 <- d2ldd0db0 <- sum(d2ldpidmu * dpidb0 * dmudd0)
    d2ldb0dd1 <- d2ldd1db0 <- sum(d2ldpidmu * dpidb0 * dmudd1)
    d2ldb12 <- sum(d2ldpi2 * dpidb1 * dpidb1 + d2pidb12 * dldpi)
    d2ldb1dd0 <- d2ldd0db1 <- sum(d2ldpidmu * dpidb1 * dmudd0)
    d2ldb1dd1 <- d2ldd1db1 <- sum(d2ldpidmu * dpidb1 * dmudd1)
    d2ldd02 <- sum(d2ldmu2 * dmudd0 * dmudd0 + d2mudd02 * dldmu)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldmu2 * dmudd0 * dmudd1 + d2mudd0dd1 * dldmu)
    d2ldd12 <- sum(d2ldmu2 * dmudd1 * dmudd1 + d2mudd12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb0dd0, d2ldb0dd1, d2ldb1db0, d2ldb12, d2ldb1dd0, d2ldb1dd1, d2ldd0db0, d2ldd0db1, d2ldd02, d2ldd0dd1, d2ldd1db0, d2ldd1db1, d2ldd1dd0, d2ldd12), nrow = 4, byrow = TRUE))
}

stores <- unique(bean.data$store)[1:9]
zip.stores <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    par.zip.newtraph <- newton.raphson(zip.loglik, zip.gradient, zip.hessian, start = c(5, -5, 5, -5), y = y, x = x)
    
    b0 <- par.zip.newtraph[1]
    b1 <- par.zip.newtraph[2]
    d0 <- par.zip.newtraph[3]
    d1 <- par.zip.newtraph[4]
    
    prices <- seq(min(x) - .1, max(x) + .1, by = 0.01)
    
    data.frame(Store = sto, b0 = b0, b1 = b1, d0 = d0, d1 = d1, Price = prices, PredictedSales = (exp(b0 + b1*prices) / (1 + exp(b0 + b1*prices))) * (exp(d0 + d1*prices)))
})
@

<<zip.table, results='asis'>>=
zip.table <- ddply(zip.stores[,1:5], .(Store), unique)

print(xtable(zip.table, digits = c(0, 0, 6, 6, 6, 6), caption = "MLEs for the first nine stores", label = "tbl:zip.table"), table.position = 'H')
@

<<zip.plot, warning=FALSE>>=
df1 <- subset(zip.stores, Store %in% stores[1:9])
df2 <- subset(bean.data, store %in% stores[1:9])
df3 <- ddply(df2, .(price, store), summarise, actual_mvm = mean(mvm))
zip.merge <- merge(df1, df3, by.x = c("Price", "Store"), by.y = c("price", "store"))
sip.merge$model <- "sip"
zip.merge$model <- "zip"

test <- cbind(sip.merge[,c("Price", "Store", "PredictedSales", "actual_mvm")], zip.merge$PredictedSales)
names(test)[3:5] <- c("SIP", "Actual", "ZIP")
predict.melt <- melt(test, id.vars = c("Price", "Store", "Actual"))

qplot(Price, value, data = predict.melt, geom = "line", facets=~Store, colour = variable, group = variable, alpha = I(0.4), size = I(3)) +
    geom_bar(aes(x = Price, y = Actual), stat = "identity", inherit.aes = FALSE) +
    theme(legend.position = "bottom")
@

\subsection{Comparison to pscl}

<<zip.pscl, results='asis'>>=
zip.pscl.table <- ldply(stores, function(sto) {
    pscl.zip <- zeroinfl(mvm ~ price, data = subset(bean.data, store == sto), dist = "poisson")

    pscl.par <- c(pscl.zip$coefficients$zero[1], pscl.zip$coefficients$zero[2], pscl.zip$coefficients$count[1], pscl.zip$coefficients$count[2])
    pscl.par <- c(sto, as.numeric(pscl.par))
    names(pscl.par) <- c("Store", "b0_pscl", "b1_pscl", "d0_pscl", "d1_pscl")
    
    pscl.par
})

my.df <- merge(zip.table, zip.pscl.table, by = "Store")

print(xtable(my.df, digits = c(0, 0, rep(6, 8)), caption = "Comparison of MLEs to results obtained from pscl package", label = "tbl:zip.compare"), table.position = 'H')
@

\section{Zero-Inflated Gamma-Poisson}

\subsection{Model Formulation}
$Y_i|\lambda_i \sim \text{independent Poisson}(\lambda_i)$ \\
$\lambda_i|\alpha,b_i \sim \text{independent Gamma}(\alpha, b_i)$ \\

\begin{align*}
h(y_i | \alpha, b_i) &= \int f(y_i | \lambda_i, p)g(\lambda_i | \alpha,b_i)d\lambda_i \\
                       &= \frac{b_i^{\alpha}}{y_i!\Gamma{(\alpha)}}\int_0^{\infty} \lambda_i^{y_i}e^{-\lambda_i}\lambda_i^{\alpha - 1}e^{-b_i\lambda_i}d\lambda_i \\
                       &= \frac{b_i^{\alpha}}{y_i!\Gamma{(\alpha)}}\int_0^{\infty} \lambda_i^{y_i + \alpha - 1}e^{-\lambda_i(b_i + 1)}d\lambda_i \\
                       &= \frac{b_i^{\alpha}\Gamma{(\alpha + y_i)}}{y_i!\Gamma{(\alpha)}(b_i + 1)^{\alpha + y_i}}
\end{align*}

\begin{align*}
Pr(Y_i = 0 | \lambda_i, p) &= Pr(Y_i = 0 | Z_i = 0)Pr(Z_i = 0) + Pr(Y_i = 0 | Z_i = 1)Pr(Z_i = 1) \\
            &= Pr(Z_i = 0) + Pr(Y_i = 0 | Z_i = 1)Pr(Z_i = 1) \\
            &= (1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha \\
\end{align*}

\begin{align*}
Pr(Y_i = y | \lambda_i, p) &= Pr(Y_i = y | Z_i = 0)Pr(Z_i = 0) + Pr(Y_i = y | Z_i = 1)Pr(Z_i = 1) \\
            &= Pr(Y_i = y | Z_i = 1)Pr(Z_i = 1) \\
            &= \pi_i\frac{b_i^{\alpha}\Gamma{(\alpha + y_i)}}{y_i!\Gamma{(\alpha)}(b_i + 1)^{\alpha + y_i}} \\
            &= \pi_i{\alpha + y_i - 1 \choose y_i}\frac{b_i^{\alpha}}{(b_i + 1)^{\alpha + y_i}} \\
            &= \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}
\end{align*}

Hence, the full zero-inflated gamma-poisson distribution pmf is: \\

$(1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}I(y_i > 0)$ \\

$log(E(Y|x)) = \delta_0 + \delta_1x_i$ \\
$\mu_i = E(Y|x) = e^{\delta_0 + \delta_1x_i} = \frac{\alpha}{b_i}$ \\
$\pi_i = \frac{e^{\beta_0 + \beta_1x_i}}{1 + e^{\beta_0 + \beta_1x_i}}$

\begin{align*}
L_i(\pi_i,\alpha, b_i) &= (1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}I(y_i > 0) \\
L_i(\pi_i,\mu_i,\alpha)  &= (1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}I(y_i > 0)\\
\ell_i(\pi_i,\mu_i,\alpha) & = (1-k_i)\log\left((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha\right)+k_i\log\left(\pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}\right)\\
\ell(\pi,\mu,\alpha) &= \sum_{i=1}^n (1-k_i)\log\left((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha\right)+k_i\log\left(\pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}\right) 
\end{align*}
where $k_i = 0$ if $y_i = 0$ and $k_i = 1$ if $y_i > 0$.  

<<zigp.newtraph, cache=TRUE>>=
zigp.loglik <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha
    
    return(sum((1 - (y > 0)) * log((1 - pi) + pi*T2) + ((y > 0) * log(pi * choose(alpha + y - 1, y) * T2 * (mu / (alpha + mu))^y))))
}

zigp.gradient <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha

    dldpi <- (1 - (y > 0)) * (T2 - 1)/((1 - pi) + pi*T2) + ((y > 0) * (1 / pi))
    
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2

    dldmu <- (1 - (y > 0)) * (-pi * (alpha / (alpha + mu))^(alpha + 1)) / ((1 - pi) + pi*T2) + ((y > 0) * (-alpha * (mu - y)) / (mu * (alpha + mu)))
    
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldb0, dldb1, dldd0, dldd1))
}

zigp.hessian <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    
    mu <- T1
    pi <- T0 / (1 + T0)
    
    T2 <- (alpha / (alpha + mu))^alpha
    
    dldpi <- (1 - (y > 0)) * (T2 - 1)/((1 - pi) + pi*T2) + ((y > 0) * (1 / pi))
    
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2

    dldmu <- (1 - (y > 0)) * (-pi * (alpha / (alpha + mu))^(alpha + 1)) / ((1 - pi) + pi*T2) + ((y > 0) * (-alpha * (mu - y)) / (mu * (alpha + mu)))
    
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    d2ldpi2 <- ((T2 - 1)^2 * ((y > 0) - 1) / (T2*pi - pi + 1)^2) + ((y > 0) * -1/pi^2)
    d2pidb02 <- (T0 * (1 - T0)) / (1 + T0)^3
    d2pidb12 <- (x^2 * T0 * (1 - T0)) / (1 + T0)^3
    d2pidb0db1 <- (x * T0 * (1 - T0)) / (1 + T0)^3
    
    piece1 <- (alpha * pi * T2 * ((alpha + 1) * (pi * (T2 - 1) + 1) - pi*(alpha + mu) * (alpha / (alpha + mu))^(alpha + 1))) / ((alpha + mu)^2 * (pi * (T2 - 1) + 1)^2)
    piece2 <- (alpha * (alpha*y - mu^2 + 2*mu*y))/(mu^2 * (alpha + mu)^2)
    d2ldmu2 <- (1 - (y > 0)) * piece1 - (y > 0)*piece2
    
    d2ldpidmu <- ((y > 0) - 1) * ((alpha / (alpha + mu))^(alpha + 1)) / (pi * (T2 - 1) + 1)^2
    
    d2mudd02 <- T1
    d2mudd12 <- x^2 * T1
    d2mudd0dd1 <- x * T1
    
    d2ldb02 <- sum(d2ldpi2 * dpidb0 * dpidb0 + d2pidb02 * dldpi)
    d2ldb0db1 <- d2ldb1db0 <- sum(d2ldpi2 * dpidb0 * dpidb1 + d2pidb0db1 * dldpi) 
    d2ldb0dd0 <- d2ldd0db0 <- sum(d2ldpidmu * dpidb0 * dmudd0) #
    d2ldb0dd1 <- d2ldd1db0 <- sum(d2ldpidmu * dpidb0 * dmudd1) #
    d2ldb12 <- sum(d2ldpi2 * dpidb1 * dpidb1 + d2pidb12 * dldpi)
    d2ldb1dd0 <- d2ldd0db1 <- sum(d2ldpidmu * dpidb1 * dmudd0) #
    d2ldb1dd1 <- d2ldd1db1 <- sum(d2ldpidmu * dpidb1 * dmudd1) #
    d2ldd02 <- sum(d2ldmu2 * dmudd0 * dmudd0 + d2mudd02 * dldmu)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldmu2 * dmudd0 * dmudd1 + d2mudd0dd1 * dldmu)
    d2ldd12 <- sum(d2ldmu2 * dmudd1 * dmudd1 + d2mudd12 * dldmu)
    
    return(matrix(c(d2ldb02, d2ldb0db1, d2ldb0dd0, d2ldb0dd1, d2ldb1db0, d2ldb12, d2ldb1dd0, d2ldb1dd1, d2ldd0db0, d2ldd0db1, d2ldd02, d2ldd0dd1, d2ldd1db0, d2ldd1db1, d2ldd1dd0, d2ldd12), nrow = 4, byrow = TRUE))
}

par.sip.newtraph.full <- newton.raphson(sip.loglik, sip.gradient, sip.hessian, start = c(1, 1), y = bean.data$mvm, x = bean.data$price)

par.zip.newtraph.full <- newton.raphson(zip.loglik, zip.gradient, zip.hessian, start = c(1, 1, 1, 1), y = bean.data$mvm, x = bean.data$price)

alpha <- 1.026
par.zigp.newtraph <- newton.raphson(zigp.loglik, zigp.gradient, zigp.hessian, start = par.zip.newtraph.full, y = bean.data$mvm, x = bean.data$price, alpha = alpha)
@

<<alpha_moment_estimator>>=
x <- bean.data$price
y <- bean.data$mvm
mui <- exp(par.zigp.newtraph[1] + par.zigp.newtraph[2]*x) / (1 + exp(par.zigp.newtraph[1] + par.zigp.newtraph[2]*x)) * exp(par.zigp.newtraph[3] + par.zigp.newtraph[4]*x)
vmui <- mui^2

## Moment estimator
alpha_hat <- (1 / length(y)) * sum((y - mui)^2 / vmui)
@

 \subsubsection{First Derivatives}
  \begin{align*}
  \frac{\partial \ell_i}{\partial \beta_i} & =  \frac{\partial \ell_i}{\partial \pi_i} \frac{\partial \pi_i}{\partial \beta_i}   \quad \text{for $i=0,1$}\\
   \frac{\partial \ell_i}{\partial \delta_i} & =  \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \delta_i}  \quad \text{for $i = 0,1$}
  \end{align*}
  \begin{align*}
   \frac{\partial \ell_i}{\partial \pi_i}   & = (1-k_i) \cdot \frac{\left((\frac{\alpha}{\alpha+\mu_i})^\alpha - 1\right)}{(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha} + k_i \cdot \frac{1}{ \pi_i} \\
 \frac{\partial \pi_i}{\partial \beta_0} & = \frac{e^{\beta_0+\beta_1x_i}}{(1+e^{\beta_0+\beta_1x_i})^2} \\
  \frac{\partial \pi_i}{\partial \beta_1} & = \frac{x_ie^{\beta_0+\beta_1x_i}}{(1+e^{\beta_0+\beta_1x_i})^2} \\
  \frac{\partial \ell_i}{\partial \mu_i} & = (1-k_i)\cdot \frac{-\pi_i(\frac{\alpha}{\alpha+\mu_i})^{\alpha+1}}{(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha} + k_i \cdot -\frac{\alpha(\mu_i-y_i)}{\mu_i(\alpha+\mu_i)} \\
   \frac{\partial \mu_i}{\partial \delta_0} & = e^{\delta_0+\delta_1 x_i} \\
   \frac{\partial \mu_i}{\partial \delta_1} & = x_ie^{\delta_0+\delta_1 x_i}
  \end{align*}
  
  \subsubsection{Second Derivatives}
  \begin{align*}
    \frac{\partial^2 \ell_i}{\partial \beta_i^2} & = \frac{\partial^2 \ell_i}{\partial \pi_i^2} \frac{\partial^2 \pi_i}{\partial \beta_i^2}  \quad \text{for $i=0,1$} \\
	  \frac{\partial^2 \ell_i}{\partial \beta_0\beta_1} & = \frac{\partial^2 \ell_i}{\partial \pi_i^2} \frac{\partial^2 \pi_i}{\partial \beta_0 \partial \beta_1} =  \frac{\partial^2 \ell_i}{\partial \beta_1\beta_0} \\
	  \frac{\partial^2 \ell_i}{\partial \delta_i^2} & = \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial^2 \mu_i}{\partial \delta_i^2}  \quad \text{for $i=0,1$}  \\ 
	    \frac{\partial^2 \ell_i}{\partial \delta_0\delta_1} & = \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial^2 \mu_i}{\partial \delta_0 \partial \delta_1} =  \frac{\partial^2 \ell_i}{\partial \delta_1\delta_0}
  \end{align*}
  \begin{align*}
 \frac{\partial^2 \ell_i}{\partial \pi_i^2}  & = (1-k_i) \cdot \frac{-\left((\frac{\alpha}{\alpha+\mu_i})^\alpha - 1\right)(\frac{\alpha}{\alpha+\mu_i})^\alpha}{[(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha]^2} + k_i \cdot \frac{-1}{ \pi_i^2} \\
  \frac{\partial^2 \pi_i}{\partial \beta_0^2} & = \frac{(1+e^{\beta_0+\beta_1x_i})^2e^{\beta_0+\beta_1x_i} - 2(e^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
  	& = \frac{e^{\beta_0+\beta_1x_i} (1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
   \frac{\partial^2 \pi_i}{\partial \beta_1^2} & = \frac{(1+e^{\beta_0+\beta_1x_i})^2x_i^2e^{\beta_0+\beta_1x_i} - 2(x_ie^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
   		& = \frac{x_i^2e^{\beta_0+\beta_1x_i}(1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
  \frac{\partial^2 \pi_i}{\partial \beta_0 \partial \beta_1} & = \frac{(1+ e^{\beta_0+\beta_1x_i})^2x_i e^{\beta_0+\beta_1x_i} - 2x_i(e^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
  	& = \frac{x_ie^{\beta_0+\beta_1x_i}(1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
  \frac{\partial^2 \ell_i}{\partial \mu_i^2} & = (1-k_i)\cdot \frac{((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha)(\pi_i (\alpha^2+\alpha)(\frac{\alpha}{\alpha+\mu_i})^{\alpha}(\alpha+\mu_i)^{-2}) - (-\pi_i(\frac{\alpha}{\alpha+\mu_i})^\alpha)^2}{((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha)^2} \\ 
  &+ k_i\cdot \frac{-\alpha\mu_i(\alpha+\mu_i) + (\alpha^2+\alpha)(\mu_i-y_i)}{\mu_i^2(\alpha+\mu_i)^2}	\\
  \frac{\partial^2 \mu_i}{\partial \delta_0^2} & = e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \mu_i}{\partial \delta_1^2} & = x_i^2e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \mu_i}{\partial \delta_0 \partial \delta_1} & =x_i e^{\delta_0+\delta_1 x_i}
   \end{align*} 

\subsection{Maximum Likelihood}
<<zigp.table, results='asis'>>=
zigp.table <- data.frame(t(par.zigp.newtraph))
names(zigp.table) <- c("b0", "b1", "d0", "d1")

print(xtable(zigp.table, digits = c(0, 6, 6, 6, 6), caption = "MLEs for the model for all stores", label = "tbl:zigp.table"), table.position = 'H')
@

<<zigp.plot, warning=FALSE>>=
bean.ddply <- ddply(bean.data, .(price), summarise, actual_mvm = mean(mvm))

prices <- seq(0, 1, by = 0.01)
b0 <- par.zigp.newtraph[1]; b1 <- par.zigp.newtraph[2]; d0 <- par.zigp.newtraph[3]; d1 <- par.zigp.newtraph[4]
zigp.predict <- data.frame(Price = prices, PredictedSales = (exp(b0 + b1*prices) / (1 + exp(b0 + b1*prices))) * exp(d0 + d1*prices))
zigp.merge <- merge(zigp.predict, bean.ddply, by.x = "Price", by.y = "price")
zigp.melt <- melt(zigp.merge, id.vars = c("Price"))

qplot(Price, PredictedSales, data = zigp.merge, geom = "line", size = I(3), colour = I("blue")) +
    geom_bar(aes(x = Price, y = actual_mvm), stat = "identity", inherit.aes = FALSE)
@

\subsection{Comparison with pscl}
The R package pscl includes a function to fit a zero-inflated negative-binomial model, statistically equivalent to a zero-inflated Gamma-Poisson mixture model. In this section, we will fit such a model and compare to the results derived by hand.

<<zigp.pscl, results='asis'>>=
pscl.zigp <- zeroinfl(mvm ~ price, data = bean.data, dist = "negbin")

pscl.par.zigp <- c(pscl.zigp$coefficients$zero[1], pscl.zigp$coefficients$zero[2], pscl.zigp$coefficients$count[1], pscl.zigp$coefficients$count[2])
pscl.par.zigp <- as.numeric(pscl.par.zigp)
names(pscl.par.zigp) <- c("b0_pscl", "b1_pscl", "d0_pscl", "d1_pscl")

names(par.zigp.newtraph) <- c("b0", "b1", "d0", "d1")
my.df <- data.frame(t(c(par.zigp.newtraph, pscl.par.zigp)))

print(xtable(my.df, digits = c(0, rep(6, 8)), caption = "Comparison of MLEs to results obtained from pscl package", label = "tbl:zip.compare"), table.position = 'H')
@

\clearpage

\section{Code Appendix}
<<Rcode, eval=FALSE, ref.label=all_labels()[-1],echo=TRUE, cache=FALSE>>=
@

\end{document}
