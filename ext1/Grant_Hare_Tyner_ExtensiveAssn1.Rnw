\documentclass[11pt]{article}

\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{amsmath}

\begin{document}
<<concordance, echo=FALSE>>=
opts_chunk$set(concordance=TRUE)
opts_knit$set(self.contained=FALSE)
@

<<libraries, cache=FALSE, echo=FALSE, message=FALSE, warning=FALSE>>=
library(pscl)
library(plyr)
library(ggplot2)
library(reshape2)
library(xtable)
@

\setlength{\parskip}{3ex}
\setlength{\parindent}{0pt}

\title{Extensive Assignment 1}
\author{Millicent Grant, Eric Hare, Samantha Tyner}

\maketitle

\clearpage

<<readthedata, echo=FALSE>>=
bean.data <- read.table("greenbeandat.txt", header = TRUE)

## Remove unnecessary columns
## Exclude stores in 7000s based on MLEs being ridiculous
bean.data <- bean.data[,-c(2, 6, 7)]
bean.data <- subset(bean.data, store < 7000)

bean.indiv <- subset(bean.data, store == 1039)
@

\setcounter{page}{1}

\section{Exploratory Analysis}

<<price_plot, message=FALSE>>=
qplot(mvm, data = bean.indiv, geom = "histogram") + facet_wrap(~price)
@

\section{Simple Poisson}

\subsection{Model Formulation}

\subsection{Maximum Likelihood Estimation}
<<sip_newtraph>>=

newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-2, 3), max.iterations = 100, step.halving = TRUE, debug = FALSE, ...) {
    current <- start
    conditions <- TRUE
    
    iterations <- 0
    while (TRUE) {        
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        if(!(any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3])) break;
        
        if (debug) cat(paste("Current loglik is", loglik(current, ...), "\n"))
        if (debug) cat(paste("New is now", new, "\n"))
        if (debug) cat(paste("New loglik is", loglik(new, ...), "\n"))
        
        if (step.halving & (loglik(new, ...) < loglik(current, ...))) {
            if (debug) cat("Uh oh, time to fix this\n")
            m <- 1
            while (m < max.iterations & loglik(new, ...) < loglik(current, ...)) {
                new <- as.vector(current - (1 / (2 * m)) * solve(hessian(current, ...)) %*% gradient(current, ...))
                m <- 2*m;
            }
            if (debug) cat(paste("We have fixed it! its now", new, "\n"))
            if (debug) cat(paste("And the new loglik is finally", loglik(new, ...), "\n"))
        }
        
        iterations <- iterations + 1
        if (iterations > max.iterations) {
            if (debug) cat(paste("Didn't converge in", max.iterations, "iterations\n"))
            break;
        }
                
        if (debug) cat("\n")
        
        current <- new
    }
    
    return(new)
}

sip.loglik <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    
    return(sum(y * log(T1) - T1 - lfactorial(y)))
}

sip.gradient <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    mu <- T1
    
    dldmu <- y/mu - 1
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldd0, dldd1))
}

sip.hessian <- function(par, y, x) {
    d0 <- par[1]
    d1 <- par[2]
    
    T1 <- exp(d0 + d1*x)
    mu <- T1
    
    d2ldmu2 <- -y/mu^2
    
    d2mudd02 <- T1
    d2mudd0dd1 <- x*T1
    d2mudd12 <- (x^2)*T1
    
    d2ldd02 <- sum(d2ldmu2 * d2mudd02)
    d2ldd0dd1 <- d2ldd1dd0 <- sum(d2ldmu2 * d2mudd0dd1)
    d2ldd12 <- sum(d2ldmu2 * d2mudd12)
    
    return(matrix(c(d2ldd02, d2ldd0dd1, d2ldd0dd1, d2ldd12), nrow = 2))
}

#newton.raphson(sip.loglik, sip.gradient, sip.hessian, start = c(5, -5), y = y, x = x, debug = FALSE)
@

<<sip>>=
par <- c(5, -0.5)

pois.func <- function(y, mu) {
    return(exp(-mu) * mu^y * (1 / factorial(y)))
}

fnsip <- function(par, y, x) {
    mu <- exp(par[1] + par[2]*x)
    return(sum(log(pois.func(y, mu))))
}

fnsiplog <- function(par, y, x) {
    mu <- exp(par[1] + par[2]*x)
    return(sum(-mu + y*log(mu) - lfactorial(y)))
}

stores <- unique(bean.data$store)[3:14]

prices <- seq(min(subset(bean.data, store %in% stores)$price) - .1, max(subset(bean.data, store %in% stores)$price) + .1, by = 0.01)
sip.stores <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    lik <- optim(par = par, fn = fnsiplog, y = y, x = x, control = list(fnscale = -1))
    
    data.frame(Store = sto, Price = prices, PredictedSales = exp(lik$par[1] + lik$par[2]*prices))
})

qplot(Price, PredictedSales, data = subset(sip.stores, Store == stores[6]), geom = "blank") +
    geom_line(colour = "red", size = 2) +
    geom_point(data = subset(bean.data, store == stores[4]), aes(x = price, y = mvm))
@

\subsection{Comparison to glm}
<<glm, results='asis'>>=
glm.model <- glm(mvm ~ price, data = subset(bean.data, store == stores[6]), family = "poisson")

sto <- stores[6]
x <- subset(bean.data, store == sto)$price
y <- subset(bean.data, store == sto)$mvm

lik <- optim(par = par, fn = fnsiplog, y = y, x = x, control = list(fnscale = -1))

glm.par <- glm.model$coefficients
glm.par <- as.numeric(glm.par)

my.df <- data.frame(OurModel = lik$par, GLM = glm.par)
print(xtable(my.df, digits = 6), table.position = 'H')
@

\section{Zero-Inflated Poisson}

\subsection{Model Formulation}

\subsection{Maximum Likelihood Estimation}
<<zip>>=
par <- c(1, 1, 5, -0.5)

fnz <- function(par, y, x) {
    b0 <- par[1]
    b1 <- par[2]
    d0 <- par[3]
    d1 <- par[4]
    
    pi <- exp(b0 + b1*x) / (1 + exp(b0 + b1*x))
    lambda <- exp(d0 + d1*x)
    
    single <- ((y == 0) * log((1 - pi) + pi*exp(-lambda))) +
              ((y > 0) * (log(pi) + y*log(lambda) - lambda))
    
    return(single)
}

fnzip <- function(par, y, x) {
    #yuck <- ((log(1 + pi*(exp(-lambda) - 1)) * (y == 0)) +
    #       (log(pi) - lambda + sum(y * log(lambda)) + sum(1 / factorial(y))) * (y > 0))
    
    return(sum(fnz(par, y, x)))
}

stores <- unique(bean.data$store)
zip.stores <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    lik <- optim(par = par, fn = fnzip, y = y, x = x, control = list(fnscale = -1))
    
    data.frame(Store = sto, Price = prices, PredictedSales = (exp(lik$par[1] + lik$par[2]*prices) / (1 + exp(lik$par[1] + lik$par[2]*prices))) * (exp(lik$par[3] + lik$par[4] * prices)))
})

qplot(Price, PredictedSales, data = subset(zip.stores, Store == stores[6]), geom = "blank") +
    geom_line(colour = "green", size = 2) +
    geom_point(data = subset(bean.data, store == stores[6]), aes(x = price, y = mvm))
@

<<zip_mles, results='asis'>>=
zip.mles <- ldply(stores, function(sto) {
    x <- subset(bean.data, store == sto)$price
    y <- subset(bean.data, store == sto)$mvm
    
    lik <- optim(par = par, fn = fnzip, y = y, x = x, control = list(fnscale = -1))
    
    c(sto, lik$par)
})
names(zip.mles) <- c("store", "b0", "b1", "d0", "d1")

print(xtable(zip.mles, digits = c(0, 4, 4, 4, 4, 0)), table.position = 'H')
@

\subsection{Comparison to pscl}

<<zip_model, results='asis'>>=
sto <- stores[6]
x <- subset(bean.data, store == sto)$price
y <- subset(bean.data, store == sto)$mvm

lik <- optim(par = par, fn = fnzip, y = y, x = x, control = list(fnscale = -1))

pscl.zip <- zeroinfl(mvm ~ price, data = subset(bean.data, store == sto), dist = "poisson")

pscl.par <- c(pscl.zip$coefficients$zero[1], pscl.zip$coefficients$zero[2], pscl.zip$coefficients$count[1], pscl.zip$coefficients$count[2])
pscl.par <- as.numeric(pscl.par)

my.df <- data.frame(OurModel = lik$par, PSCL = pscl.par)
print(xtable(my.df, digits = 6), table.position = 'H')
@

\section{Zero-Inflated Gamma-Poisson}

\subsection{Model Formulation}
$Y_i|\lambda_i \sim \text{independent Poisson}(\lambda_i)$ \\
$\lambda_i|\alpha,b_i \sim \text{independent Gamma}(\alpha, b_i)$ \\

\begin{align*}
h(y_i | \alpha, b_i) &= \int f(y_i | \lambda_i, p)g(\lambda_i | \alpha,b_i)d\lambda_i \\
                       &= \frac{b_i^{\alpha}}{y_i!\Gamma{(\alpha)}}\int_0^{\infty} \lambda_i^{y_i}e^{-\lambda_i}\lambda_i^{\alpha - 1}e^{-b_i\lambda_i}d\lambda_i \\
                       &= \frac{b_i^{\alpha}}{y_i!\Gamma{(\alpha)}}\int_0^{\infty} \lambda_i^{y_i + \alpha - 1}e^{-\lambda_i(b_i + 1)}d\lambda_i \\
                       &= \frac{b_i^{\alpha}\Gamma{(\alpha + y_i)}}{y_i!\Gamma{(\alpha)}(b_i + 1)^{\alpha + y_i}}
\end{align*}

<<test>>=
alpha <- 20
beta <- 5
y <- bean.indiv$mvm

gammapois <- function(y, alpha, beta) {
    return((beta^alpha * gamma(alpha + y)) / (factorial(y) * gamma(alpha) * (beta + 1)^(alpha + y)))
}

my.df <- data.frame(x = 0:20, fn = gammapois(0:20, alpha, beta))

qplot(x, fn, data = my.df, geom = "bar", stat = "identity")
@

\begin{align*}
Pr(Y_i = 0 | \lambda_i, p) &= Pr(Y_i = 0 | Z_i = 0)Pr(Z_i = 0) + Pr(Y_i = 0 | Z_i = 1)Pr(Z_i = 1) \\
            &= Pr(Z_i = 0) + Pr(Y_i = 0 | Z_i = 1)Pr(Z_i = 1) \\
            &= (1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha \\
\end{align*}

\begin{align*}
Pr(Y_i = y | \lambda_i, p) &= Pr(Y_i = y | Z_i = 0)Pr(Z_i = 0) + Pr(Y_i = y | Z_i = 1)Pr(Z_i = 1) \\
            &= Pr(Y_i = y | Z_i = 1)Pr(Z_i = 1) \\
            &= \pi_i\frac{b_i^{\alpha}\Gamma{(\alpha + y_i)}}{y_i!\Gamma{(\alpha)}(b_i + 1)^{\alpha + y_i}} \\
            &= \pi_i{\alpha + y_i - 1 \choose y_i}\frac{b_i^{\alpha}}{(b_i + 1)^{\alpha + y_i}} \\
            &= \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}
\end{align*}

Hence, the full zero-inflated gamma-poisson distribution pmf is: \\

$(1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}I(y_i > 0)$ \\

$log(E(Y|x)) = \delta_0 + \delta_1x_i$ \\
$\mu_i = E(Y|x) = e^{\delta_0 + \delta_1x_i} = \frac{\alpha}{b_i}$ \\
$\pi_i = \frac{e^{\beta_0 + \beta_1x_i}}{1 + e^{\beta_0 + \beta_1x_i}}$

\begin{align*}
L_i(\pi_i,\alpha, b_i) &= (1 - \pi_i) + \pi_i(\frac{b_i}{b_i + 1})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{b_i}{b_i + 1})^{\alpha}(\frac{1}{b_i + 1})^{y_i}I(y_i > 0) \\
L_i(\pi_i,\mu_i,\alpha)  &= (1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha I(y_i = 0) + \pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}I(y_i > 0)\\
\ell_i(\pi_i,\mu_i,\alpha) & = (1-k_i)\log\left((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha\right)+k_i\log\left(\pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}\right)\\
\ell(\pi,\mu,\alpha) &= \sum_{i=1}^n (1-k_i)\log\left((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha\right)+k_i\log\left(\pi_i{\alpha + y_i - 1 \choose y_i}(\frac{\alpha}{\alpha + \mu_i})^{\alpha}(\frac{\mu_i}{\alpha + \mu_i})^{y_i}\right) 
\end{align*}
where $k_i = 0$ if $y_i = 0$ and $k_i = 1$ if $y_i > 0$.  

<<newt-raph>>=
newton.raphson <- function(loglik, gradient, hessian, start, lower = rep(-Inf, length(start)), upper = rep(Inf, length(start)), tol = rep(1e-5, 3), max.iterations = 1000, debug = FALSE, ...) {
    current <- runif(length(start))
    new <- start
    conditions <- TRUE
    
    iterations <- 0
    while (conditions) {
        new <- as.vector(current - solve(hessian(current, ...)) %*% gradient(current, ...))
        new[new < lower] <- lower[new < lower] + tol[1]
        new[new > upper] <- upper[new > upper] - tol[1]
        
        conditions <- any(abs(gradient(new, ...)) > tol[1]) | loglik(new, ...) - loglik(current, ...) > tol[2] | dist(rbind(current, new))[1] > tol[3]
        
        if (debug) cat(paste("New is now", new, "\n"))
        
        iterations = iterations + 1
        if (iterations > max.iterations) stop(paste("Algorithm did not converge in", max.iterations, "iterations."))
        
        current <- new
    }
    
    return(new)
}

alpha <- 1
par <- c(5, -5, 5, -5)
y <- bean.data$mvm
x <- bean.data$price

zigp.loglik <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha
    
    return(sum((1 - (y > 0)) * log((1 - pi) + pi*T2) + ((y > 0) * log(pi * choose(alpha + y - 1, y) * T2 * (mu / (alpha + mu))^y))))
}

zigp.gradient <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    mu <- T1
    pi <- T0 / (1 + T0)
    T2 <- (alpha / (alpha + mu))^alpha

    dldpi <- (1 - (y > 0)) * (T2 - 1)/((1 - pi) + pi*T2) + ((y > 0) * (1 / pi))
    
    dpidb0 <- T0 / (1 + T0)^2
    dpidb1 <- x*T0 / (1 + T0)^2

    dldmu <- (1 - (y > 0)) * (-pi * (alpha / (alpha + mu))^(alpha + 1)) / ((1 - pi) + pi*T2) + ((y > 0) * (-alpha * (mu - y)) / (mu * (alpha + mu)))
    
    dmudd0 <- T1
    dmudd1 <- x*T1
    
    dldb0 <- sum(dldpi * dpidb0)
    dldb1 <- sum(dldpi * dpidb1)
    dldd0 <- sum(dldmu * dmudd0)
    dldd1 <- sum(dldmu * dmudd1)
    
    return(c(dldb0, dldb1, dldd0, dldd1))
}

zigp.hessian <- function(par, y, x, alpha) {
    b0 <- par[1]; b1 <- par[2]; d0 <- par[3]; d1 <- par[4];
    
    T0 <- exp(b0 + b1*x)
    T1 <- exp(d0 + d1*x)
    
    mu <- T1
    pi <- T0 / (1 + T0)
    
    T2 <- (alpha / (alpha + mu))^alpha
    
    d2ldpi2 <- (1 - (y > 0)) * -((T2 - 1) * T2) / ((1 - pi) + pi*T2)^2 + ((y > 0) * -1/pi^2)
    d2pidb02 <- (T0 * (1 - T0)) / (1 + T0)^3
    d2pidb12 <- (x^2 * T0 * (1 - T0)) / (1 + T0)^3
    d2pidb0db1 <- (x * T0 * (1 - T0)) / (1 + T0)^3
    
    d2ldmu2 <- (1 - (y > 0)) * (((1 - pi) + pi*T2) * (pi * (alpha^2 + alpha) * T2 * (alpha + mu)^-2) - (-pi * T2)^2)/((1 - pi) + pi*T2)^2 + ((y > 0) * ((-alpha * mu * (alpha + mu) + (alpha^2 + alpha)*(mu - y))/(mu^2 * (alpha + mu)^2)))
    
    d2mudd02 <- T1
    d2mudd12 <- x^2 * T1
    d2mudd0dd1 <- x * T1
    
    d2ldb02 <- sum(d2ldpi2 * d2pidb02)
    d2ldb12 <- sum(d2ldpi2 * d2pidb12)
    d2ldb0b1 <- d2ldb1db0 <- sum(d2ldpi2 * d2pidb0db1)
    d2ldd02 <- sum(d2ldmu2 * d2mudd02)
    d2ldd12 <- sum(d2ldmu2 * d2mudd12)
    d2ldd0d1 <- d2ldd1dd0 <- sum(d2ldmu2 * d2mudd0dd1)
    
    return(matrix(c(d2ldb02, d2ldb0b1, 0, 0, d2ldb0b1, d2ldb12, 0, 0,
             0, 0, d2ldd02, d2ldd0d1, 0, 0, d2ldd0d1, d2ldd12), nrow = 4, byrow = TRUE))
}

#newton.raphson(zigp.loglik, zigp.gradient, zigp.hessian, start = c(5, -5, 5, -5), y = y, x = x, alpha = alpha, debug = TRUE)
@

 \subsubsection{First Derivatives}
  \begin{align*}
  \frac{\partial \ell_i}{\partial \beta_i} & =  \frac{\partial \ell_i}{\partial \pi_i} \frac{\partial \pi_i}{\partial \beta_i}   \quad \text{for $i=0,1$}\\
   \frac{\partial \ell_i}{\partial \delta_i} & =  \frac{\partial \ell_i}{\partial \mu_i} \frac{\partial \mu_i}{\partial \delta_i}  \quad \text{for $i = 0,1$}
  \end{align*}
  \begin{align*}
   \frac{\partial \ell_i}{\partial \pi_i}   & = (1-k_i) \cdot \frac{\left((\frac{\alpha}{\alpha+\mu_i})^\alpha - 1\right)}{(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha} + k_i \cdot \frac{1}{ \pi_i} \\
 \frac{\partial \pi_i}{\partial \beta_0} & = \frac{e^{\beta_0+\beta_1x_i}}{(1+e^{\beta_0+\beta_1x_i})^2} \\
  \frac{\partial \pi_i}{\partial \beta_1} & = \frac{x_ie^{\beta_0+\beta_1x_i}}{(1+e^{\beta_0+\beta_1x_i})^2} \\
  \frac{\partial \ell_i}{\partial \mu_i} & = (1-k_i)\cdot \frac{-\pi_i(\frac{\alpha}{\alpha+\mu_i})^{\alpha+1}}{(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha} + k_i \cdot -\frac{\alpha(\mu_i-y_i)}{\mu_i(\alpha+\mu_i)} \\
   \frac{\partial \mu_i}{\partial \delta_0} & = e^{\delta_0+\delta_1 x_i} \\
   \frac{\partial \mu_i}{\partial \delta_1} & = x_ie^{\delta_0+\delta_1 x_i}
  \end{align*}
  
  \subsubsection{Second Derivatives}
  \begin{align*}
    \frac{\partial^2 \ell_i}{\partial \beta_i^2} & = \frac{\partial^2 \ell_i}{\partial \pi_i^2} \frac{\partial^2 \pi_i}{\partial \beta_i^2}  \quad \text{for $i=0,1$} \\
	  \frac{\partial^2 \ell_i}{\partial \beta_0\beta_1} & = \frac{\partial^2 \ell_i}{\partial \pi_i^2} \frac{\partial^2 \pi_i}{\partial \beta_0 \partial \beta_1} =  \frac{\partial^2 \ell_i}{\partial \beta_1\beta_0} \\
	  \frac{\partial^2 \ell_i}{\partial \delta_i^2} & = \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial^2 \mu_i}{\partial \delta_i^2}  \quad \text{for $i=0,1$}  \\ 
	    \frac{\partial^2 \ell_i}{\partial \delta_0\delta_1} & = \frac{\partial^2 \ell_i}{\partial \mu_i^2} \frac{\partial^2 \mu_i}{\partial \delta_0 \partial \delta_1} =  \frac{\partial^2 \ell_i}{\partial \delta_1\delta_0}
  \end{align*}
  \begin{align*}
 \frac{\partial^2 \ell_i}{\partial \pi_i^2}  & = (1-k_i) \cdot \frac{-\left((\frac{\alpha}{\alpha+\mu_i})^\alpha - 1\right)(\frac{\alpha}{\alpha+\mu_i})^\alpha}{[(1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha]^2} + k_i \cdot \frac{-1}{ \pi_i^2} \\
  \frac{\partial^2 \pi_i}{\partial \beta_0^2} & = \frac{(1+e^{\beta_0+\beta_1x_i})^2e^{\beta_0+\beta_1x_i} - 2(e^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
  	& = \frac{e^{\beta_0+\beta_1x_i} (1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
   \frac{\partial^2 \pi_i}{\partial \beta_1^2} & = \frac{(1+e^{\beta_0+\beta_1x_i})^2x_i^2e^{\beta_0+\beta_1x_i} - 2(x_ie^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
   		& = \frac{x_i^2e^{\beta_0+\beta_1x_i}(1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
  \frac{\partial^2 \pi_i}{\partial \beta_0 \partial \beta_1} & = \frac{(1+ e^{\beta_0+\beta_1x_i})^2x_i e^{\beta_0+\beta_1x_i} - 2x_i(e^{\beta_0+\beta_1x_i})^2(1+e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^4} \\
  	& = \frac{x_ie^{\beta_0+\beta_1x_i}(1-e^{\beta_0+\beta_1x_i})}{(1+e^{\beta_0+\beta_1x_i})^3} \\
  \frac{\partial^2 \ell_i}{\partial \mu_i^2} & = (1-k_i)\cdot \frac{((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha)(\pi_i (\alpha^2+\alpha)(\frac{\alpha}{\alpha+\mu_i})^{\alpha}(\alpha+\mu_i)^{-2}) - (-\pi_i(\frac{\alpha}{\alpha+\mu_i})^\alpha)^2}{((1 - \pi_i) + \pi_i(\frac{\alpha}{\alpha + \mu_i})^\alpha)^2} \\ 
  &+ k_i\cdot \frac{-\alpha\mu_i(\alpha+\mu_i) + (\alpha^2+\alpha)(\mu_i-y_i)}{\mu_i^2(\alpha+\mu_i)^2}	\\
  \frac{\partial^2 \mu_i}{\partial \delta_0^2} & = e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \mu_i}{\partial \delta_1^2} & = x_i^2e^{\delta_0+\delta_1 x_i} \\
  \frac{\partial^2 \mu_i}{\partial \delta_0 \partial \delta_1} & =x_i e^{\delta_0+\delta_1 x_i}
   \end{align*} 

\subsection{Parameter Estimation}

<<test2>>=
y <- bean.data$mvm
x <- bean.data$price

zigammapoisone <- function(y, mu, alpha, pi) {
    return(((y == 0) * ((1 - pi) + pi*(alpha / (alpha + mu))^alpha)) +
            ((y > 0) * pi*(choose(alpha + y - 1, y) * (alpha / (alpha + mu))^alpha * (mu / (alpha + mu))^y)))
}

fnzigp <- function(par, y, x, alpha) {
    mu <- exp(par[3] + par[4]*x)
    pi <- exp(par[1] + par[2]*x) / (1 + exp(par[1] + par[2]*x))
    return(sum(log(zigammapoisone(y, mu, alpha, pi))))
}

fnzigplog <- function(par, y, x, alpha) {
    mu <- exp(par[3] + par[4]*x)
    pi <- exp(par[1] + par[2]*x) / (1 + exp(par[1] + par[2]*x))
    return(sum((y == 0) * log((1 - pi) + pi*(alpha / (alpha + mu))^alpha) +
               (y > 0) * (log(pi) + lchoose(alpha + y - 1, y) + alpha*log(alpha) - alpha * log(alpha + mu) + y * log(mu) - y * log(alpha + mu))))
}

fnzigpfixed <- function(par, y, x, fixedpar) {
    mu <- exp(fixedpar[3] + fixedpar[4]*x)
    pi <- exp(fixedpar[1] + fixedpar[2]*x) / (1 + exp(fixedpar[1] + fixedpar[2]*x))
    return(sum((y == 0) * log((1 - pi) + pi*(par[1] / (par[1] + mu))^par[1]) +
               (y > 0) * (log(pi) + lchoose(par[1] + y - 1, y) + par[1]*log(par[1]) - par[1] * log(par[1] + mu) + y * log(mu) - y * log(par[1] + mu))))
}

previous <- rep(0, 5)
current <- c(5, -5, 5, -5, 1)
while (any(current != previous)) {
    previous <- current
    lik <- optim(par = current[1:4], fn = fnzigplog, y = y, x = x, alpha = current[5], control = list(fnscale = -1))
    lik2 <- optim(par = current[5], fn = fnzigpfixed, y = y, x = x, fixedpar = lik$par, control = list(fnscale = -1), method = "Brent", lower = 0, upper = 1000)
    current <- c(lik$par, lik2$par)
    
    print(current)
}

par <- current
@

<<zigpplot, echo=FALSE>>=
prices <- seq(min(bean.data$price) - .1, max(bean.data$price) + .1, by = 0.01)

bean.ddply <- ddply(bean.data, .(price), summarise, mvm = mean(mvm))

zigp.data <- data.frame(Price = prices, PredictedSales = (exp(par[1] + par[2]*prices) / (1 + exp(par[1] + par[2]*prices))) * (exp(par[3] + par[4] * prices)))

qplot(Price, PredictedSales, data = zigp.data, geom = "blank") +
    geom_line(colour = "blue", size = 2) +
    geom_bar(data = bean.ddply, stat = "identity", aes(x = price, y = mvm))
@

\subsection{Comparison with pscl}
The R package pscl includes a function to fit a zero-inflated negative-binomial model, statistically equivalent to a zero-inflated Gamma-Poisson mixture model. In this section, we will fit such a model and compare to the results derived by hand.

<<pscl_compare, results='asis'>>=
pscl.model <- zeroinfl(mvm ~ price, data = bean.data, dist = "negbin")

pscl.par <- c(pscl.model$coefficients$zero[1], pscl.model$coefficients$zero[2], pscl.model$coefficients$count[1], pscl.model$coefficients$count[2], pscl.model$theta)
pscl.par <- as.numeric(pscl.par)

my.df <- data.frame(OurModel = par, PSCL = pscl.par)
print(xtable(my.df, digits = 6), table.position = 'H')
@

<<all_compare, echo=FALSE>>=
prices <- seq(min(bean.data$price) - .1, max(bean.data$price) + .1, by = 0.01)

bean.ddply <- ddply(bean.data, .(price), summarise, mvm = mean(mvm))

lik.sip <- optim(par = c(5, -0.5), fn = fnsiplog, y = y, x = x, control = list(fnscale = -1))
lik.zip <- optim(par = c(5, -5, 5, -0.5), fn = fnzip, y = y, x = x, control = list(fnscale = -1))

compare.data <- data.frame(Price = prices, PredictedSalesSIP = exp(lik.sip$par[1] + lik.sip$par[2]*prices), PredictedSalesZIP =  (exp(lik.zip$par[1] + lik.zip$par[2]*prices) / (1 + exp(lik.zip$par[1] + lik.zip$par[2]*prices))) * (exp(lik.zip$par[3] + lik.zip$par[4] * prices)), PredictedSalesZIGP = (exp(par[1] + par[2]*prices) / (1 + exp(par[1] + par[2]*prices))) * (exp(par[3] + par[4] * prices)))

compare.melt <- melt(compare.data, id.vars = 1)

qplot(Price, value, data = compare.melt, geom = "blank") +
    geom_line(size = 2, aes(colour = variable, x = Price, y = value)) +
    geom_bar(data = bean.ddply, stat = "identity", aes(x = price, y = mvm))

qplot(Price, value, data = subset(compare.melt, Price >= .3), geom = "blank") +
    geom_line(size = 2, aes(colour = variable, x = Price, y = value)) +
    geom_bar(data = subset(bean.ddply, price >= .3), stat = "identity", aes(x = price, y = mvm))

qplot(Price, value, data = subset(compare.melt, Price >= .5), geom = "blank") +
    geom_line(size = 2, aes(colour = variable, x = Price, y = value)) +
    geom_bar(data = subset(bean.ddply, price >= .5), stat = "identity", aes(x = price, y = mvm))
@

\end{document}
